{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Notebook 11: Text Embeddings a S√©mantick√© Vyhled√°v√°n√≠\n",
    "\n",
    "**Autor:** Praut s.r.o. - AI Integration & Business Automation\n",
    "\n",
    "V tomto notebooku se nauƒç√≠me pracovat s textov√Ωmi embeddings - vektorov√Ωmi reprezentacemi textu. Uk√°≈æeme si jak vytv√°≈ôet embeddings, prov√°dƒõt s√©mantick√© vyhled√°v√°n√≠ a budovat jednoduch√© RAG syst√©my.\n",
    "\n",
    "## Co se nauƒç√≠te:\n",
    "- Vytv√°≈ôen√≠ text embeddings pomoc√≠ Sentence Transformers\n",
    "- S√©mantick√© vyhled√°v√°n√≠ a similarity search\n",
    "- Clustering dokument≈Ø\n",
    "- Jednoduch√© vektorov√© datab√°ze\n",
    "- Z√°klady RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalace pot≈ôebn√Ωch knihoven\n",
    "!pip install -q sentence-transformers faiss-cpu numpy pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Detekce za≈ô√≠zen√≠\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Pou≈æ√≠v√°m za≈ô√≠zen√≠: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Z√°klady Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naƒçten√≠ modelu pro embeddings\n",
    "# all-MiniLM-L6-v2 je rychl√Ω a kvalitn√≠ model pro obecn√© pou≈æit√≠\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "print(f\"‚úÖ Model naƒçten\")\n",
    "print(f\"   Dimenze embeddings: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"   Max d√©lka sekvence: {model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvo≈ôen√≠ embeddings pro jednotliv√© vƒõty\n",
    "sentences = [\n",
    "    \"Python je popul√°rn√≠ programovac√≠ jazyk.\",\n",
    "    \"JavaScript se pou≈æ√≠v√° pro webov√Ω v√Ωvoj.\",\n",
    "    \"Koƒçky jsou obl√≠ben√° dom√°c√≠ zv√≠≈ôata.\",\n",
    "    \"Programov√°n√≠ vy≈æaduje logick√© my≈°len√≠.\",\n",
    "    \"Psi jsou vƒõrn√≠ spoleƒçn√≠ci ƒçlovƒõka.\"\n",
    "]\n",
    "\n",
    "# Vytvo≈ôen√≠ embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(f\"Poƒçet vƒõt: {len(sentences)}\")\n",
    "print(f\"Tvar embeddings: {embeddings.shape}\")\n",
    "print(f\"\\nUk√°zka prvn√≠ho embeddingu (prvn√≠ch 10 hodnot):\")\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√Ωpoƒçet podobnosti mezi vƒõtami\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Vizualizace\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity_matrix, cmap='RdYlGn', aspect='auto')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.xticks(range(len(sentences)), [s[:30] + '...' for s in sentences], rotation=45, ha='right')\n",
    "plt.yticks(range(len(sentences)), [s[:30] + '...' for s in sentences])\n",
    "plt.title('Matice podobnosti vƒõt')\n",
    "\n",
    "# P≈ôid√°n√≠ hodnot do bunƒõk\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        plt.text(j, i, f'{similarity_matrix[i, j]:.2f}', \n",
    "                ha='center', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. S√©mantick√© vyhled√°v√°n√≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:\n",
    "    \"\"\"\n",
    "    T≈ô√≠da pro s√©mantick√© vyhled√°v√°n√≠ v korpusu dokument≈Ø.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_documents(self, documents, metadata=None):\n",
    "        \"\"\"\n",
    "        P≈ôid√° dokumenty do indexu.\n",
    "        \n",
    "        Args:\n",
    "            documents: Seznam textov√Ωch dokument≈Ø\n",
    "            metadata: Voliteln√° metadata pro ka≈æd√Ω dokument\n",
    "        \"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "        if metadata:\n",
    "            self.metadata.extend(metadata)\n",
    "        else:\n",
    "            self.metadata.extend([{} for _ in documents])\n",
    "        \n",
    "        # Vytvo≈ôen√≠ embeddings\n",
    "        new_embeddings = self.model.encode(documents, show_progress_bar=True)\n",
    "        \n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = new_embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "        \n",
    "        print(f\"P≈ôid√°no {len(documents)} dokument≈Ø. Celkem: {len(self.documents)}\")\n",
    "    \n",
    "    def search(self, query, top_k=5):\n",
    "        \"\"\"\n",
    "        Vyhled√° nejpodobnƒõj≈°√≠ dokumenty k dotazu.\n",
    "        \"\"\"\n",
    "        if self.embeddings is None or len(self.documents) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Embedding dotazu\n",
    "        query_embedding = self.model.encode([query])\n",
    "        \n",
    "        # V√Ωpoƒçet podobnosti\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Se≈ôazen√≠ podle podobnosti\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                \"document\": self.documents[idx],\n",
    "                \"score\": float(similarities[idx]),\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"index\": int(idx)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def find_similar(self, document_index, top_k=5):\n",
    "        \"\"\"\n",
    "        Najde dokumenty podobn√© dan√©mu dokumentu.\n",
    "        \"\"\"\n",
    "        if document_index >= len(self.documents):\n",
    "            return []\n",
    "        \n",
    "        doc_embedding = self.embeddings[document_index:document_index+1]\n",
    "        similarities = cosine_similarity(doc_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Vynechat samotn√Ω dokument\n",
    "        similarities[document_index] = -1\n",
    "        \n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"document\": self.documents[idx],\n",
    "                \"score\": float(similarities[idx]),\n",
    "                \"index\": int(idx)\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "\n",
    "# Test\n",
    "search_engine = SemanticSearch()\n",
    "print(\"‚úÖ SemanticSearch inicializov√°n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P≈ô√≠prava testovac√≠ho korpusu\n",
    "documents = [\n",
    "    \"Python je interpretovan√Ω programovac√≠ jazyk vysok√© √∫rovnƒõ s dynamick√Ωm typov√°n√≠m.\",\n",
    "    \"Machine learning je podoblast umƒõl√© inteligence zab√Ωvaj√≠c√≠ se algoritmy, kter√© se uƒç√≠ z dat.\",\n",
    "    \"Django je webov√Ω framework pro Python, kter√Ω umo≈æ≈àuje rychl√Ω v√Ωvoj webov√Ωch aplikac√≠.\",\n",
    "    \"TensorFlow je open-source knihovna pro numerick√© v√Ωpoƒçty a strojov√© uƒçen√≠.\",\n",
    "    \"JavaScript je skriptovac√≠ jazyk bƒõ≈ænƒõ pou≈æ√≠van√Ω pro interaktivn√≠ webov√© str√°nky.\",\n",
    "    \"React je JavaScript knihovna pro tvorbu u≈æivatelsk√Ωch rozhran√≠.\",\n",
    "    \"PostgreSQL je v√Ωkonn√° open-source relaƒçn√≠ datab√°ze.\",\n",
    "    \"Docker je platforma pro kontejnerizaci aplikac√≠.\",\n",
    "    \"Kubernetes orchestruje kontejnerizovan√© aplikace v clusteru.\",\n",
    "    \"Git je distribuovan√Ω syst√©m spr√°vy verz√≠ pro sledov√°n√≠ zmƒõn v souborech.\",\n",
    "    \"REST API je architektonick√Ω styl pro n√°vrh webov√Ωch slu≈æeb.\",\n",
    "    \"GraphQL je dotazovac√≠ jazyk pro API vyvinut√Ω Facebookem.\",\n",
    "    \"Redis je in-memory datab√°ze ƒçasto pou≈æ√≠van√° jako cache.\",\n",
    "    \"Celery je distribuovan√° fronta √∫loh pro Python.\",\n",
    "    \"Angular je TypeScript framework pro tvorbu webov√Ωch aplikac√≠.\"\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    {\"category\": \"programming\", \"language\": \"python\"},\n",
    "    {\"category\": \"ai\", \"topic\": \"ml\"},\n",
    "    {\"category\": \"web\", \"language\": \"python\"},\n",
    "    {\"category\": \"ai\", \"topic\": \"ml\"},\n",
    "    {\"category\": \"programming\", \"language\": \"javascript\"},\n",
    "    {\"category\": \"web\", \"language\": \"javascript\"},\n",
    "    {\"category\": \"database\", \"type\": \"relational\"},\n",
    "    {\"category\": \"devops\", \"topic\": \"containers\"},\n",
    "    {\"category\": \"devops\", \"topic\": \"orchestration\"},\n",
    "    {\"category\": \"devops\", \"topic\": \"version-control\"},\n",
    "    {\"category\": \"web\", \"topic\": \"api\"},\n",
    "    {\"category\": \"web\", \"topic\": \"api\"},\n",
    "    {\"category\": \"database\", \"type\": \"cache\"},\n",
    "    {\"category\": \"devops\", \"language\": \"python\"},\n",
    "    {\"category\": \"web\", \"language\": \"typescript\"}\n",
    "]\n",
    "\n",
    "# P≈ôid√°n√≠ dokument≈Ø\n",
    "search_engine.add_documents(documents, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vyhled√°v√°n√≠\n",
    "queries = [\n",
    "    \"Jak vytvo≈ôit webovou aplikaci v Pythonu?\",\n",
    "    \"N√°stroje pro spr√°vu kontejner≈Ø\",\n",
    "    \"Knihovny pro strojov√© uƒçen√≠\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nüîç Dotaz: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = search_engine.search(query, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. [{result['score']:.3f}] {result['document'][:80]}...\")\n",
    "        if result['metadata']:\n",
    "            print(f\"   Metadata: {result['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. V√≠cejazyƒçn√© embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠cejazyƒçn√Ω model\n",
    "multilingual_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device=device)\n",
    "\n",
    "print(\"‚úÖ V√≠cejazyƒçn√Ω model naƒçten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cross-lingual similarity\n",
    "sentences_multilingual = [\n",
    "    \"The cat is sitting on the mat.\",          # EN\n",
    "    \"Koƒçka sed√≠ na koberci.\",                   # CZ\n",
    "    \"Die Katze sitzt auf der Matte.\",          # DE\n",
    "    \"Le chat est assis sur le tapis.\",         # FR\n",
    "    \"The dog is running in the park.\",         # EN\n",
    "    \"Pes bƒõ≈æ√≠ v parku.\",                        # CZ\n",
    "]\n",
    "\n",
    "# Embeddings\n",
    "multi_embeddings = multilingual_model.encode(sentences_multilingual)\n",
    "\n",
    "# Matice podobnosti\n",
    "multi_sim = cosine_similarity(multi_embeddings)\n",
    "\n",
    "# Vizualizace\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(multi_sim, cmap='RdYlGn')\n",
    "plt.colorbar(label='Similarity')\n",
    "\n",
    "labels = ['Cat (EN)', 'Koƒçka (CZ)', 'Katze (DE)', 'Chat (FR)', 'Dog (EN)', 'Pes (CZ)']\n",
    "plt.xticks(range(len(labels)), labels, rotation=45, ha='right')\n",
    "plt.yticks(range(len(labels)), labels)\n",
    "plt.title('Cross-lingual Similarity')\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        plt.text(j, i, f'{multi_sim[i, j]:.2f}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering dokument≈Ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClusterer:\n",
    "    \"\"\"\n",
    "    Clustering dokument≈Ø pomoc√≠ embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    def cluster(self, documents, n_clusters=3):\n",
    "        \"\"\"\n",
    "        Seskup√≠ dokumenty do cluster≈Ø.\n",
    "        \"\"\"\n",
    "        # Vytvo≈ôen√≠ embeddings\n",
    "        embeddings = self.model.encode(documents, show_progress_bar=True)\n",
    "        \n",
    "        # K-Means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        # Organizace v√Ωsledk≈Ø\n",
    "        clusters = {i: [] for i in range(n_clusters)}\n",
    "        for doc, label in zip(documents, cluster_labels):\n",
    "            clusters[label].append(doc)\n",
    "        \n",
    "        return {\n",
    "            \"clusters\": clusters,\n",
    "            \"labels\": cluster_labels,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"centroids\": kmeans.cluster_centers_\n",
    "        }\n",
    "    \n",
    "    def visualize_clusters(self, embeddings, labels, documents=None):\n",
    "        \"\"\"\n",
    "        Vizualizuje clustery pomoc√≠ t-SNE.\n",
    "        \"\"\"\n",
    "        # Redukce dimenz√≠\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "        reduced = tsne.fit_transform(embeddings)\n",
    "        \n",
    "        # Vizualizace\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(reduced[:, 0], reduced[:, 1], c=labels, cmap='viridis', s=100, alpha=0.7)\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        \n",
    "        if documents:\n",
    "            for i, doc in enumerate(documents):\n",
    "                plt.annotate(doc[:20] + '...', (reduced[i, 0], reduced[i, 1]), \n",
    "                           fontsize=8, alpha=0.7)\n",
    "        \n",
    "        plt.title('Document Clusters (t-SNE visualization)')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test\n",
    "clusterer = DocumentClusterer()\n",
    "print(\"‚úÖ DocumentClusterer p≈ôipraven\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clusteringu\n",
    "result = clusterer.cluster(documents, n_clusters=4)\n",
    "\n",
    "print(\"V√Ωsledky clusteringu:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for cluster_id, docs in result['clusters'].items():\n",
    "    print(f\"\\nüìÅ Cluster {cluster_id} ({len(docs)} dokument≈Ø):\")\n",
    "    for doc in docs:\n",
    "        print(f\"   ‚Ä¢ {doc[:70]}...\")\n",
    "\n",
    "# Vizualizace\n",
    "clusterer.visualize_clusters(result['embeddings'], result['labels'], documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FAISS - Efektivn√≠ vektorov√© vyhled√°v√°n√≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "class FAISSVectorStore:\n",
    "    \"\"\"\n",
    "    Vektorov√° datab√°ze pomoc√≠ FAISS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        # Inicializace FAISS indexu\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product (cosine sim pro normalizovan√© vektory)\n",
    "        \n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add(self, documents, metadata=None):\n",
    "        \"\"\"\n",
    "        P≈ôid√° dokumenty do indexu.\n",
    "        \"\"\"\n",
    "        # Vytvo≈ôen√≠ a normalizace embeddings\n",
    "        embeddings = self.model.encode(documents)\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # P≈ôid√°n√≠ do FAISS\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        # Ulo≈æen√≠ dokument≈Ø a metadat\n",
    "        self.documents.extend(documents)\n",
    "        if metadata:\n",
    "            self.metadata.extend(metadata)\n",
    "        else:\n",
    "            self.metadata.extend([{} for _ in documents])\n",
    "        \n",
    "        print(f\"P≈ôid√°no {len(documents)} dokument≈Ø. Index obsahuje {self.index.ntotal} vektor≈Ø.\")\n",
    "    \n",
    "    def search(self, query, top_k=5):\n",
    "        \"\"\"\n",
    "        Vyhled√° nejpodobnƒõj≈°√≠ dokumenty.\n",
    "        \"\"\"\n",
    "        # Embedding dotazu\n",
    "        query_embedding = self.model.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "        \n",
    "        # Vyhled√°n√≠\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.documents):\n",
    "                results.append({\n",
    "                    \"document\": self.documents[idx],\n",
    "                    \"score\": float(score),\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"index\": int(idx)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Ulo≈æ√≠ index na disk.\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        faiss.write_index(self.index, f\"{path}.faiss\")\n",
    "        with open(f\"{path}.pkl\", 'wb') as f:\n",
    "            pickle.dump({'documents': self.documents, 'metadata': self.metadata}, f)\n",
    "        print(f\"Index ulo≈æen do {path}\")\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Naƒçte index z disku.\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        self.index = faiss.read_index(f\"{path}.faiss\")\n",
    "        with open(f\"{path}.pkl\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.documents = data['documents']\n",
    "            self.metadata = data['metadata']\n",
    "        print(f\"Index naƒçten z {path}. Obsahuje {self.index.ntotal} vektor≈Ø.\")\n",
    "\n",
    "# Test\n",
    "faiss_store = FAISSVectorStore()\n",
    "print(\"‚úÖ FAISS VectorStore p≈ôipraven\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P≈ôid√°n√≠ dokument≈Ø\n",
    "faiss_store.add(documents, metadata)\n",
    "\n",
    "# Test vyhled√°v√°n√≠\n",
    "query = \"Framework pro tvorbu API\"\n",
    "results = faiss_store.search(query, top_k=3)\n",
    "\n",
    "print(f\"\\nüîç Dotaz: {query}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. [{result['score']:.3f}] {result['document']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Praktick√° automatizace: FAQ Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAQBot:\n",
    "    \"\"\"\n",
    "    Jednoduch√Ω FAQ bot vyu≈æ√≠vaj√≠c√≠ s√©mantick√© vyhled√°v√°n√≠.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.vector_store = FAISSVectorStore()\n",
    "        self.threshold = threshold\n",
    "        self.qa_pairs = []\n",
    "    \n",
    "    def add_faq(self, question, answer, tags=None):\n",
    "        \"\"\"\n",
    "        P≈ôid√° ot√°zku a odpovƒõƒè do FAQ.\n",
    "        \"\"\"\n",
    "        self.qa_pairs.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"tags\": tags or []\n",
    "        })\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"\n",
    "        Vytvo≈ô√≠ index z p≈ôidan√Ωch FAQ.\n",
    "        \"\"\"\n",
    "        questions = [qa['question'] for qa in self.qa_pairs]\n",
    "        metadata = [\n",
    "            {\"answer\": qa['answer'], \"tags\": qa['tags']}\n",
    "            for qa in self.qa_pairs\n",
    "        ]\n",
    "        \n",
    "        self.vector_store.add(questions, metadata)\n",
    "        print(f\"Index vytvo≈ôen s {len(self.qa_pairs)} FAQ polo≈ækami.\")\n",
    "    \n",
    "    def ask(self, question):\n",
    "        \"\"\"\n",
    "        Odpov√≠ na ot√°zku pomoc√≠ FAQ.\n",
    "        \"\"\"\n",
    "        results = self.vector_store.search(question, top_k=3)\n",
    "        \n",
    "        if not results or results[0]['score'] < self.threshold:\n",
    "            return {\n",
    "                \"found\": False,\n",
    "                \"message\": \"Bohu≈æel jsem nena≈°el odpov√≠daj√≠c√≠ odpovƒõƒè. Zkuste formulovat ot√°zku jinak.\",\n",
    "                \"suggestions\": [r['document'] for r in results]\n",
    "            }\n",
    "        \n",
    "        best_match = results[0]\n",
    "        \n",
    "        return {\n",
    "            \"found\": True,\n",
    "            \"answer\": best_match['metadata']['answer'],\n",
    "            \"matched_question\": best_match['document'],\n",
    "            \"confidence\": best_match['score'],\n",
    "            \"tags\": best_match['metadata'].get('tags', []),\n",
    "            \"alternatives\": [\n",
    "                {\"question\": r['document'], \"score\": r['score']}\n",
    "                for r in results[1:]\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Vytvo≈ôen√≠ FAQ bota\n",
    "faq_bot = FAQBot(threshold=0.4)\n",
    "\n",
    "# P≈ôid√°n√≠ FAQ\n",
    "faqs = [\n",
    "    (\"Jak resetovat heslo?\", \"Pro reset hesla kliknƒõte na 'Zapomenut√© heslo' na p≈ôihla≈°ovac√≠ str√°nce a zadejte sv≈Øj email.\", [\"√∫ƒçet\", \"heslo\"]),\n",
    "    (\"Jak√© jsou zp≈Øsoby platby?\", \"P≈ôij√≠m√°me platby kartou (Visa, Mastercard), bankovn√≠m p≈ôevodem a Apple Pay.\", [\"platba\", \"objedn√°vka\"]),\n",
    "    (\"Jak dlouho trv√° doruƒçen√≠?\", \"Standardn√≠ doruƒçen√≠ trv√° 3-5 pracovn√≠ch dn≈Ø. Expresn√≠ doruƒçen√≠ je mo≈æn√© do 24 hodin.\", [\"doruƒçen√≠\", \"objedn√°vka\"]),\n",
    "    (\"Jak mohu vr√°tit zbo≈æ√≠?\", \"Zbo≈æ√≠ m≈Ø≈æete vr√°tit do 30 dn≈Ø od doruƒçen√≠. Vypl≈àte formul√°≈ô pro vr√°cen√≠ v sekci 'Moje objedn√°vky'.\", [\"vr√°cen√≠\", \"reklamace\"]),\n",
    "    (\"Kde najdu fakturu?\", \"Fakturu najdete v emailu s potvrzen√≠m objedn√°vky nebo v sekci 'Moje objedn√°vky' po p≈ôihl√°≈°en√≠.\", [\"faktura\", \"√∫ƒçet\"]),\n",
    "    (\"Jak zmƒõnit doruƒçovac√≠ adresu?\", \"Doruƒçovac√≠ adresu m≈Ø≈æete zmƒõnit v nastaven√≠ √∫ƒçtu nebo bƒõhem objedn√°vky p≈ôed potvrzen√≠m.\", [\"adresa\", \"doruƒçen√≠\"]),\n",
    "    (\"Je mo≈æn√© sledovat z√°silku?\", \"Ano, po odesl√°n√≠ obdr≈æ√≠te email s trackovac√≠m ƒç√≠slem pro sledov√°n√≠ z√°silky.\", [\"doruƒçen√≠\", \"tracking\"]),\n",
    "    (\"Jak kontaktovat z√°kaznickou podporu?\", \"Podporu m≈Ø≈æete kontaktovat emailem na podpora@example.cz nebo telefonicky na 800 123 456.\", [\"podpora\", \"kontakt\"]),\n",
    "]\n",
    "\n",
    "for q, a, tags in faqs:\n",
    "    faq_bot.add_faq(q, a, tags)\n",
    "\n",
    "# Vytvo≈ôen√≠ indexu\n",
    "faq_bot.build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FAQ bota\n",
    "test_questions = [\n",
    "    \"Zapomnƒõl jsem heslo, co m√°m dƒõlat?\",\n",
    "    \"ƒå√≠m m≈Ø≈æu zaplatit?\",\n",
    "    \"Kdy mi p≈ôijde bal√≠k?\",\n",
    "    \"Jak√© je poƒças√≠ dnes?\"  # Nesouvisej√≠c√≠ ot√°zka\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n‚ùì Ot√°zka: {question}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    response = faq_bot.ask(question)\n",
    "    \n",
    "    if response['found']:\n",
    "        print(f\"‚úÖ Odpovƒõƒè: {response['answer']}\")\n",
    "        print(f\"   Confidence: {response['confidence']:.1%}\")\n",
    "        print(f\"   Matched: {response['matched_question']}\")\n",
    "        if response['tags']:\n",
    "            print(f\"   Tags: {', '.join(response['tags'])}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {response['message']}\")\n",
    "        if response['suggestions']:\n",
    "            print(\"   Mo≈æn√° jste mƒõli na mysli:\")\n",
    "            for sugg in response['suggestions']:\n",
    "                print(f\"   ‚Ä¢ {sugg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Duplicate Detection (Detekce duplicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateDetector:\n",
    "    \"\"\"\n",
    "    Detekce duplicitn√≠ch nebo velmi podobn√Ωch text≈Ø.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0.85):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def find_duplicates(self, documents):\n",
    "        \"\"\"\n",
    "        Najde skupiny duplicitn√≠ch dokument≈Ø.\n",
    "        \"\"\"\n",
    "        # Embeddings\n",
    "        embeddings = self.model.encode(documents, show_progress_bar=True)\n",
    "        \n",
    "        # Matice podobnosti\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Hled√°n√≠ duplicit\n",
    "        duplicate_groups = []\n",
    "        used = set()\n",
    "        \n",
    "        for i in range(len(documents)):\n",
    "            if i in used:\n",
    "                continue\n",
    "            \n",
    "            group = [i]\n",
    "            for j in range(i + 1, len(documents)):\n",
    "                if j not in used and similarity_matrix[i, j] >= self.threshold:\n",
    "                    group.append(j)\n",
    "                    used.add(j)\n",
    "            \n",
    "            if len(group) > 1:\n",
    "                duplicate_groups.append({\n",
    "                    \"indices\": group,\n",
    "                    \"documents\": [documents[idx] for idx in group],\n",
    "                    \"similarity\": float(np.mean([similarity_matrix[group[0], idx] for idx in group[1:]]))\n",
    "                })\n",
    "                used.add(i)\n",
    "        \n",
    "        # Unik√°tn√≠ dokumenty\n",
    "        unique_indices = [i for i in range(len(documents)) if i not in used]\n",
    "        \n",
    "        return {\n",
    "            \"duplicate_groups\": duplicate_groups,\n",
    "            \"unique_count\": len(unique_indices),\n",
    "            \"duplicate_count\": len(used),\n",
    "            \"total\": len(documents)\n",
    "        }\n",
    "\n",
    "# Test\n",
    "detector = DuplicateDetector(threshold=0.8)\n",
    "\n",
    "test_docs = [\n",
    "    \"Python je skvƒõl√Ω programovac√≠ jazyk.\",\n",
    "    \"Python je v√Ωborn√Ω programovac√≠ jazyk pro zaƒç√°teƒçn√≠ky.\",  # Podobn√Ω\n",
    "    \"JavaScript se pou≈æ√≠v√° pro webov√Ω v√Ωvoj.\",\n",
    "    \"Pro v√Ωvoj webov√Ωch str√°nek se ƒçasto pou≈æ√≠v√° JavaScript.\",  # Podobn√Ω\n",
    "    \"Machine learning mƒõn√≠ svƒõt technologi√≠.\",\n",
    "    \"Koƒçky jsou skvƒõl√° dom√°c√≠ zv√≠≈ôata.\",\n",
    "    \"Strojov√© uƒçen√≠ revolucionizuje technologick√Ω pr≈Ømysl.\"  # Podobn√Ω k ML\n",
    "]\n",
    "\n",
    "result = detector.find_duplicates(test_docs)\n",
    "\n",
    "print(f\"Celkem dokument≈Ø: {result['total']}\")\n",
    "print(f\"Unik√°tn√≠ch: {result['unique_count']}\")\n",
    "print(f\"Duplicitn√≠ch: {result['duplicate_count']}\")\n",
    "\n",
    "print(\"\\nSkupiny duplicit:\")\n",
    "for i, group in enumerate(result['duplicate_groups'], 1):\n",
    "    print(f\"\\n  Skupina {i} (podobnost: {group['similarity']:.1%}):\")\n",
    "    for doc in group['documents']:\n",
    "        print(f\"    ‚Ä¢ {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Classification pomoc√≠ Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingClassifier:\n",
    "    \"\"\"\n",
    "    Zero-shot klasifikace pomoc√≠ embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "        self.class_embeddings = None\n",
    "        self.class_names = []\n",
    "    \n",
    "    def set_classes(self, class_descriptions: dict):\n",
    "        \"\"\"\n",
    "        Nastav√≠ t≈ô√≠dy pomoc√≠ textov√Ωch popis≈Ø.\n",
    "        \n",
    "        Args:\n",
    "            class_descriptions: {\"n√°zev_t≈ô√≠dy\": \"popis t≈ô√≠dy\"}\n",
    "        \"\"\"\n",
    "        self.class_names = list(class_descriptions.keys())\n",
    "        descriptions = list(class_descriptions.values())\n",
    "        \n",
    "        self.class_embeddings = self.model.encode(descriptions)\n",
    "        print(f\"Nastaveno {len(self.class_names)} t≈ô√≠d: {', '.join(self.class_names)}\")\n",
    "    \n",
    "    def classify(self, text, return_all=False):\n",
    "        \"\"\"\n",
    "        Klasifikuje text do jedn√© z t≈ô√≠d.\n",
    "        \"\"\"\n",
    "        if self.class_embeddings is None:\n",
    "            raise ValueError(\"Nejprve nastavte t≈ô√≠dy pomoc√≠ set_classes()\")\n",
    "        \n",
    "        # Embedding textu\n",
    "        text_embedding = self.model.encode([text])\n",
    "        \n",
    "        # Podobnost s t≈ô√≠dami\n",
    "        similarities = cosine_similarity(text_embedding, self.class_embeddings)[0]\n",
    "        \n",
    "        # Nejlep≈°√≠ t≈ô√≠da\n",
    "        best_idx = similarities.argmax()\n",
    "        \n",
    "        result = {\n",
    "            \"class\": self.class_names[best_idx],\n",
    "            \"confidence\": float(similarities[best_idx])\n",
    "        }\n",
    "        \n",
    "        if return_all:\n",
    "            result[\"all_scores\"] = {\n",
    "                name: float(score)\n",
    "                for name, score in zip(self.class_names, similarities)\n",
    "            }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def classify_batch(self, texts):\n",
    "        \"\"\"\n",
    "        Klasifikuje v√≠ce text≈Ø najednou.\n",
    "        \"\"\"\n",
    "        return [self.classify(text) for text in texts]\n",
    "\n",
    "# Test\n",
    "classifier = EmbeddingClassifier()\n",
    "\n",
    "# Definice t≈ô√≠d\n",
    "classes = {\n",
    "    \"technology\": \"Technology, computers, software, programming, AI, internet\",\n",
    "    \"sports\": \"Sports, athletics, games, competitions, teams, players\",\n",
    "    \"health\": \"Health, medicine, fitness, wellness, diseases, treatments\",\n",
    "    \"business\": \"Business, finance, economy, companies, markets, investments\",\n",
    "    \"entertainment\": \"Entertainment, movies, music, celebrities, arts, culture\"\n",
    "}\n",
    "\n",
    "classifier.set_classes(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test klasifikace\n",
    "test_texts = [\n",
    "    \"Apple announced new iPhone with improved camera and longer battery life.\",\n",
    "    \"The team won the championship after an exciting final match.\",\n",
    "    \"New study shows benefits of regular exercise for heart health.\",\n",
    "    \"Stock market reached record highs amid positive economic data.\",\n",
    "    \"The new movie broke box office records in its opening weekend.\"\n",
    "]\n",
    "\n",
    "print(\"V√Ωsledky klasifikace:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    result = classifier.classify(text, return_all=True)\n",
    "    \n",
    "    print(f\"\\nüìù {text[:60]}...\")\n",
    "    print(f\"   ‚û°Ô∏è {result['class'].upper()} ({result['confidence']:.1%})\")\n",
    "    \n",
    "    # Top 3 sk√≥re\n",
    "    sorted_scores = sorted(result['all_scores'].items(), key=lambda x: -x[1])[:3]\n",
    "    scores_str = \", \".join([f\"{k}: {v:.1%}\" for k, v in sorted_scores])\n",
    "    print(f\"   Scores: {scores_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrnut√≠\n",
    "\n",
    "V tomto notebooku jsme se nauƒçili:\n",
    "\n",
    "1. **Text Embeddings** - vektorov√© reprezentace textu pomoc√≠ Sentence Transformers\n",
    "2. **S√©mantick√© vyhled√°v√°n√≠** - hled√°n√≠ dokument≈Ø na z√°kladƒõ v√Ωznamu\n",
    "3. **V√≠cejazyƒçn√© embeddings** - pr√°ce s v√≠ce jazyky\n",
    "4. **Clustering dokument≈Ø** - automatick√© seskupov√°n√≠ podobn√Ωch text≈Ø\n",
    "5. **FAISS** - efektivn√≠ vektorov√© vyhled√°v√°n√≠ pro velk√© kolekce\n",
    "6. **FAQ Bot** - praktick√° aplikace s√©mantick√©ho vyhled√°v√°n√≠\n",
    "7. **Detekce duplicit** - hled√°n√≠ podobn√Ωch dokument≈Ø\n",
    "8. **Zero-shot klasifikace** - klasifikace bez tr√©nov√°n√≠\n",
    "\n",
    "### Dal≈°√≠ kroky\n",
    "- Notebook 12: Fine-tuning model≈Ø"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
