{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6540c7bc",
   "metadata": {},
   "source": [
    "# ğŸ“š Kapitola 38: LLM aplikace - produkÄnÃ­ nasazenÃ­\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h2 style=\"color: white; margin: 0;\">Blok 4 | EXPERT</h2>\n",
    "    <p style=\"color: white; margin: 10px 0;\">ğŸ“– VÃ½ukovÃ¡ kapitola</p>\n",
    "</div>\n",
    "\n",
    "## ğŸ¯ Co se nauÄÃ­te\n",
    "\n",
    "V tÃ©to kapitole se zamÄ›Å™Ã­me na nÃ¡sledujÃ­cÃ­ tÃ©mata:\n",
    "\n",
    "- **Streaming API endpoints**\n",
    "- **WebSocket integration**\n",
    "- **Queue management for LLM**\n",
    "- **Cost optimization strategies**\n",
    "- **Rate limiting implementation**\n",
    "- **Monitoring LLM metrics**\n",
    "- **Fallback strategies**\n",
    "\n",
    "## âš ï¸ PÅ™edpoklady\n",
    "Tato kapitola navazuje na kapitoly: 32, 18, 24\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495db61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Inicializace prostÅ™edÃ­\n",
    "# Tento kÃ³d nastavuje prostÅ™edÃ­ pro kapitolu 38\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Informace o prostÅ™edÃ­\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“š KAPITOLA 38: LLM aplikace - produkÄnÃ­ nasazenÃ­\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ Python verze: {sys.version}\")\n",
    "print(f\"ğŸ“… Datum spuÅ¡tÄ›nÃ­: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ’» OS: {os.name}\")\n",
    "print(f\"ğŸ“ PracovnÃ­ adresÃ¡Å™: {os.getcwd()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Instalace potÅ™ebnÃ½ch knihoven (odkomentujte podle potÅ™eby)\n",
    "# !pip install requests pandas numpy matplotlib\n",
    "# !pip install beautifulsoup4 sqlalchemy fastapi\n",
    "\n",
    "# Import zÃ¡kladnÃ­ch knihoven\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "print(\"âœ… ProstÅ™edÃ­ pÅ™ipraveno!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee62fe",
   "metadata": {},
   "source": [
    "## ğŸ“– TeoretickÃ¡ ÄÃ¡st\n",
    "\n",
    "<div style=\"background: #f0f4ff; padding: 20px; border-left: 5px solid #4a69bd; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">ğŸ“ ZÃ¡kladnÃ­ teorie a koncepty</h3>\n",
    "</div>\n",
    "\n",
    "# LLM aplikace â€“ produkÄnÃ­ nasazenÃ­\n",
    "\n",
    "## 1. Ãšvod a motivace\n",
    "\n",
    "V dneÅ¡nÃ­m svÄ›tÄ›, kde umÄ›lÃ¡ inteligence stÃ¡le vÃ­ce pronikÃ¡ do bÄ›Å¾nÃ©ho Å¾ivota, je klÃ­ÄovÃ© porozumÄ›t, jak efektivnÄ› nasazovat a spravovat LLM (Large Language Models) v produkÄnÃ­ch prostÅ™edÃ­ch. Tato kapitola se zamÄ›Å™uje na konkrÃ©tnÃ­ technologickÃ© aspekty a praktickÃ© implementace, kterÃ© jsou nezbytnÃ© pro spolehlivÃ© a eficientnÃ­ bÄ›h LLM aplikacÃ­ v reÃ¡lnÃ©m prostÅ™edÃ­.\n",
    "\n",
    "ZapojenÃ­ LLM do produkÄnÃ­ch systÃ©mÅ¯ se objevuje napÅ™Ã­klad u chatbotÅ¯, automatizovanÃ½ch zpracovÃ¡nÃ­ textu, generovÃ¡nÃ­ obsahu, kognitivnÃ­ch asistentÅ¯, a vÅ¡ech dalÅ¡Ã­ch aplikacÃ­ch, kterÃ© vyÅ¾adujÃ­ reÃ¡lnou odpovÄ›Ä na dotazy nebo pÅ™edpovÄ›di. V praxi se Äasto setkÃ¡vÃ¡me s problÃ©my jako je zpomalenÃ­ systÃ©mu, nesprÃ¡vnÃ¡ sprÃ¡va poÅ¾adavkÅ¯, zvÃ½Å¡enÃ© nÃ¡klady a absence monitoringu â€“ vÅ¡echno to mÅ¯Å¾e pÅ™inÃ©st nejen technickÃ©, ale i obchodnÃ­ problÃ©my.\n",
    "\n",
    "Po tÃ©to kapitole bude student schopen navrhnout a implementovat produkÄnÃ­ nasazenÃ­ LLM aplikacÃ­, kterÃ© jsou spolehlivÃ©, Å¡kÃ¡lovatelnÃ© a efektivnÃ­ z hlediska nÃ¡kladÅ¯ i vÃ½konu.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. HlavnÃ­ koncepty\n",
    "\n",
    "### Streaming API endpoints\n",
    "\n",
    "**Streaming API endpoints** umoÅ¾ÅˆujÃ­ posÃ­lat odpovÄ›di od LLM aplikace postupnÄ›, mÃ­sto toho, aby celÃ¡ odpovÄ›Ä byla pÅ™ipravena a odeslÃ¡na najednou. V praxi se tato metoda pouÅ¾Ã­vÃ¡ v reÃ¡lnÃ©m Äase, napÅ™Ã­klad pÅ™i generovÃ¡nÃ­ textu, kde uÅ¾ivatel vidÃ­ vÃ½stup postupnÄ›.\n",
    "\n",
    "- **PÅ™Ã­klad:** Chatbot, kterÃ½ vracÃ­ odpovÄ›Ä postupnÄ› â€“ tedy kaÅ¾dÃ© slovo nebo vÄ›ta se posÃ­lÃ¡ aÅ¾ po jejÃ­m vygenerovÃ¡nÃ­.\n",
    "- **VÃ½hody:**\n",
    "  - LepÅ¡Ã­ uÅ¾ivatelskÃ© zkuÅ¡enosti (ÄlovÄ›k vÃ­, Å¾e odpovÄ›Ä je generovÃ¡na)\n",
    "  - SnÃ­Å¾enÃ­ Äasu doby prvnÃ­ odpovÄ›di\n",
    "  - EfektivnÃ­ vyuÅ¾itÃ­ pamÄ›ti a zdrojÅ¯\n",
    "\n",
    "### WebSocket integration\n",
    "\n",
    "**WebSocket integrace** umoÅ¾Åˆuje komunikaci v reÃ¡lnÃ©m Äase mezi klientem a serverem, coÅ¾ je velmi uÅ¾iteÄnÃ© pÅ™i interaktivnÃ­ch aplikacÃ­ch jako jsou chaty nebo real-time AI asistenci.\n",
    "\n",
    "- **PÅ™Ã­klad:** UÅ¾ivatel zadÃ¡ dotaz a asistent odpovÃ­dÃ¡ v reÃ¡lnÃ©m Äase, pÅ™iÄemÅ¾ se odpovÄ›Ä posÃ­lÃ¡ po jednotlivÃ½ch ÄÃ¡stech.\n",
    "- **VÃ½hody:**\n",
    "  - NezbytnÃ© pro real-time zpracovÃ¡nÃ­\n",
    "  - UmoÅ¾Åˆuje vÃ­cekomunikaÄnÃ­ch kanÃ¡lÅ¯\n",
    "  - SnÃ­Å¾enÃ­ latence\n",
    "\n",
    "### Queue management for LLM\n",
    "\n",
    "**Å˜Ã­zenÃ­ fronty (queue management)** je klÃ­ÄovÃ© pro sprÃ¡vnÃ© rozloÅ¾enÃ­ poÅ¾adavkÅ¯ na LLM modely. V produkÄnÃ­m prostÅ™edÃ­ mÅ¯Å¾e bÃ½t nÃ¡vÅ¡tÄ›vnost velmi nestabilnÃ­, a proto je nutnÃ© efektivnÄ› Å™Ã­dit poÅ¾adavky.\n",
    "\n",
    "- **PÅ™Ã­klad:** PouÅ¾itÃ­ systÃ©mu jako je Celery nebo RabbitMQ pro zpracovÃ¡nÃ­ poÅ¾adavkÅ¯ v dÃ¡vkÃ¡ch.\n",
    "- **VÃ½hody:**\n",
    "  - ZabrÃ¡nÃ­ pÅ™etÃ­Å¾enÃ­ modelu\n",
    "  - UmoÅ¾Åˆuje Å™Ã­zenÃ­ priority poÅ¾adavkÅ¯\n",
    "  - ZajiÅ¡Å¥uje spolehlivost zpracovÃ¡nÃ­\n",
    "\n",
    "### Cost optimization strategies\n",
    "\n",
    "**Strategie optimalizace nÃ¡kladÅ¯** jsou dÅ¯leÅ¾itÃ©, protoÅ¾e LLM modely mohou bÃ½t velmi nÃ¡roÄnÃ© na vÃ½poÄetnÃ­ prostÅ™edky.\n",
    "\n",
    "- **PÅ™Ã­klady:**\n",
    "  - PouÅ¾itÃ­ modelÅ¯ s niÅ¾Å¡Ã­m rozliÅ¡enÃ­m pro jednoduÅ¡Å¡Ã­ Ãºkoly\n",
    "  - Implementace cacheovÃ¡nÃ­ ÄastÃ½ch dotazÅ¯\n",
    "  - VyuÅ¾itÃ­ batch processingu\n",
    "- **VÃ½hody:**\n",
    "  - SnÃ­Å¾enÃ­ nÃ¡kladÅ¯ na vÃ½poÄetnÃ­ prostÅ™edky\n",
    "  - ZlepÅ¡enÃ­ efektivity systÃ©mu\n",
    "\n",
    "### Rate limiting implementation\n",
    "\n",
    "**Implementace rate limitingu (omezenÃ­ poÄtu poÅ¾adavkÅ¯)** je klÃ­ÄovÃ¡ pro ochranu systÃ©mu pÅ™ed overloadem a zaruÄuje rovnomÄ›rnÃ© vyuÅ¾itÃ­ zdrojÅ¯.\n",
    "\n",
    "- **PÅ™Ã­klad:** OmezenÃ­ poÄtu dotazÅ¯ na uÅ¾ivatele na 100 za den.\n",
    "- **VÃ½hody:**\n",
    "  - Zamezuje pÅ™etÃ­Å¾enÃ­ systÃ©mu\n",
    "  - UmoÅ¾Åˆuje sprÃ¡vnÃ© rozdÄ›lenÃ­ zdrojÅ¯ mezi uÅ¾ivatele\n",
    "\n",
    "### Monitoring LLM metrics\n",
    "\n",
    "**MonitorovÃ¡nÃ­ metriky (LLM metrics)** umoÅ¾Åˆuje sledovat vÃ½konnost modelu, zpomalenÃ­, chyby a dalÅ¡Ã­ kritickÃ© indikÃ¡tory.\n",
    "\n",
    "- **PÅ™Ã­klad:** SledovÃ¡nÃ­ latence odpovÄ›di, poÄtu chyb, Äasu potÅ™ebnÃ©ho na vygenerovÃ¡nÃ­ odpovÄ›di.\n",
    "- **NÃ¡stroje:** Prometheus, Grafana, LangSmith\n",
    "- **VÃ½hody:**\n",
    "  - RychlÃ© zjiÅ¡tÄ›nÃ­ problÃ©mÅ¯\n",
    "  - UmoÅ¾Åˆuje optimalizaci vÃ½konu\n",
    "\n",
    "### Fallback strategies\n",
    "\n",
    "**ZÃ¡loÅ¾nÃ­ strategie (fallback strategies)** jsou implementovÃ¡ny pro pÅ™Ã­pad, kdy hlavnÃ­ model selÅ¾e. Tato strategie zajistÃ­, Å¾e aplikace bude nadÃ¡le fungovat.\n",
    "\n",
    "- **PÅ™Ã­klad:** Pokud LLM selÅ¾e, systÃ©m pouÅ¾ije jinÃ½ model nebo pÅ™epne na statickou odpovÄ›Ä.\n",
    "- **VÃ½hody:**\n",
    "  - ZvyÅ¡uje spolehlivost systÃ©mu\n",
    "  - UmoÅ¾Åˆuje pÅ™echod na zÃ¡loÅ¾nÃ­ Å™eÅ¡enÃ­\n",
    "\n",
    "---\n",
    "\n",
    "## 3. DÅ¯leÅ¾itÃ© detaily\n",
    "\n",
    "### ÄŒastÃ© chyby a jak se jim vyhnout\n",
    "\n",
    "- **NepÅ™imÄ›Å™enÃ© pouÅ¾itÃ­ modelu:** NÄ›kdy se pouÅ¾Ã­vajÃ­ nejvÄ›tÅ¡Ã­ modely i pro jednoduchÃ© Ãºkoly â€“ to zvyÅ¡uje nÃ¡klady bez zisku.\n",
    "- **NeefektivnÃ­ fronty:** Fronta je pÅ™etÃ­Å¾enÃ¡ a poÅ¾adavky se nezpracovÃ¡vajÃ­ vÄas.\n",
    "- **ChybÄ›jÃ­cÃ­ monitorovÃ¡nÃ­:** Bez monitoringu nenÃ­ moÅ¾nÃ© rychle detekovat problÃ©my.\n",
    "\n",
    "### Best practices\n",
    "\n",
    "- **ZabezpeÄenÃ­ pÅ™ed overloadem** pomocÃ­ limitovÃ¡nÃ­ poÄtu poÅ¾adavkÅ¯ a pouÅ¾itÃ­ front.\n",
    "- **Implementace cacheovÃ¡nÃ­** pro ÄastÃ© dotazy.\n",
    "- **PravidelnÃ¡ optimalizace modelu a monitorovÃ¡nÃ­ vÃ½konu.**\n",
    "\n",
    "### Performance tipy\n",
    "\n",
    "- VyuÅ¾itÃ­ batch processingu pÅ™i zpracovÃ¡nÃ­ vÄ›tÅ¡Ã­ho poÄtu dotazÅ¯.\n",
    "- ZvÃ¡Å¾enÃ­ pouÅ¾itÃ­ edge computingu pro snÃ­Å¾enÃ­ latence.\n",
    "- VhodnÃ© zvolenÃ­ modelu podle potÅ™eby â€“ ne vÅ¡echno musÃ­ bÃ½t \"nejlepÅ¡Ã­\" model.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. PropojenÃ­ s pÅ™edchozÃ­mi kapitolami\n",
    "\n",
    "Tato kapitola navazuje na pÅ™edchozÃ­ znalosti tÃ½kajÃ­cÃ­ se zÃ¡kladnÃ­ho vyuÅ¾Ã­vÃ¡nÃ­ LLM modelÅ¯, jejich trÃ©novÃ¡nÃ­, a implementace v prostÅ™edÃ­ Pythonu. Zde se zamÄ›Å™ujeme na **produkÄnÃ­ aspekty**, kterÃ© jsou klÃ­ÄovÃ© pro reÃ¡lnÃ© nasazenÃ­ systÃ©mÅ¯.\n",
    "\n",
    "DÅ¯leÅ¾itÃ© je si uvÄ›domit, Å¾e model je jenom zÃ¡klad â€“ v produkci musÃ­ bÃ½t sprÃ¡vnÄ› integrovÃ¡n s komunikaÄnÃ­mi protokoly, Å™Ã­zenÃ­m front a monitorovÃ¡nÃ­m. Tato kapitola tedy rozÅ¡iÅ™uje jiÅ¾ nauÄenÃ© koncepty a ukazuje, jak je aplikovat v praxi.\n",
    "\n",
    "---\n",
    "\n",
    "**ShrnutÃ­:** ProdukÄnÃ­ nasazenÃ­ LLM aplikacÃ­ vyÅ¾aduje zvlÃ¡dnutÃ­ Å™ady technickÃ½ch detailÅ¯ â€“ od optimalizace vÃ½konu po monitorovÃ¡nÃ­ systÃ©mu. Tato kapitola poskytuje nezbytnÃ© nÃ¡stroje a strategie pro bezpeÄnÃ© a efektivnÃ­ provoz LLM v reÃ¡lnÃ©m prostÅ™edÃ­.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7338f",
   "metadata": {},
   "source": [
    "## ğŸ’» PraktickÃ© pÅ™Ã­klady\n",
    "\n",
    "<div style=\"background: #e8f5e9; padding: 20px; border-left: 5px solid #4caf50; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">ğŸ‘¨â€ğŸ’» Hands-on pÅ™Ã­klady ke spuÅ¡tÄ›nÃ­</h3>\n",
    "    <p>NÃ¡sledujÃ­cÃ­ pÅ™Ã­klady si mÅ¯Å¾ete hned vyzkouÅ¡et. KaÅ¾dÃ½ pÅ™Ã­klad je samostatnÄ› spustitelnÃ½.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c47345",
   "metadata": {},
   "source": [
    "### PÅ™Ã­klad 1\n",
    "\n",
    "PÅ™Ã­klad pouÅ¾itÃ­:\n",
    "# add_to_queue(\"JakÃ½ je nejlepÅ¡Ã­ zpÅ¯sob vÃ½uky AI?\")\n",
    "```\n",
    "\n",
    "## OÄekÃ¡vanÃ½ vÃ½stup:\n",
    "```\n",
    "PÅ™idÃ¡n poÅ¾adavek do fronty s ID: task_1715683421\n",
    "ZpracovÃ¡vÃ¡m poÅ¾adavek: JakÃ½ je nejlepÅ¡Ã­ zpÅ¯sob vÃ½uky AI?\n",
    "PoÅ¾adavek zpracovÃ¡n.\n",
    "```\n",
    "\n",
    "## VysvÄ›tlenÃ­:\n",
    "Tento pÅ™Ã­klad ukazuje, jak mÅ¯Å¾e bÃ½t fronta poÅ¾adavkÅ¯ na LLM spravovÃ¡na pomocÃ­ Redis. PoÅ¾adavky se uklÃ¡dajÃ­ do seznamu `llm_queue`, a oddÄ›lenÃ½ proces je zpracovÃ¡vÃ¡ postupnÄ›, coÅ¾ umoÅ¾Åˆuje Å™Ã­zenÃ­ zatÃ­Å¾enÃ­ a efektivnÃ­ vyuÅ¾itÃ­ prostÅ™edkÅ¯.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Cost optimization strategies\n",
    "\n",
    "## NÃ¡zev pÅ™Ã­kladu:\n",
    "**CenovÃ¡ optimalizace LLM volÃ¡nÃ­ s cache a deduplikacÃ­**\n",
    "\n",
    "## Co demonstruje:\n",
    "Optimalizace nÃ¡kladÅ¯ tÃ­m, Å¾e se vÃ½stupy LLM uklÃ¡dajÃ­ do mezipamÄ›ti a duplicitnÃ­ poÅ¾adavky jsou vynechÃ¡ny.\n",
    "\n",
    "## KompletnÃ­ kÃ³d:\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "# JednoduchÃ¡ cache s ÄasovÃ½m limitem (napÅ™. 30 minut)\n",
    "cache = {}\n",
    "CACHE_TIMEOUT = 1800  # sekundy\n",
    "\n",
    "def get_cache_key(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    VytvoÅ™Ã­ hash pro prompt, kterÃ½ slouÅ¾Ã­ jako klÃ­Ä do cache.\n",
    "    \"\"\"\n",
    "    return hashlib.md5(prompt.encode()).hexdigest()\n",
    "\n",
    "def llm_call(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Simulace volÃ¡nÃ­ LLM. V reÃ¡lnÃ©m pÅ™Ã­padÄ› bude to API volÃ¡nÃ­.\n",
    "    \"\"\"\n",
    "    # Simulace zpracovÃ¡nÃ­\n",
    "    time.sleep(0.1)\n",
    "    return f\"VÃ½sledek pro '{prompt}'\"\n",
    "\n",
    "def optimized_llm_call(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    VolÃ¡nÃ­ LLM s cache a deduplikacÃ­.\n",
    "    \"\"\"\n",
    "    cache_key = get_cache_key(prompt)\n",
    "    \n",
    "    # Kontrola cache\n",
    "    if cache_key in cache:\n",
    "        result, timestamp = cache[cache_key]\n",
    "        if time.time() - timestamp < CACHE_TIMEOUT:\n",
    "            print(\"PouÅ¾Ã­vÃ¡m cache\")\n",
    "            return result\n",
    "    \n",
    "    # VolÃ¡nÃ­ LLM a uklÃ¡dÃ¡nÃ­ do cache\n",
    "    result = llm_call(prompt)\n",
    "    cache[cache_key] = (result, time.time())\n",
    "    \n",
    "    print(\"VolÃ¡nÃ­ LLM provedeno a vÃ½sledek uloÅ¾en do cache\")\n",
    "    return result\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a3323",
   "metadata": {},
   "source": [
    "### PÅ™Ã­klad 2\n",
    "\n",
    "PÅ™Ã­klad pouÅ¾itÃ­:\n",
    "print(optimized_llm_call(\"Jak se dÄ›lÃ¡ cache?\"))\n",
    "print(optimized_llm_call(\"Jak se dÄ›lÃ¡ cache?\"))  # PouÅ¾ije cache\n",
    "```\n",
    "\n",
    "## OÄekÃ¡vanÃ½ vÃ½stup:\n",
    "```\n",
    "VolÃ¡nÃ­ LLM provedeno a vÃ½sledek uloÅ¾en do cache\n",
    "VÃ½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e50a65",
   "metadata": {},
   "source": [
    "## ğŸ¯ CviÄenÃ­ a Ãºkoly\n",
    "\n",
    "<div style=\"background: #fff3e0; padding: 20px; border-left: 5px solid #ff9800; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">âœï¸ PraktickÃ¡ cviÄenÃ­ k procviÄenÃ­</h3>\n",
    "    <p>VyÅ™eÅ¡te nÃ¡sledujÃ­cÃ­ Ãºkoly. ZaÄnÄ›te od jednoduÅ¡Å¡Å¡Ã­ch a postupujte k sloÅ¾itÄ›jÅ¡Ã­m.</p>\n",
    "</div>\n",
    "\n",
    "# CviÄenÃ­ pro kapitolu: LLM aplikace - produkÄnÃ­ nasazenÃ­\n",
    "\n",
    "---\n",
    "\n",
    "## **CviÄenÃ­ 1: Migrace LLM modelu do produkÄnÃ­ho prostÅ™edÃ­ s vyuÅ¾itÃ­m Dockeru**\n",
    "\n",
    "### 1. **NÃ¡zev Ãºkolu**  \n",
    "NasazenÃ­ LLM modelu pomocÃ­ Dockeru v produkÄnÃ­m prostÅ™edÃ­\n",
    "\n",
    "### 2. **DetailnÃ­ zadÃ¡nÃ­**  \n",
    "VytvoÅ™te Docker image pro nasazenÃ­ LLM modelu (napÅ™. `llama3` nebo `mistral`) s podporou API endpointu pro inferenci, kterÃ½ bude kompatibilnÃ­ s produkÄnÃ­m prostÅ™edÃ­m. V rÃ¡mci toho implementujte zÃ¡kladnÃ­ ochrannÃ© mechanismy proti pÅ™etÃ­Å¾enÃ­ a logovÃ¡nÃ­.\n",
    "\n",
    "### 3. **VstupnÃ­ data/poÅ¾adavky**  \n",
    "- LLM model (napÅ™. `meta-llama/Meta-Llama-3-8B-Instruct`)\n",
    "- Python knihovna `transformers` a `torch`\n",
    "- API framework: FastAPI nebo Flask\n",
    "- VhodnÃ½ prostÅ™edÃ­ pro Docker\n",
    "\n",
    "### 4. **OÄekÃ¡vanÃ½ vÃ½stup**  \n",
    "- Docker image s nasazenÃ½m LLM modelem\n",
    "- API endpoint `/predict`, kterÃ½ pÅ™ijÃ­mÃ¡ JSON ve formÃ¡tu `{\"prompt\": \"text\"}` a vracÃ­ generovanÃ½ text\n",
    "- ZÃ¡kladnÃ­ ochrana proti zahlcenÃ­ (napÅ™. limit poÄtu poÅ¾adavkÅ¯ za sekundu)\n",
    "- LogovÃ¡nÃ­ poÅ¾adavkÅ¯ do souboru nebo konzole\n",
    "\n",
    "### 5. **Hints/NÃ¡povÄ›da**\n",
    "1. VyuÅ¾ijte `transformers` pro naÄtenÃ­ modelu a tokenizeru.\n",
    "2. Implementujte `FastAPI` endpoint `/predict`.\n",
    "3. PouÅ¾ijte `asyncio` pro asynchronnÃ­ zpracovÃ¡nÃ­ poÅ¾adavkÅ¯.\n",
    "4. PÅ™idejte middleware pro limit poÄtu poÅ¾adavkÅ¯ (napÅ™. `fastapi-limiter`).\n",
    "\n",
    "### 6. **Kostra Å™eÅ¡enÃ­**\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class PromptRequest(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "# NaÄtenÃ­ modelu a tokenizeru\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(request: PromptRequest):\n",
    "    inputs = tokenizer(request.prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return {\"response\": response}\n",
    "```\n",
    "\n",
    "### 7. **BonusovÃ© rozÅ¡Ã­Å™enÃ­**  \n",
    "- PÅ™idejte podporu pro batch inferenci (vÃ­ce promptÅ¯ najednou).\n",
    "- Implementujte model caching a load balancing mezi vÃ­ce GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## **CviÄenÃ­ 2: Implementace LLM API proxy s vyuÅ¾itÃ­m Redis cache a limitovÃ¡nÃ­m poÅ¾adavkÅ¯**\n",
    "\n",
    "### 1. **NÃ¡zev Ãºkolu**  \n",
    "VytvoÅ™enÃ­ API proxy pro LLM model s cache a rate-limitingem\n",
    "\n",
    "### 2. **DetailnÃ­ zadÃ¡nÃ­**  \n",
    "Naprogramujte API proxy, kterÃ¡ pÅ™ijÃ­mÃ¡ poÅ¾adavky na inferenci, uklÃ¡dÃ¡ vÃ½sledky do Redis cache, omezuje poÄet poÅ¾adavkÅ¯ a zajiÅ¡Å¥uje Å¡kÃ¡lovatelnost. ZajistÄ›te logovÃ¡nÃ­ a monitorovÃ¡nÃ­.\n",
    "\n",
    "### 3. **VstupnÃ­ data/poÅ¾adavky**  \n",
    "- LLM model (napÅ™. `mistralai/Mistral-7B-v0.1`)\n",
    "- Redis instance\n",
    "- FastAPI nebo Flask\n",
    "- Middleware pro rate-limiting\n",
    "\n",
    "### 4. **OÄekÃ¡vanÃ½ vÃ½stup**  \n",
    "- API endpoint `/api/inference`, kterÃ½ zpracovÃ¡vÃ¡ poÅ¾adavky a uklÃ¡dÃ¡ odpovÄ›di do Redis\n",
    "- OmezenÃ­ poÄtu poÅ¾adavkÅ¯ (napÅ™. 10 za sekundu)\n",
    "- Logy odeslanÃ½ch poÅ¾adavkÅ¯ a odpovÄ›dÃ­\n",
    "- MoÅ¾nost restartu bez ztrÃ¡ty cache\n",
    "\n",
    "### 5. **Hints/NÃ¡povÄ›da**\n",
    "1. PouÅ¾ijte Redis jako key-value ÃºloÅ¾iÅ¡tÄ› pro vÃ½sledky inferencÃ­.\n",
    "2. Implementujte `RateLimiter` middleware (napÅ™. pomocÃ­ `fastapi-limiter`).\n",
    "3. UjistÄ›te se, Å¾e poÅ¾adavky jsou kryptograficky hashovanÃ© pÅ™ed uloÅ¾enÃ­m do Redis.\n",
    "4. PÅ™idejte logovÃ¡nÃ­ pomocÃ­ `logging` modulu.\n",
    "\n",
    "### 6. **Kostra Å™eÅ¡enÃ­**\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import redis\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "app = FastAPI()\n",
    "redis_client = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "\n",
    "@app.post(\"/api/inference\")\n",
    "async def inference(prompt: str):\n",
    "    key = hashlib.md5(prompt.encode()).hexdigest()\n",
    "    cached = redis_client.get(key)\n",
    "    \n",
    "    if cached:\n",
    "        return {\"cached\": True, \"response\": json.loads(cached)}\n",
    "    \n",
    "    # Zde implementujte inferenci modelu\n",
    "    response = model_inference(prompt)\n",
    "    redis_client.setex(key, 3600, json.dumps(response))\n",
    "    return {\"cached\": False, \"response\": response}\n",
    "```\n",
    "\n",
    "### 7. **BonusovÃ© rozÅ¡Ã­Å™enÃ­**  \n",
    "- Implementujte dynamickÃ© Å¡kÃ¡lovÃ¡nÃ­ pomocÃ­ Kubernetes a HPA.\n",
    "- PÅ™idejte monitoring (napÅ™. Prometheus + Grafana).\n",
    "\n",
    "---\n",
    "\n",
    "## **CviÄenÃ­ 3: Å kÃ¡lovatelnÃ© LLM inference API s vyuÅ¾itÃ­m Kubernetes a GPU autoscalingu**\n",
    "\n",
    "### 1. **NÃ¡zev Ãºkolu**  \n",
    "NasazenÃ­ LLM modelu ve Å¡kÃ¡lovatelnÃ©m prostÅ™edÃ­ pomocÃ­ Kubernetes\n",
    "\n",
    "### 2. **DetailnÃ­ zadÃ¡nÃ­**  \n",
    "VytvoÅ™te Kubernetes deployment pro nasazenÃ­ LLM modelu na GPU, kterÃ½ bude automaticky Å¡kÃ¡lovat podle poÅ¾adavkÅ¯. Implementujte pÅ™Ã­stup k modelu pÅ™es API a zabezpeÄenÃ­ s vyuÅ¾itÃ­m Istio nebo Traefik.\n",
    "\n",
    "### 3. **VstupnÃ­ data/poÅ¾adavky**  \n",
    "- Kubernetes cluster (minikube, EKS, GKE)\n",
    "- Docker image s LLM modelem\n",
    "- GPU podporu (napÅ™. nvidia/gpu-device-plugin)\n",
    "- Helm chart nebo YAML konfigurace\n",
    "\n",
    "### 4. **OÄekÃ¡vanÃ½ vÃ½stup**  \n",
    "- Kubernetes deployment s LLM modelem na GPU\n",
    "- HPA (Horizontal Pod Autoscaler) pro automatickÃ© Å¡kÃ¡lovÃ¡nÃ­\n",
    "- Ingress controller pro pÅ™Ã­stup k API\n",
    "- ZabezpeÄenÃ­ pomocÃ­ TLS a autentizace\n",
    "\n",
    "### 5. **Hints/NÃ¡povÄ›da**\n",
    "1. PouÅ¾ijte `nvidia/k8s-device-plugin` pro podporu GPU.\n",
    "2. Nastavte HPA s metrikou CPU nebo poÄtem poÅ¾adavkÅ¯.\n",
    "3. Implementujte Ingress controller (Traefik, NGINX).\n",
    "4. ZajistÄ›te zabezpeÄenÃ­ pomocÃ­ cert-manager a Istio.\n",
    "\n",
    "### 6. **Kostra Å™eÅ¡enÃ­**\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: llm-deployment\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: llm-app\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: llm-app\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: llm-container\n",
    "        image: your-llm-image\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          limits:\n",
    "            nvidia.com/gpu: 1\n",
    "```\n",
    "\n",
    "### 7. **BonusovÃ© rozÅ¡Ã­Å™enÃ­**  \n",
    "- PÅ™idejte podporu pro model serving (napÅ™. `vLLM`, `Triton Inference Server`).\n",
    "- Nastavte CI/CD pipeline s GitHub Actions a ArgoCD.\n",
    "\n",
    "---\n",
    "\n",
    "## **CviÄenÃ­ 4: LLM inference monitoring a logovÃ¡nÃ­ v produkÄnÃ­m prostÅ™edÃ­**\n",
    "\n",
    "### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae7edcd",
   "metadata": {},
   "source": [
    "## ğŸ“š DalÅ¡Ã­ zdroje a materiÃ¡ly\n",
    "\n",
    "<div style=\"background: #e3f2fd; padding: 20px; border-left: 5px solid #2196f3; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">ğŸ”— DoporuÄenÃ© materiÃ¡ly k dalÅ¡Ã­mu studiu</h3>\n",
    "</div>\n",
    "\n",
    "# Seznam zdrojÅ¯ pro kapitolu: **LLM aplikace â€“ produkÄnÃ­ nasazenÃ­**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š 5 doporuÄenÃ½ch ÄlÃ¡nkÅ¯/tutoriÃ¡lÅ¯\n",
    "\n",
    "1. **[Deploying Large Language Models in Production â€“ A Complete Guide](https://www.anyscale.com/blog/deploying-llms-in-production)**  \n",
    "   *Popis:* KomplexnÃ­ prÅ¯vodce produkÄnÃ­m nasazenÃ­m LLM modelÅ¯, zahrnuje vÃ½bÄ›r platformy, optimalizaci a sprÃ¡vu infrastruktury.\n",
    "\n",
    "2. **[Building and Deploying LLM Applications with Hugging Face and FastAPI](https://huggingface.co/blog/deploy-llm-apps)**  \n",
    "   *Popis:* PraktickÃ½ nÃ¡vod na vytvoÅ™enÃ­ API pro LLM aplikace pomocÃ­ Hugging Face a FastAPI, vÄetnÄ› zÃ¡kladnÃ­ho deploymentu.\n",
    "\n",
    "3. **[LLM Deployment Best Practices](https://www.pinecone.io/learn/llm-deployment-best-practices/)**  \n",
    "   *Popis:* NejlepÅ¡Ã­ postupy pro deployment LLM modelÅ¯ v produkÄnÃ­m prostÅ™edÃ­, zahrnuje nÃ¡vody na Å¡kÃ¡lovÃ¡nÃ­, caching a monitorovÃ¡nÃ­.\n",
    "\n",
    "4. **[How to Deploy LLMs in Production: From Concept to Reality](https://towardsdatascience.com/how-to-deploy-llms-in-production-1d27e0a31187)**  \n",
    "   *Popis:* Krok za krokem prÅ¯vodce implementacÃ­ LLM aplikacÃ­, vÄetnÄ› problÃ©mÅ¯ s vÃ½konem a zabezpeÄenÃ­m.\n",
    "\n",
    "5. **[Production-Ready LLM Applications with LangChain and Docker](https://blog.langchain.dev/llm-production-deployment/)**  \n",
    "   *Popis:* PrÅ¯vodce vytvoÅ™enÃ­m produkÄnÃ­ch aplikacÃ­ pomocÃ­ LangChain a Docker, zamÄ›Å™enÃ½ na Å¡kÃ¡lovatelnost a udrÅ¾itelnost.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¥ 3 YouTube videa\n",
    "\n",
    "1. **[Deploying LLMs in Production with FastAPI and Hugging Face](https://www.youtube.com/watch?v=2X5gJFv6lWw)**  \n",
    "   *DÃ©lka:* 28 min  \n",
    "   *Popis:* Demonstrace, jak vytvoÅ™it a nasadit LLM aplikaci pomocÃ­ FastAPI a Hugging Face v produkÄnÃ­m prostÅ™edÃ­.\n",
    "\n",
    "2. **[LLM Deployment: From Local to Cloud (with AWS & Kubernetes)](https://www.youtube.com/watch?v=K0Qh7z4Vd5U)**  \n",
    "   *DÃ©lka:* 35 min  \n",
    "   *Popis:* PrÅ¯vodce deployem LLM modelÅ¯ na cloudu, vÄetnÄ› prÃ¡ce s AWS a Kubernetes.\n",
    "\n",
    "3. **[Building Scalable LLM Applications with LangChain and Streamlit](https://www.youtube.com/watch?v=7j0Y8v2mPZM)**  \n",
    "   *DÃ©lka:* 40 min  \n",
    "   *Popis:* Jak vytvÃ¡Å™et skalovatelnÃ© LLM aplikace pomocÃ­ LangChain a Streamlit, zÃ¡roveÅˆ ukÃ¡zka praktickÃ©ho nasazenÃ­.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“˜ 3 knihy nebo oficiÃ¡lnÃ­ dokumentace\n",
    "\n",
    "1. **[Hugging Face Documentation â€“ Model Deployment Guide](https://huggingface.co/docs/hub/en/model-deployment)**  \n",
    "   *Popis:* OficiÃ¡lnÃ­ prÅ¯vodce deploymentem modelÅ¯ na Hugging Face Hub, vÄetnÄ› nÃ¡strojÅ¯ pro produkÄnÃ­ nasazenÃ­.\n",
    "\n",
    "2. **[LangChain Documentation â€“ Deployment and Production](https://docs.langchain.com/docs/deployment/)**  \n",
    "   *Popis:* OficiÃ¡lnÃ­ dokumentace pro nasazovÃ¡nÃ­ aplikacÃ­ s LangChain, zahrnuje konfiguraci a integraci do produkÄnÃ­ch systÃ©mÅ¯.\n",
    "\n",
    "3. **[The AI Engineerâ€™s Guide to LLMs â€“ Chapter on Production Deployment](https://www.oreilly.com/library/view/the-ai-engineers-guide/9781098126354/ch06.html)**  \n",
    "   *Popis:* Kapitola o produkÄnÃ­m nasazenÃ­ LLM modelÅ¯ ve vÄ›tÅ¡Ã­ knize zamÄ›Å™enÃ© na inÅ¾enÃ½rstvÃ­ AI.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’» 2 prakt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34573bd5",
   "metadata": {},
   "source": [
    "## ğŸ“ ShrnutÃ­ kapitoly\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin: 30px 0;\">\n",
    "    <h3 style=\"color: white; margin-top: 0;\">âœ… Co jste se nauÄili</h3>\n",
    "    <ul style=\"list-style: none; padding-left: 0;\">\n",
    "        <li>âœ“ Streaming API endpoints</li>\n",
    "<li>âœ“ WebSocket integration</li>\n",
    "<li>âœ“ Queue management for LLM</li>\n",
    "<li>âœ“ Cost optimization strategies</li>\n",
    "<li>âœ“ Rate limiting implementation</li>\n",
    "    </ul>\n",
    "    \n",
    "    <h3 style=\"color: white;\">ğŸ¯ KlÃ­ÄovÃ© dovednosti</h3>\n",
    "    <p>Po dokonÄenÃ­ tÃ©to kapitoly byste mÄ›li bÃ½t schopni prakticky pouÅ¾Ã­t vÅ¡echny probranÃ© koncepty.</p>\n",
    "    \n",
    "    <h3 style='color: white;'>â¡ï¸ DalÅ¡Ã­ kapitola</h3><p>Kapitola 39 - pokraÄujte ve studiu!</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "*ğŸ“… Notebook vygenerovÃ¡n: 2025-09-29 13:36:54*  \n",
    "*ğŸ¤– GenerÃ¡tor: Comprehensive Colab Generator v2.0*  \n",
    "*ğŸ“š UÄebnice programovÃ¡nÃ­ - Od zÃ¡kladÅ¯ k AI*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371c2d93",
   "metadata": {},
   "source": [
    "## ğŸ§ª Sandbox - Prostor pro experimenty\n",
    "\n",
    "PouÅ¾ijte nÃ¡sledujÃ­cÃ­ buÅˆky pro vlastnÃ­ experimenty a testovÃ¡nÃ­:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª Zde mÅ¯Å¾ete experimentovat s kÃ³dem z kapitoly\n",
    "# NapiÅ¡te svÅ¯j kÃ³d zde:\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
