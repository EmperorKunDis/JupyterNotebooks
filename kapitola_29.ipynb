{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kapitola 29: Vicevrstve neuronove site - Jak vyresit problem XOR\n",
        "\n",
        "---\n",
        "\n",
        "## Co se naucite\n",
        "\n",
        "V teto kapitole se naucite:\n",
        "- Proc jednoduchy **Perceptron selze** na problemu XOR\n",
        "- Co je **vicevrstvy Perceptron (MLP)**\n",
        "- Jak funguje **skryta vrstva**\n",
        "- Princip **zpetneho sireni chyby (backpropagation)**\n",
        "- Naprogramovat **MLP od nuly** pro XOR\n",
        "- Pouzit **sklearn MLPClassifier** pro klasifikaci\n",
        "\n",
        "## Proc vicevrstve site?\n",
        "\n",
        "V minule kapitole jsme vytvorili Perceptron, ktery zvladl AND a OR.\n",
        "Ale na XOR selhal - jednou primkou nelze oddelit diagonalni body.\n",
        "\n",
        "**Reseni**: Vice neuronu ve vrstvach = schopnost ucit se slozitejsi vzory!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Instalace a import knihoven"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Instalace knihoven\n",
        "!pip install numpy matplotlib scikit-learn -q\n",
        "\n",
        "print(\"Knihovny nainstalovany!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import knihoven\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "\n",
        "# Nastaveni\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"NumPy verze:\", np.__version__)\n",
        "print(\"Prostredi pripraveno!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Problem XOR - Proc Perceptron selhava\n",
        "\n",
        "XOR (exclusive or) vraci 1 pouze kdyz vstupy jsou **ruzne**:\n",
        "\n",
        "| Vstup 1 | Vstup 2 | XOR |\n",
        "|---------|---------|-----|\n",
        "| 0 | 0 | 0 |\n",
        "| 0 | 1 | 1 |\n",
        "| 1 | 0 | 1 |\n",
        "| 1 | 1 | 0 |\n",
        "\n",
        "**Proc Perceptron selhava?** Nelze nakreslit jednu primku, ktera by oddělila body!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Vizualizace problemu XOR\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Body\n",
        "colors = ['red' if y == 0 else 'blue' for y in y_xor]\n",
        "plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=300, edgecolors='black', linewidths=2, zorder=5)\n",
        "\n",
        "# Popisky\n",
        "labels = ['(0,0) = 0', '(0,1) = 1', '(1,0) = 1', '(1,1) = 0']\n",
        "positions = [(0.12, 0.12), (-0.15, 1.12), (1.12, -0.12), (1.12, 1.12)]\n",
        "for i, (label, pos) in enumerate(zip(labels, positions)):\n",
        "    plt.annotate(label, xy=(X_xor[i, 0], X_xor[i, 1]), xytext=pos, fontsize=12, fontweight='bold')\n",
        "\n",
        "# Neuspesne pokusy o oddeleni\n",
        "x_line = np.linspace(-0.3, 1.3, 100)\n",
        "plt.plot(x_line, 0.5 * np.ones_like(x_line), 'g--', alpha=0.5, linewidth=2, label='Horizontalni primka - NEFUNGUJE')\n",
        "plt.plot(0.5 * np.ones_like(x_line), x_line, 'm--', alpha=0.5, linewidth=2, label='Vertikalni primka - NEFUNGUJE')\n",
        "plt.plot(x_line, x_line, 'orange', linestyle='--', alpha=0.5, linewidth=2, label='Diagonalni primka - NEFUNGUJE')\n",
        "\n",
        "plt.xlim(-0.3, 1.5)\n",
        "plt.ylim(-0.3, 1.5)\n",
        "plt.xlabel('Vstup 1', fontsize=12)\n",
        "plt.ylabel('Vstup 2', fontsize=12)\n",
        "plt.title('Problem XOR: Nelze oddelit jednou primkou!', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"XOR je NELINEARNI problem - jeden neuron ho nezvladne!\")\n",
        "print(\"Potrebujeme VICEVRSTOU NEURONOVOU SIT (MLP).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Reseni: Vicevrstvy Perceptron (MLP)\n",
        "\n",
        "**Multi-Layer Perceptron (MLP)** pridava mezi vstup a vystup **skrytou vrstvu**.\n",
        "\n",
        "```\n",
        "VSTUPNI VRSTVA     SKRYTA VRSTVA     VYSTUPNI VRSTVA\n",
        "   (2 neurony)      (2 neurony)        (1 neuron)\n",
        "       \n",
        "      x1  ─────┐     ┌───  h1  ───┐\n",
        "               ├─────┤            ├───── y\n",
        "      x2  ─────┘     └───  h2  ───┘\n",
        "```\n",
        "\n",
        "**Kazdy neuron ve skryte vrstve** se nauci rozpoznavat jiny vzor.\n",
        "**Vystupni neuron** kombinuje jejich vystupy do finalniho rozhodnuti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Vizualizace architektury MLP\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "ax.axis('off')\n",
        "\n",
        "# Pozice neuronu\n",
        "layer_positions = {\n",
        "    'input': [(0.1, 0.7), (0.1, 0.3)],\n",
        "    'hidden': [(0.5, 0.8), (0.5, 0.5), (0.5, 0.2)],\n",
        "    'output': [(0.9, 0.5)]\n",
        "}\n",
        "\n",
        "# Vykreslit neurony\n",
        "for layer, positions in layer_positions.items():\n",
        "    for pos in positions:\n",
        "        if layer == 'input':\n",
        "            color = 'lightgreen'\n",
        "        elif layer == 'hidden':\n",
        "            color = 'lightblue'\n",
        "        else:\n",
        "            color = 'lightyellow'\n",
        "        circle = plt.Circle(pos, 0.06, color=color, ec='black', lw=2)\n",
        "        ax.add_patch(circle)\n",
        "\n",
        "# Spojeni mezi vrstvami\n",
        "for inp in layer_positions['input']:\n",
        "    for hid in layer_positions['hidden']:\n",
        "        ax.annotate('', xy=(hid[0]-0.06, hid[1]), xytext=(inp[0]+0.06, inp[1]),\n",
        "                   arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
        "\n",
        "for hid in layer_positions['hidden']:\n",
        "    for out in layer_positions['output']:\n",
        "        ax.annotate('', xy=(out[0]-0.06, out[1]), xytext=(hid[0]+0.06, hid[1]),\n",
        "                   arrowprops=dict(arrowstyle='->', color='gray', lw=1.5))\n",
        "\n",
        "# Popisky\n",
        "ax.text(0.1, 0.95, 'VSTUPNI VRSTVA', ha='center', fontsize=12, fontweight='bold', color='green')\n",
        "ax.text(0.5, 0.95, 'SKRYTA VRSTVA', ha='center', fontsize=12, fontweight='bold', color='blue')\n",
        "ax.text(0.9, 0.95, 'VYSTUPNI VRSTVA', ha='center', fontsize=12, fontweight='bold', color='orange')\n",
        "\n",
        "ax.text(0.1, 0.7, 'x₁', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "ax.text(0.1, 0.3, 'x₂', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "ax.text(0.5, 0.8, 'h₁', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "ax.text(0.5, 0.5, 'h₂', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "ax.text(0.5, 0.2, 'h₃', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "ax.text(0.9, 0.5, 'y', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Poznamky\n",
        "ax.text(0.5, 0.05, 'Kazdy neuron ve skryte vrstve se nauci rozpoznavat jiny vzor.\\nVystupni neuron kombinuje jejich vystupy.', \n",
        "       ha='center', fontsize=11, style='italic')\n",
        "\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0, 1)\n",
        "plt.title('Architektura MLP: 2 vstupy - 3 skryte neurony - 1 vystup', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Aktivacni funkce Sigmoid\n",
        "\n",
        "Misto skokove funkce pouzijeme **Sigmoid** - hladka krivka mezi 0 a 1.\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "**Proc Sigmoid?**\n",
        "- Hladka derivace umoznuje uceni pomoci gradientu\n",
        "- Vraci pravdepodobnost (hodnota mezi 0 a 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sigmoid funkce a jeji derivace\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid aktivacni funkce\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip pro numericou stabilitu\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"Derivace sigmoidu - potrebna pro backpropagation\"\"\"\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Vizualizace\n",
        "x = np.linspace(-6, 6, 100)\n",
        "y_sig = sigmoid(x)\n",
        "y_deriv = sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x, y_sig, 'b-', linewidth=3, label='σ(x)')\n",
        "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
        "plt.fill_between(x, 0, y_sig, alpha=0.2)\n",
        "plt.title('Sigmoid funkce', fontweight='bold', fontsize=14)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('σ(x)')\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x, y_deriv, 'r-', linewidth=3, label=\"σ'(x)\")\n",
        "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
        "plt.fill_between(x, 0, y_deriv, alpha=0.2, color='red')\n",
        "plt.title('Derivace sigmoidu', fontweight='bold', fontsize=14)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel(\"σ'(x)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Sigmoid: Hladky prechod mezi 0 a 1\")\n",
        "print(\"Derivace: Maximalni v bode 0, umoznuje gradientni uceni\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Backpropagation - Zpetne sireni chyby\n",
        "\n",
        "**Backpropagation** je algoritmus pro uceni vicevrstvych siti:\n",
        "\n",
        "1. **Forward pass**: Posli data siti a vypocti vystup\n",
        "2. **Vypocti chybu**: Porovnej vystup s ocekavanou hodnotou\n",
        "3. **Backward pass**: Propaguj chybu zpet a uprav vahy\n",
        "\n",
        "**Analogie**: Ticha posta pozpatku s obviňovanim - kazda vrstva dostane svuj \"dil viny\" za chybu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Vizualizace backpropagation\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Forward pass\n",
        "ax1 = axes[0]\n",
        "ax1.set_xlim(0, 1)\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.axis('off')\n",
        "ax1.set_title('1. FORWARD PASS', fontweight='bold', fontsize=12, color='green')\n",
        "\n",
        "# Neurony\n",
        "for pos in [(0.2, 0.7), (0.2, 0.3)]:\n",
        "    ax1.add_patch(plt.Circle(pos, 0.08, color='lightgreen', ec='black', lw=2))\n",
        "for pos in [(0.5, 0.7), (0.5, 0.3)]:\n",
        "    ax1.add_patch(plt.Circle(pos, 0.08, color='lightblue', ec='black', lw=2))\n",
        "ax1.add_patch(plt.Circle((0.8, 0.5), 0.08, color='lightyellow', ec='black', lw=2))\n",
        "\n",
        "# Sipky doprava\n",
        "for start in [(0.2, 0.7), (0.2, 0.3)]:\n",
        "    for end in [(0.5, 0.7), (0.5, 0.3)]:\n",
        "        ax1.annotate('', xy=(end[0]-0.08, end[1]), xytext=(start[0]+0.08, start[1]),\n",
        "                    arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
        "for start in [(0.5, 0.7), (0.5, 0.3)]:\n",
        "    ax1.annotate('', xy=(0.72, 0.5), xytext=(start[0]+0.08, start[1]),\n",
        "                arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
        "\n",
        "ax1.text(0.5, 0.1, 'Data jdou DOPREDU\\npres sit', ha='center', fontsize=10)\n",
        "\n",
        "# Compute error\n",
        "ax2 = axes[1]\n",
        "ax2.set_xlim(0, 1)\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.axis('off')\n",
        "ax2.set_title('2. VYPOCET CHYBY', fontweight='bold', fontsize=12, color='red')\n",
        "\n",
        "ax2.add_patch(plt.Circle((0.5, 0.6), 0.1, color='lightyellow', ec='black', lw=2))\n",
        "ax2.text(0.5, 0.6, 'y', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "ax2.text(0.5, 0.35, 'Chyba = y_skutecne - y_predikovane', ha='center', fontsize=11, color='red')\n",
        "ax2.text(0.5, 0.2, 'E = (target - output)²', ha='center', fontsize=11)\n",
        "\n",
        "# Backward pass\n",
        "ax3 = axes[2]\n",
        "ax3.set_xlim(0, 1)\n",
        "ax3.set_ylim(0, 1)\n",
        "ax3.axis('off')\n",
        "ax3.set_title('3. BACKWARD PASS', fontweight='bold', fontsize=12, color='blue')\n",
        "\n",
        "# Neurony\n",
        "for pos in [(0.2, 0.7), (0.2, 0.3)]:\n",
        "    ax3.add_patch(plt.Circle(pos, 0.08, color='lightgreen', ec='black', lw=2))\n",
        "for pos in [(0.5, 0.7), (0.5, 0.3)]:\n",
        "    ax3.add_patch(plt.Circle(pos, 0.08, color='lightblue', ec='black', lw=2))\n",
        "ax3.add_patch(plt.Circle((0.8, 0.5), 0.08, color='lightyellow', ec='black', lw=2))\n",
        "\n",
        "# Sipky DOLEVA (zpetne sireni)\n",
        "for end in [(0.5, 0.7), (0.5, 0.3)]:\n",
        "    ax3.annotate('', xy=(end[0]+0.08, end[1]), xytext=(0.72, 0.5),\n",
        "                arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
        "for end in [(0.2, 0.7), (0.2, 0.3)]:\n",
        "    for start in [(0.5, 0.7), (0.5, 0.3)]:\n",
        "        ax3.annotate('', xy=(end[0]+0.08, end[1]), xytext=(start[0]-0.08, start[1]),\n",
        "                    arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
        "\n",
        "ax3.text(0.5, 0.1, 'Chyba jde ZPET\\na upravuje vahy', ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Implementace MLP od nuly pro XOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MLP_XOR:\n",
        "    \"\"\"\n",
        "    Vicevrstvy Perceptron (MLP) pro problem XOR\n",
        "    Architektura: 2 vstupy -> 2 skryte neurony -> 1 vystup\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, learning_rate=0.5):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_history = []\n",
        "        \n",
        "        # Inicializace vah (nahodne hodnoty mezi -1 a 1)\n",
        "        # Vahy mezi vstupni a skrytou vrstvou (2 vstupy -> 2 skryte neurony)\n",
        "        self.weights_input_hidden = np.random.uniform(-1, 1, (2, 2))\n",
        "        self.bias_hidden = np.random.uniform(-1, 1, (1, 2))\n",
        "        \n",
        "        # Vahy mezi skrytou a vystupni vrstvou (2 skryte -> 1 vystup)\n",
        "        self.weights_hidden_output = np.random.uniform(-1, 1, (2, 1))\n",
        "        self.bias_output = np.random.uniform(-1, 1, (1, 1))\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "    \n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass - vypocet vystupu site\n",
        "        \"\"\"\n",
        "        # Skryta vrstva\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
        "        \n",
        "        # Vystupni vrstva\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.predicted_output = self.sigmoid(self.output_input)\n",
        "        \n",
        "        return self.predicted_output\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        Backward pass - zpetne sireni chyby a aktualizace vah\n",
        "        \"\"\"\n",
        "        # Chyba na vystupu\n",
        "        output_error = y - self.predicted_output\n",
        "        output_delta = output_error * self.sigmoid_derivative(self.predicted_output)\n",
        "        \n",
        "        # Chyba ve skryte vrstve\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
        "        \n",
        "        # Aktualizace vah (gradientni sestup)\n",
        "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * self.learning_rate\n",
        "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n",
        "        \n",
        "        self.weights_input_hidden += X.T.dot(hidden_delta) * self.learning_rate\n",
        "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n",
        "        \n",
        "        return np.mean(np.abs(output_error))\n",
        "    \n",
        "    def fit(self, X, y, epochs=10000, print_every=1000):\n",
        "        \"\"\"\n",
        "        Trenovani site\n",
        "        \"\"\"\n",
        "        y = y.reshape(-1, 1)  # Zajistit spravny tvar\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            self.forward(X)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss = self.backward(X, y)\n",
        "            self.loss_history.append(loss)\n",
        "            \n",
        "            if (epoch + 1) % print_every == 0:\n",
        "                print(f\"Epocha {epoch + 1}/{epochs}, Chyba: {loss:.4f}\")\n",
        "        \n",
        "        print(\"\\nTrenovani dokonceno!\")\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predikce (zaokrouhleno na 0 nebo 1)\n",
        "        \"\"\"\n",
        "        output = self.forward(X)\n",
        "        return np.round(output).astype(int).flatten()\n",
        "\n",
        "print(\"Trida MLP_XOR definovana!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Trenovani MLP na XOR\n",
        "print(\"=\" * 50)\n",
        "print(\"TRENOVANI MLP NA PROBLEMU XOR\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Data\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Vytvoreni a trenovani modelu\n",
        "mlp = MLP_XOR(learning_rate=0.5)\n",
        "mlp.fit(X_xor, y_xor, epochs=10000, print_every=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Testovani modelu\n",
        "print(\"\\nVYSLEDKY:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"Vstup    | Ocekavano | Predikovano | Spravne?\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "predictions = mlp.predict(X_xor)\n",
        "raw_output = mlp.forward(X_xor)\n",
        "\n",
        "for i in range(len(X_xor)):\n",
        "    correct = \"ANO\" if predictions[i] == y_xor[i] else \"NE\"\n",
        "    print(f\"[{X_xor[i, 0]}, {X_xor[i, 1]}]   |     {y_xor[i]}     |      {predictions[i]}      |   {correct}\")\n",
        "\n",
        "print(\"-\" * 40)\n",
        "accuracy = np.mean(predictions == y_xor) * 100\n",
        "print(f\"Presnost: {accuracy:.0f}%\")\n",
        "\n",
        "print(f\"\\nSurove vystupy site (pred zaokrouhlenim):\")\n",
        "for i in range(len(X_xor)):\n",
        "    print(f\"  [{X_xor[i, 0]}, {X_xor[i, 1]}] -> {raw_output[i, 0]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Vizualizace uceni\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(mlp.loss_history, 'b-', linewidth=1)\n",
        "plt.title('Pruběh učení MLP', fontweight='bold', fontsize=14)\n",
        "plt.xlabel('Epocha')\n",
        "plt.ylabel('Chyba (MAE)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Rozhodovaci hranice\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# Vytvoreni mrizky\n",
        "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
        "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = mlp.forward(grid_points)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Vykresleni\n",
        "plt.contourf(xx, yy, Z, levels=50, cmap='RdYlBu', alpha=0.8)\n",
        "plt.colorbar(label='Vystup site')\n",
        "plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
        "\n",
        "# Body\n",
        "colors = ['red' if y == 0 else 'blue' for y in y_xor]\n",
        "plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidths=2, zorder=5)\n",
        "\n",
        "plt.title('Rozhodovaci hranice MLP pro XOR', fontweight='bold', fontsize=14)\n",
        "plt.xlabel('Vstup 1')\n",
        "plt.ylabel('Vstup 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"MLP dokaze nareslit NELINEARNI rozhodovaci hranici!\")\n",
        "print(\"Tim vyresi problem XOR, ktery jednoduchy Perceptron nezvladne.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. MLPClassifier ze scikit-learn\n",
        "\n",
        "V praxi pouzivame hotove implementace. Scikit-learn ma `MLPClassifier`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MLPClassifier ze sklearn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Vytvoreni modelu\n",
        "mlp_sklearn = MLPClassifier(\n",
        "    hidden_layer_sizes=(4,),  # 1 skryta vrstva se 4 neurony\n",
        "    activation='relu',         # Aktivacni funkce (relu je moderni)\n",
        "    solver='adam',             # Optimalizator\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Trenovani\n",
        "mlp_sklearn.fit(X_xor, y_xor)\n",
        "\n",
        "# Predikce\n",
        "predictions_sklearn = mlp_sklearn.predict(X_xor)\n",
        "\n",
        "print(\"MLPClassifier ze sklearn:\")\n",
        "print(f\"Predikce: {predictions_sklearn}\")\n",
        "print(f\"Ocekavano: {y_xor}\")\n",
        "print(f\"Presnost: {mlp_sklearn.score(X_xor, y_xor) * 100:.0f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Mini-projekt: Klasifikace slozitejsich dat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generovani slozitejsich dat - \"mesice\"\n",
        "X_moons, y_moons = make_moons(n_samples=200, noise=0.15, random_state=42)\n",
        "\n",
        "print(f\"Dataset 'Moons': {len(X_moons)} vzorku\")\n",
        "\n",
        "# Vizualizace\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_moons[y_moons==0, 0], X_moons[y_moons==0, 1], c='red', label='Trida 0', alpha=0.6)\n",
        "plt.scatter(X_moons[y_moons==1, 0], X_moons[y_moons==1, 1], c='blue', label='Trida 1', alpha=0.6)\n",
        "plt.title('Dataset \"Moons\" - nelinearni problem', fontweight='bold')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Porovnani ruznych architektur MLP\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Rozdeleni dat\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_moons, y_moons, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Ruzne architektury\n",
        "architektury = [\n",
        "    (2,),           # 1 vrstva, 2 neurony\n",
        "    (10,),          # 1 vrstva, 10 neuronu\n",
        "    (10, 5),        # 2 vrstvy: 10 a 5 neuronu\n",
        "    (20, 10, 5),    # 3 vrstvy\n",
        "]\n",
        "\n",
        "print(\"Porovnani architektur MLP na datasetu 'Moons':\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"{'Architektura':<20} | {'Train Acc':<10} | {'Test Acc':<10}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "modely = []\n",
        "for arch in architektury:\n",
        "    model = MLPClassifier(\n",
        "        hidden_layer_sizes=arch,\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        max_iter=1000,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    train_acc = model.score(X_train, y_train) * 100\n",
        "    test_acc = model.score(X_test, y_test) * 100\n",
        "    \n",
        "    print(f\"{str(arch):<20} | {train_acc:>8.1f}% | {test_acc:>8.1f}%\")\n",
        "    modely.append((arch, model, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Vizualizace rozhodovacich hranic pro ruzne architektury\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "for idx, (arch, model, acc) in enumerate(modely):\n",
        "    ax = axes.flatten()[idx]\n",
        "    \n",
        "    # Mrizka\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.linspace(X_moons[:, 0].min()-0.5, X_moons[:, 0].max()+0.5, 100),\n",
        "        np.linspace(X_moons[:, 1].min()-0.5, X_moons[:, 1].max()+0.5, 100)\n",
        "    )\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Vykresleni\n",
        "    ax.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
        "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
        "    ax.scatter(X_moons[y_moons==0, 0], X_moons[y_moons==0, 1], c='red', s=20, alpha=0.6)\n",
        "    ax.scatter(X_moons[y_moons==1, 0], X_moons[y_moons==1, 1], c='blue', s=20, alpha=0.6)\n",
        "    \n",
        "    ax.set_title(f'Architektura: {arch}\\nTest presnost: {acc:.1f}%', fontweight='bold')\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "plt.suptitle('Vliv architektury MLP na rozhodovaci hranici', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Vice neuronu = slozitejsi rozhodovaci hranice\")\n",
        "print(\"Ale pozor na PREUCENI (overfitting)!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Shrnuti a klicove poznatky\n",
        "\n",
        "### Co jsme se naucili:\n",
        "\n",
        "| Koncept | Popis |\n",
        "|---------|-------|\n",
        "| **MLP** | Vicevrstvy Perceptron - site s 1+ skrytymi vrstvami |\n",
        "| **Skryta vrstva** | Neurony mezi vstupem a vystupem |\n",
        "| **Sigmoid** | Hladka aktivacni funkce (0-1) |\n",
        "| **Backpropagation** | Algoritmus pro uceni - zpetne sireni chyby |\n",
        "| **XOR problem** | Nelinearni problem, ktery MLP zvladne |\n",
        "\n",
        "### Klicove poznatky:\n",
        "\n",
        "1. **Perceptron je omezeny** na linearni problemy\n",
        "2. **MLP pridava skryte vrstvy** pro uceni nelinearnich vzoru\n",
        "3. **Backpropagation** umoznuje uceni hlubokych siti\n",
        "4. **Vice neuronu** = schopnost ucit se slozitejsi vzory\n",
        "5. **Pozor na overfitting** - prilis velka sit muze preucit data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Kviz: Otestujte sve znalosti"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def kviz_mlp():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"KVIZ: Vicevrstve neuronove site\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    otazky = [\n",
        "        {\n",
        "            \"otazka\": \"Proc jednoduchy Perceptron nemuze vyresit XOR?\",\n",
        "            \"moznosti\": [\"a) Ma malo vah\", \"b) XOR neni linearni separovatelny\", \"c) Chybi mu bias\", \"d) Je prilis rychly\"],\n",
        "            \"spravne\": \"b\",\n",
        "            \"vysvetleni\": \"XOR nelze oddelit jednou primkou, proto Perceptron selhava.\"\n",
        "        },\n",
        "        {\n",
        "            \"otazka\": \"Co je skryta vrstva v MLP?\",\n",
        "            \"moznosti\": [\"a) Vrstva, ktera je neviditelna\", \"b) Vrstva mezi vstupem a vystupem\", \"c) Vystupni vrstva\", \"d) Vstupni vrstva\"],\n",
        "            \"spravne\": \"b\",\n",
        "            \"vysvetleni\": \"Skryta vrstva je mezi vstupni a vystupni vrstvou a uci se mezivystupy.\"\n",
        "        },\n",
        "        {\n",
        "            \"otazka\": \"Co je backpropagation?\",\n",
        "            \"moznosti\": [\"a) Smer toku dat\", \"b) Algoritmus pro zpetne sireni chyby\", \"c) Typ aktivacni funkce\", \"d) Zpusob generovani dat\"],\n",
        "            \"spravne\": \"b\",\n",
        "            \"vysvetleni\": \"Backpropagation siri chybu zpet siti a upravuje vahy.\"\n",
        "        },\n",
        "        {\n",
        "            \"otazka\": \"Proc pouzivame sigmoid misto skokove funkce?\",\n",
        "            \"moznosti\": [\"a) Je rychlejsi\", \"b) Ma hladkou derivaci pro gradientni uceni\", \"c) Vraci vetsi hodnoty\", \"d) Je jednodussi\"],\n",
        "            \"spravne\": \"b\",\n",
        "            \"vysvetleni\": \"Sigmoid ma hladkou derivaci, ktera umoznuje gradientni sestup.\"\n",
        "        },\n",
        "        {\n",
        "            \"otazka\": \"Co se stane kdyz pouzijeme prilis velkou sit?\",\n",
        "            \"moznosti\": [\"a) Vzdy se zlepsi presnost\", \"b) Muze dojit k overfittingu\", \"c) Nic se nedeje\", \"d) Sit se neda natrenovat\"],\n",
        "            \"spravne\": \"b\",\n",
        "            \"vysvetleni\": \"Prilis velka sit se muze preucit na trenovacich datech.\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    skore = 0\n",
        "    \n",
        "    for i, q in enumerate(otazky, 1):\n",
        "        print(f\"\\n{i}. {q['otazka']}\")\n",
        "        for m in q['moznosti']:\n",
        "            print(f\"   {m}\")\n",
        "        \n",
        "        odpoved = input(\"   Vase odpoved (a/b/c/d): \").strip().lower()\n",
        "        \n",
        "        if odpoved == q['spravne']:\n",
        "            print(\"   Spravne!\")\n",
        "            skore += 1\n",
        "        else:\n",
        "            print(f\"   Spatne. Spravna odpoved je {q['spravne']})\")\n",
        "        print(f\"   Vysvetleni: {q['vysvetleni']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"VYSLEDEK: {skore}/{len(otazky)} spravnych odpovedi\")\n",
        "\n",
        "# Spustit kviz\n",
        "kviz_mlp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Vyzva pro vas\n",
        "\n",
        "1. **Experimentujte s learning rate**: Zkuste 0.1, 0.01, 1.0 - jak to ovlivni uceni?\n",
        "2. **Zmente pocet epoch**: Co se stane pri 1000, 50000?\n",
        "3. **Circles dataset**: Pouzijte `make_circles()` a zkuste ruzne architektury"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prostor pro experimenty\n",
        "# X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=42)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Zaver\n",
        "\n",
        "V teto kapitole jste:\n",
        "\n",
        "- Pochopili **limitace jednoducheho Perceptronu**\n",
        "- Naucili se, co je **vicevrstvy Perceptron (MLP)**\n",
        "- Implementovali **MLP od nuly** pro problem XOR\n",
        "- Pochopili princip **backpropagation**\n",
        "- Pouzili **sklearn MLPClassifier**\n",
        "\n",
        "**MLP je zakladem modernich neuronovych siti!** GPT, obrazove modely - vse stoji na techto principech.\n",
        "\n",
        "---\n",
        "*Kapitola 29 | Vicevrstve neuronove site | AI Akademie*"
      ]
    }
  ]
}
