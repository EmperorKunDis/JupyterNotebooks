{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Multimod√°ln√≠ Modely\n",
    "\n",
    "**Autor:** Praut s.r.o. - AI Integration & Business Automation\n",
    "\n",
    "V tomto notebooku se nauƒç√≠me:\n",
    "- Z√°klady multimod√°ln√≠ch model≈Ø (text + obraz)\n",
    "- Image Captioning - popis obr√°zk≈Ø\n",
    "- Visual Question Answering (VQA)\n",
    "- CLIP - spojen√≠ textu a obrazu\n",
    "- Multimod√°ln√≠ embeddingy\n",
    "- Praktick√© aplikace v byznysu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalace pot≈ôebn√Ωch knihoven\n",
    "!pip install transformers accelerate pillow requests torch torchvision -q\n",
    "!pip install open_clip_torch sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    AutoModelForCausalLM,\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    BlipForQuestionAnswering,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Kontrola GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Pou≈æ√≠v√°m za≈ô√≠zen√≠: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pomocn√© funkce pro naƒç√≠t√°n√≠ obr√°zk≈Ø\n",
    "\n",
    "def load_image_from_url(url: str) -> Image.Image:\n",
    "    \"\"\"Naƒçte obr√°zek z URL.\"\"\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "def show_image(image: Image.Image, title: str = \"\"):\n",
    "    \"\"\"Zobraz√≠ obr√°zek.\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def show_images_grid(images: List[Image.Image], titles: List[str] = None, cols: int = 3):\n",
    "    \"\"\"Zobraz√≠ m≈ô√≠≈æku obr√°zk≈Ø.\"\"\"\n",
    "    n = len(images)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "    axes = axes.flatten() if n > 1 else [axes]\n",
    "    \n",
    "    for i, (ax, img) in enumerate(zip(axes, images)):\n",
    "        ax.imshow(img)\n",
    "        if titles and i < len(titles):\n",
    "            ax.set_title(titles[i], fontsize=10)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # Skr√Ωt pr√°zdn√© subploty\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Captioning - BLIP\n",
    "\n",
    "BLIP (Bootstrapping Language-Image Pre-training) je model pro:\n",
    "- Generov√°n√≠ popis≈Ø obr√°zk≈Ø\n",
    "- Visual Question Answering\n",
    "- Image-Text Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioner:\n",
    "    \"\"\"T≈ô√≠da pro generov√°n√≠ popis≈Ø obr√°zk≈Ø.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"Salesforce/blip-image-captioning-base\"):\n",
    "        print(f\"Naƒç√≠t√°m model: {model_name}\")\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "        print(\"Model naƒçten!\")\n",
    "    \n",
    "    def generate_caption(self, \n",
    "                        image: Image.Image,\n",
    "                        conditional_text: str = None,\n",
    "                        max_length: int = 50,\n",
    "                        num_beams: int = 4) -> str:\n",
    "        \"\"\"\n",
    "        Generuje popis obr√°zku.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL obr√°zek\n",
    "            conditional_text: Voliteln√Ω prefix pro ≈ô√≠zen√© generov√°n√≠\n",
    "            max_length: Maxim√°ln√≠ d√©lka popisu\n",
    "            num_beams: Poƒçet beams pro beam search\n",
    "        \"\"\"\n",
    "        # P≈ô√≠prava vstupu\n",
    "        if conditional_text:\n",
    "            inputs = self.processor(image, conditional_text, return_tensors=\"pt\").to(device)\n",
    "        else:\n",
    "            inputs = self.processor(image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generov√°n√≠\n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams\n",
    "            )\n",
    "        \n",
    "        # Dek√≥dov√°n√≠\n",
    "        caption = self.processor.decode(output[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    \n",
    "    def generate_multiple_captions(self,\n",
    "                                   image: Image.Image,\n",
    "                                   num_captions: int = 3,\n",
    "                                   temperature: float = 0.9) -> List[str]:\n",
    "        \"\"\"Generuje v√≠ce r≈Øzn√Ωch popis≈Ø.\"\"\"\n",
    "        inputs = self.processor(image, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        captions = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_captions):\n",
    "                output = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=50,\n",
    "                    do_sample=True,\n",
    "                    temperature=temperature,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "                caption = self.processor.decode(output[0], skip_special_tokens=True)\n",
    "                if caption not in captions:\n",
    "                    captions.append(caption)\n",
    "        \n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializace captioneru\n",
    "captioner = ImageCaptioner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test na uk√°zkov√Ωch obr√°zc√≠ch\n",
    "\n",
    "test_urls = [\n",
    "    \"https://images.unsplash.com/photo-1587300003388-59208cc962cb?w=400\",  # Pes\n",
    "    \"https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=400\",  # Kancel√°≈ô\n",
    "    \"https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=400\",  # Dashboard\n",
    "]\n",
    "\n",
    "print(\"Generuji popisy obr√°zk≈Ø...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, url in enumerate(test_urls, 1):\n",
    "    try:\n",
    "        image = load_image_from_url(url)\n",
    "        caption = captioner.generate_caption(image)\n",
    "        \n",
    "        print(f\"\\nObr√°zek {i}:\")\n",
    "        print(f\"Popis: {caption}\")\n",
    "        \n",
    "        # Zobrazen√≠\n",
    "        show_image(image, caption)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Chyba p≈ôi zpracov√°n√≠ obr√°zku {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ≈ò√≠zen√© generov√°n√≠ s prefixem\n",
    "\n",
    "url = \"https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=400\"\n",
    "image = load_image_from_url(url)\n",
    "\n",
    "prefixes = [\n",
    "    \"a photo of\",\n",
    "    \"this image shows\",\n",
    "    \"in this picture we can see\",\n",
    "    \"the scene depicts\"\n",
    "]\n",
    "\n",
    "print(\"≈ò√≠zen√© generov√°n√≠ s r≈Øzn√Ωmi prefixy:\")\n",
    "print(\"=\" * 60)\n",
    "for prefix in prefixes:\n",
    "    caption = captioner.generate_caption(image, conditional_text=prefix)\n",
    "    print(f\"Prefix '{prefix}': {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visual Question Answering (VQA)\n",
    "\n",
    "VQA umo≈æ≈àuje kl√°st ot√°zky o obsahu obr√°zku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualQA:\n",
    "    \"\"\"T≈ô√≠da pro Visual Question Answering.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"Salesforce/blip-vqa-base\"):\n",
    "        print(f\"Naƒç√≠t√°m VQA model: {model_name}\")\n",
    "        self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "        self.model = BlipForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "        print(\"VQA model naƒçten!\")\n",
    "    \n",
    "    def answer_question(self, \n",
    "                       image: Image.Image, \n",
    "                       question: str,\n",
    "                       max_length: int = 30) -> str:\n",
    "        \"\"\"Odpov√≠d√° na ot√°zku o obr√°zku.\"\"\"\n",
    "        \n",
    "        inputs = self.processor(image, question, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model.generate(**inputs, max_length=max_length)\n",
    "        \n",
    "        answer = self.processor.decode(output[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "    \n",
    "    def batch_qa(self, \n",
    "                image: Image.Image, \n",
    "                questions: List[str]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Odpov√≠d√° na v√≠ce ot√°zek.\"\"\"\n",
    "        results = []\n",
    "        for q in questions:\n",
    "            answer = self.answer_question(image, q)\n",
    "            results.append({\"question\": q, \"answer\": answer})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializace VQA\n",
    "vqa = VisualQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VQA\n",
    "\n",
    "url = \"https://images.unsplash.com/photo-1587300003388-59208cc962cb?w=400\"\n",
    "image = load_image_from_url(url)\n",
    "\n",
    "questions = [\n",
    "    \"What animal is in the picture?\",\n",
    "    \"What color is it?\",\n",
    "    \"Is it inside or outside?\",\n",
    "    \"How many animals are there?\",\n",
    "    \"What is the animal doing?\"\n",
    "]\n",
    "\n",
    "print(\"Visual Question Answering:\")\n",
    "print(\"=\" * 60)\n",
    "show_image(image, \"Test Image\")\n",
    "\n",
    "results = vqa.batch_qa(image, questions)\n",
    "for r in results:\n",
    "    print(f\"Q: {r['question']}\")\n",
    "    print(f\"A: {r['answer']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CLIP - Contrastive Language-Image Pre-training\n",
    "\n",
    "CLIP spojuje text a obr√°zky ve spoleƒçn√©m embedding prostoru:\n",
    "- Vyhled√°v√°n√≠ obr√°zk≈Ø podle textu\n",
    "- Zero-shot klasifikace obr√°zk≈Ø\n",
    "- Similarity matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPSystem:\n",
    "    \"\"\"T≈ô√≠da pro pr√°ci s CLIP modelem.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        print(f\"Naƒç√≠t√°m CLIP model: {model_name}\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "        print(\"CLIP model naƒçten!\")\n",
    "    \n",
    "    def get_image_embeddings(self, images: List[Image.Image]) -> torch.Tensor:\n",
    "        \"\"\"Vytvo≈ô√≠ embeddingy pro obr√°zky.\"\"\"\n",
    "        inputs = self.processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.model.get_image_features(**inputs)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return image_features\n",
    "    \n",
    "    def get_text_embeddings(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Vytvo≈ô√≠ embeddingy pro texty.\"\"\"\n",
    "        inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        return text_features\n",
    "    \n",
    "    def compute_similarity(self, \n",
    "                          images: List[Image.Image], \n",
    "                          texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Vypoƒç√≠t√° podobnost mezi obr√°zky a texty.\"\"\"\n",
    "        image_emb = self.get_image_embeddings(images)\n",
    "        text_emb = self.get_text_embeddings(texts)\n",
    "        \n",
    "        # Kosinov√° podobnost\n",
    "        similarity = (image_emb @ text_emb.T).cpu().numpy()\n",
    "        return similarity * 100  # ≈†k√°lov√°n√≠ pro lep≈°√≠ ƒçitelnost\n",
    "    \n",
    "    def zero_shot_classify(self, \n",
    "                          image: Image.Image, \n",
    "                          labels: List[str],\n",
    "                          template: str = \"a photo of {}\") -> Dict[str, float]:\n",
    "        \"\"\"Zero-shot klasifikace obr√°zku.\"\"\"\n",
    "        \n",
    "        # Vytvo≈ôen√≠ textov√Ωch popis≈Ø z template\n",
    "        texts = [template.format(label) for label in labels]\n",
    "        \n",
    "        # V√Ωpoƒçet podobnosti\n",
    "        similarity = self.compute_similarity([image], texts)[0]\n",
    "        \n",
    "        # Softmax pro pravdƒõpodobnosti\n",
    "        probs = np.exp(similarity) / np.exp(similarity).sum()\n",
    "        \n",
    "        return {label: float(prob) for label, prob in zip(labels, probs)}\n",
    "    \n",
    "    def search_images(self, \n",
    "                     images: List[Image.Image],\n",
    "                     query: str,\n",
    "                     top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"Vyhled√° obr√°zky podle textov√©ho dotazu.\"\"\"\n",
    "        \n",
    "        similarity = self.compute_similarity(images, [query])[:, 0]\n",
    "        \n",
    "        # Se≈ôazen√≠ podle podobnosti\n",
    "        indices = np.argsort(similarity)[::-1][:top_k]\n",
    "        \n",
    "        return [(int(idx), float(similarity[idx])) for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializace CLIP\n",
    "clip_system = CLIPSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot klasifikace\n",
    "\n",
    "url = \"https://images.unsplash.com/photo-1587300003388-59208cc962cb?w=400\"\n",
    "image = load_image_from_url(url)\n",
    "\n",
    "labels = [\"dog\", \"cat\", \"bird\", \"horse\", \"elephant\", \"fish\"]\n",
    "\n",
    "results = clip_system.zero_shot_classify(image, labels)\n",
    "\n",
    "print(\"Zero-shot klasifikace:\")\n",
    "print(\"=\" * 40)\n",
    "show_image(image, \"Test Image\")\n",
    "\n",
    "for label, prob in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    bar = \"‚ñà\" * int(prob * 30)\n",
    "    print(f\"{label:12} {prob:6.1%} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-Text Similarity Matrix\n",
    "\n",
    "# Naƒçten√≠ nƒõkolika obr√°zk≈Ø\n",
    "image_urls = [\n",
    "    \"https://images.unsplash.com/photo-1587300003388-59208cc962cb?w=300\",  # Pes\n",
    "    \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=300\",  # Koƒçka\n",
    "    \"https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=300\",  # Kancel√°≈ô\n",
    "]\n",
    "\n",
    "images = [load_image_from_url(url) for url in image_urls]\n",
    "texts = [\"a dog\", \"a cat\", \"an office workspace\", \"a landscape\"]\n",
    "\n",
    "# V√Ωpoƒçet similarity matrix\n",
    "similarity = clip_system.compute_similarity(images, texts)\n",
    "\n",
    "# Vizualizace\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(similarity, cmap='Blues')\n",
    "plt.colorbar(label='Similarity Score')\n",
    "plt.xticks(range(len(texts)), texts, rotation=45, ha='right')\n",
    "plt.yticks(range(len(images)), [f'Image {i+1}' for i in range(len(images))])\n",
    "plt.title('Image-Text Similarity Matrix')\n",
    "\n",
    "# Hodnoty do bunƒõk\n",
    "for i in range(len(images)):\n",
    "    for j in range(len(texts)):\n",
    "        plt.text(j, i, f'{similarity[i,j]:.1f}', \n",
    "                ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Zobrazen√≠ obr√°zk≈Ø\n",
    "show_images_grid(images, [f\"Image {i+1}\" for i in range(len(images))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multimod√°ln√≠ Vyhled√°v√°n√≠\n",
    "\n",
    "Kombinace CLIP s vektorovou datab√°z√≠ pro vyhled√°v√°n√≠ obr√°zk≈Ø."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalSearchEngine:\n",
    "    \"\"\"Vyhled√°vaƒç obr√°zk≈Ø s multimod√°ln√≠mi embeddingy.\"\"\"\n",
    "    \n",
    "    def __init__(self, clip_model: CLIPSystem = None):\n",
    "        self.clip = clip_model or CLIPSystem()\n",
    "        self.images: List[Image.Image] = []\n",
    "        self.image_embeddings: torch.Tensor = None\n",
    "        self.metadata: List[Dict] = []\n",
    "    \n",
    "    def add_images(self, \n",
    "                  images: List[Image.Image], \n",
    "                  metadata: List[Dict] = None):\n",
    "        \"\"\"P≈ôid√° obr√°zky do indexu.\"\"\"\n",
    "        \n",
    "        # V√Ωpoƒçet embedding≈Ø\n",
    "        new_embeddings = self.clip.get_image_embeddings(images)\n",
    "        \n",
    "        # P≈ôid√°n√≠ do seznamu\n",
    "        self.images.extend(images)\n",
    "        \n",
    "        # Metadata\n",
    "        if metadata:\n",
    "            self.metadata.extend(metadata)\n",
    "        else:\n",
    "            self.metadata.extend([{} for _ in images])\n",
    "        \n",
    "        # Aktualizace embedding≈Ø\n",
    "        if self.image_embeddings is None:\n",
    "            self.image_embeddings = new_embeddings\n",
    "        else:\n",
    "            self.image_embeddings = torch.cat([self.image_embeddings, new_embeddings])\n",
    "        \n",
    "        print(f\"P≈ôid√°no {len(images)} obr√°zk≈Ø. Celkem: {len(self.images)}\")\n",
    "    \n",
    "    def search_by_text(self, \n",
    "                      query: str, \n",
    "                      top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Vyhled√° obr√°zky podle textov√©ho dotazu.\"\"\"\n",
    "        \n",
    "        if not self.images:\n",
    "            return []\n",
    "        \n",
    "        # Text embedding\n",
    "        text_emb = self.clip.get_text_embeddings([query])\n",
    "        \n",
    "        # Podobnost\n",
    "        similarity = (self.image_embeddings @ text_emb.T).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Top-k v√Ωsledky\n",
    "        top_indices = np.argsort(similarity)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                \"index\": int(idx),\n",
    "                \"score\": float(similarity[idx]),\n",
    "                \"image\": self.images[idx],\n",
    "                \"metadata\": self.metadata[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def search_by_image(self, \n",
    "                       query_image: Image.Image, \n",
    "                       top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"Vyhled√° podobn√© obr√°zky.\"\"\"\n",
    "        \n",
    "        if not self.images:\n",
    "            return []\n",
    "        \n",
    "        # Image embedding\n",
    "        query_emb = self.clip.get_image_embeddings([query_image])\n",
    "        \n",
    "        # Podobnost\n",
    "        similarity = (self.image_embeddings @ query_emb.T).squeeze().cpu().numpy()\n",
    "        \n",
    "        # Top-k v√Ωsledky\n",
    "        top_indices = np.argsort(similarity)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                \"index\": int(idx),\n",
    "                \"score\": float(similarity[idx]),\n",
    "                \"image\": self.images[idx],\n",
    "                \"metadata\": self.metadata[idx]\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvo≈ôen√≠ vyhled√°vaƒçe\n",
    "search_engine = MultimodalSearchEngine(clip_system)\n",
    "\n",
    "# P≈ôid√°n√≠ obr√°zk≈Ø s metadaty\n",
    "sample_urls = [\n",
    "    (\"https://images.unsplash.com/photo-1587300003388-59208cc962cb?w=300\", {\"category\": \"animals\", \"tags\": [\"dog\", \"pet\"]}),\n",
    "    (\"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=300\", {\"category\": \"animals\", \"tags\": [\"cat\", \"pet\"]}),\n",
    "    (\"https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=300\", {\"category\": \"work\", \"tags\": [\"office\", \"computer\"]}),\n",
    "    (\"https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=300\", {\"category\": \"work\", \"tags\": [\"dashboard\", \"data\"]}),\n",
    "    (\"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=300\", {\"category\": \"nature\", \"tags\": [\"mountains\", \"landscape\"]}),\n",
    "]\n",
    "\n",
    "images = []\n",
    "metadata = []\n",
    "for url, meta in sample_urls:\n",
    "    try:\n",
    "        img = load_image_from_url(url)\n",
    "        images.append(img)\n",
    "        metadata.append(meta)\n",
    "    except:\n",
    "        print(f\"Nepoda≈ôilo se naƒç√≠st: {url}\")\n",
    "\n",
    "search_engine.add_images(images, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vyhled√°v√°n√≠ podle textu\n",
    "\n",
    "queries = [\n",
    "    \"a cute pet\",\n",
    "    \"working at desk with computer\",\n",
    "    \"beautiful nature scenery\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nüîç Dotaz: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results = search_engine.search_by_text(query, top_k=3)\n",
    "    \n",
    "    result_images = [r[\"image\"] for r in results]\n",
    "    result_titles = [f\"Score: {r['score']:.2f}\\n{r['metadata'].get('tags', [])}\" for r in results]\n",
    "    \n",
    "    show_images_grid(result_images, result_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Praktick√© Aplikace pro Byznys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductAnalyzer:\n",
    "    \"\"\"Analyz√°tor produktov√Ωch fotografi√≠ pro e-commerce.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.captioner = captioner\n",
    "        self.vqa = vqa\n",
    "        self.clip = clip_system\n",
    "    \n",
    "    def analyze_product(self, image: Image.Image) -> Dict[str, Any]:\n",
    "        \"\"\"Komplexn√≠ anal√Ωza produktov√© fotografie.\"\"\"\n",
    "        \n",
    "        results = {\n",
    "            \"description\": None,\n",
    "            \"attributes\": {},\n",
    "            \"categories\": {},\n",
    "            \"quality_checks\": {}\n",
    "        }\n",
    "        \n",
    "        # 1. Popis produktu\n",
    "        results[\"description\"] = self.captioner.generate_caption(\n",
    "            image, \n",
    "            conditional_text=\"this product is\"\n",
    "        )\n",
    "        \n",
    "        # 2. Atributy produktu\n",
    "        attribute_questions = [\n",
    "            (\"color\", \"What is the main color?\"),\n",
    "            (\"material\", \"What material is this made of?\"),\n",
    "            (\"brand_visible\", \"Is there a brand logo visible?\"),\n",
    "        ]\n",
    "        \n",
    "        for attr_name, question in attribute_questions:\n",
    "            results[\"attributes\"][attr_name] = self.vqa.answer_question(image, question)\n",
    "        \n",
    "        # 3. Kategorizace\n",
    "        categories = [\"electronics\", \"clothing\", \"furniture\", \"food\", \"toys\", \"sports\"]\n",
    "        results[\"categories\"] = self.clip.zero_shot_classify(image, categories)\n",
    "        \n",
    "        # 4. Quality checks\n",
    "        quality_labels = [\"professional photo\", \"amateur photo\", \"blurry image\", \"well-lit image\"]\n",
    "        results[\"quality_checks\"] = self.clip.zero_shot_classify(image, quality_labels)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_listing(self, image: Image.Image) -> str:\n",
    "        \"\"\"Generuje n√°vrh produktov√©ho listingu.\"\"\"\n",
    "        \n",
    "        analysis = self.analyze_product(image)\n",
    "        \n",
    "        # Hlavn√≠ kategorie\n",
    "        top_category = max(analysis[\"categories\"].items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        listing = f\"\"\"\n",
    "üì¶ PRODUKTOV√ù LISTING\n",
    "{'='*40}\n",
    "\n",
    "üìù Popis:\n",
    "{analysis['description']}\n",
    "\n",
    "üè∑Ô∏è Kategorie: {top_category}\n",
    "\n",
    "üìã Atributy:\n",
    "- Barva: {analysis['attributes'].get('color', 'N/A')}\n",
    "- Materi√°l: {analysis['attributes'].get('material', 'N/A')}\n",
    "\n",
    "üìä Kvalita fotografie:\n",
    "- Profesion√°ln√≠: {analysis['quality_checks'].get('professional photo', 0):.1%}\n",
    "- Dobr√© osvƒõtlen√≠: {analysis['quality_checks'].get('well-lit image', 0):.1%}\n",
    "\"\"\"\n",
    "        return listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test produktov√©ho analyz√°toru\n",
    "\n",
    "analyzer = ProductAnalyzer()\n",
    "\n",
    "# Test na obr√°zku\n",
    "url = \"https://images.unsplash.com/photo-1523275335684-37898b6baf30?w=400\"  # Hodinky\n",
    "try:\n",
    "    product_image = load_image_from_url(url)\n",
    "    show_image(product_image, \"Produkt k anal√Ωze\")\n",
    "    \n",
    "    listing = analyzer.generate_listing(product_image)\n",
    "    print(listing)\n",
    "except Exception as e:\n",
    "    print(f\"Chyba: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentModerator:\n",
    "    \"\"\"Moder√°tor obsahu pomoc√≠ CLIP.\"\"\"\n",
    "    \n",
    "    def __init__(self, clip_model: CLIPSystem):\n",
    "        self.clip = clip_model\n",
    "        \n",
    "        # Kategorie pro moderaci\n",
    "        self.safe_categories = [\n",
    "            \"safe content\",\n",
    "            \"family friendly image\",\n",
    "            \"professional photo\",\n",
    "            \"nature scene\",\n",
    "            \"product photo\"\n",
    "        ]\n",
    "        \n",
    "        self.unsafe_categories = [\n",
    "            \"violent content\",\n",
    "            \"inappropriate content\",\n",
    "            \"spam or advertisement\",\n",
    "            \"low quality image\"\n",
    "        ]\n",
    "    \n",
    "    def moderate(self, image: Image.Image) -> Dict[str, Any]:\n",
    "        \"\"\"Moderuje obr√°zek.\"\"\"\n",
    "        \n",
    "        all_categories = self.safe_categories + self.unsafe_categories\n",
    "        scores = self.clip.zero_shot_classify(image, all_categories)\n",
    "        \n",
    "        # V√Ωpoƒçet bezpeƒçnosti\n",
    "        safe_score = sum(scores.get(cat, 0) for cat in self.safe_categories)\n",
    "        unsafe_score = sum(scores.get(cat, 0) for cat in self.unsafe_categories)\n",
    "        \n",
    "        return {\n",
    "            \"is_safe\": safe_score > unsafe_score,\n",
    "            \"safe_score\": safe_score,\n",
    "            \"unsafe_score\": unsafe_score,\n",
    "            \"detailed_scores\": scores,\n",
    "            \"recommendation\": \"approve\" if safe_score > 0.6 else \"review\" if safe_score > 0.4 else \"reject\"\n",
    "        }\n",
    "\n",
    "# Test moder√°toru\n",
    "moderator = ContentModerator(clip_system)\n",
    "\n",
    "url = \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400\"\n",
    "test_image = load_image_from_url(url)\n",
    "\n",
    "result = moderator.moderate(test_image)\n",
    "print(\"V√Ωsledek moderace:\")\n",
    "print(f\"  Bezpeƒçn√Ω: {result['is_safe']}\")\n",
    "print(f\"  Doporuƒçen√≠: {result['recommendation']}\")\n",
    "print(f\"  Safe score: {result['safe_score']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Produkƒçn√≠ T≈ô√≠da pro Multimod√°ln√≠ AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalAIService:\n",
    "    \"\"\"Produkƒçn√≠ slu≈æba pro multimod√°ln√≠ AI.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Inicializuji MultimodalAIService...\")\n",
    "        self._captioner = None\n",
    "        self._vqa = None\n",
    "        self._clip = None\n",
    "        self._cache = {}\n",
    "        print(\"Slu≈æba p≈ôipravena (lazy loading)\")\n",
    "    \n",
    "    @property\n",
    "    def captioner(self):\n",
    "        if self._captioner is None:\n",
    "            self._captioner = ImageCaptioner()\n",
    "        return self._captioner\n",
    "    \n",
    "    @property\n",
    "    def vqa(self):\n",
    "        if self._vqa is None:\n",
    "            self._vqa = VisualQA()\n",
    "        return self._vqa\n",
    "    \n",
    "    @property\n",
    "    def clip(self):\n",
    "        if self._clip is None:\n",
    "            self._clip = CLIPSystem()\n",
    "        return self._clip\n",
    "    \n",
    "    def process_image(self, \n",
    "                     image: Image.Image,\n",
    "                     tasks: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Zpracuje obr√°zek vybran√Ωmi √∫lohami.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL obr√°zek\n",
    "            tasks: Seznam √∫loh ['caption', 'vqa', 'classify']\n",
    "        \"\"\"\n",
    "        \n",
    "        if tasks is None:\n",
    "            tasks = [\"caption\"]\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        if \"caption\" in tasks:\n",
    "            results[\"caption\"] = self.captioner.generate_caption(image)\n",
    "        \n",
    "        if \"vqa\" in tasks:\n",
    "            default_questions = [\n",
    "                \"What is the main subject?\",\n",
    "                \"What colors are visible?\"\n",
    "            ]\n",
    "            results[\"vqa\"] = self.vqa.batch_qa(image, default_questions)\n",
    "        \n",
    "        if \"classify\" in tasks:\n",
    "            default_labels = [\"product\", \"person\", \"nature\", \"document\", \"food\"]\n",
    "            results[\"classification\"] = self.clip.zero_shot_classify(image, default_labels)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_process(self, \n",
    "                     images: List[Image.Image],\n",
    "                     tasks: List[str] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Zpracuje v√≠ce obr√°zk≈Ø.\"\"\"\n",
    "        return [self.process_image(img, tasks) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test produkƒçn√≠ slu≈æby\n",
    "\n",
    "service = MultimodalAIService()\n",
    "\n",
    "# Naƒçten√≠ obr√°zku\n",
    "url = \"https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=400\"\n",
    "image = load_image_from_url(url)\n",
    "\n",
    "# Komplexn√≠ zpracov√°n√≠\n",
    "results = service.process_image(image, tasks=[\"caption\", \"classify\"])\n",
    "\n",
    "print(\"V√Ωsledky zpracov√°n√≠:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"\\nPopis: {results.get('caption')}\")\n",
    "\n",
    "if \"classification\" in results:\n",
    "    print(\"\\nKlasifikace:\")\n",
    "    for label, prob in sorted(results[\"classification\"].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {label}: {prob:.1%}\")\n",
    "\n",
    "show_image(image, results.get('caption', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrnut√≠\n",
    "\n",
    "V tomto notebooku jsme se nauƒçili:\n",
    "\n",
    "1. **Image Captioning** - automatick√© generov√°n√≠ popis≈Ø obr√°zk≈Ø\n",
    "2. **Visual QA** - odpov√≠d√°n√≠ na ot√°zky o obr√°zc√≠ch\n",
    "3. **CLIP** - spojen√≠ textu a obrazu v spoleƒçn√©m prostoru\n",
    "4. **Multimod√°ln√≠ vyhled√°v√°n√≠** - hled√°n√≠ obr√°zk≈Ø podle textu\n",
    "5. **Praktick√© aplikace** - produktov√° anal√Ωza, moderace obsahu\n",
    "\n",
    "### Dal≈°√≠ modely k prozkoum√°n√≠:\n",
    "- **BLIP-2** - vylep≈°en√Ω BLIP s LLM\n",
    "- **LLaVA** - Large Language and Vision Assistant\n",
    "- **GPT-4V** - multimod√°ln√≠ GPT\n",
    "- **Flamingo** - few-shot multimod√°ln√≠ learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
