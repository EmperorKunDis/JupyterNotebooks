{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24707af1",
   "metadata": {},
   "source": [
    "# üìö Kapitola 14: Web scraping z√°klady\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h2 style=\"color: white; margin: 0;\">Blok 2 | INTERMEDIATE</h2>\n",
    "    <p style=\"color: white; margin: 10px 0;\">üìñ V√Ωukov√° kapitola</p>\n",
    "</div>\n",
    "\n",
    "## üéØ Co se nauƒç√≠te\n",
    "\n",
    "V t√©to kapitole se zamƒõ≈ô√≠me na n√°sleduj√≠c√≠ t√©mata:\n",
    "\n",
    "- **BeautifulSoup4 installation**\n",
    "- **HTML/XML parsing**\n",
    "- **CSS selectors a find methods**\n",
    "- **Navigating the parse tree**\n",
    "- **Data extraction patterns**\n",
    "- **Robots.txt a etika scrapingu**\n",
    "- **Selenium pro dynamic content**\n",
    "\n",
    "## ‚ö†Ô∏è P≈ôedpoklady\n",
    "Tato kapitola navazuje na kapitoly: 13\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Inicializace prost≈ôed√≠\n",
    "# Tento k√≥d nastavuje prost≈ôed√≠ pro kapitolu 14\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Informace o prost≈ôed√≠\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìö KAPITOLA 14: Web scraping z√°klady\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üêç Python verze: {sys.version}\")\n",
    "print(f\"üìÖ Datum spu≈°tƒõn√≠: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üíª OS: {os.name}\")\n",
    "print(f\"üìÅ Pracovn√≠ adres√°≈ô: {os.getcwd()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Instalace pot≈ôebn√Ωch knihoven (odkomentujte podle pot≈ôeby)\n",
    "# !pip install requests pandas numpy matplotlib\n",
    "# !pip install beautifulsoup4 sqlalchemy fastapi\n",
    "\n",
    "# Import z√°kladn√≠ch knihoven\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "print(\"‚úÖ Prost≈ôed√≠ p≈ôipraveno!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873bdf30",
   "metadata": {},
   "source": [
    "## üìñ Teoretick√° ƒç√°st\n",
    "\n",
    "<div style=\"background: #f0f4ff; padding: 20px; border-left: 5px solid #4a69bd; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">üéì Z√°kladn√≠ teorie a koncepty</h3>\n",
    "</div>\n",
    "\n",
    "# Web scraping z√°klady\n",
    "\n",
    "## 1. √övod a motivace\n",
    "\n",
    "Web scraping je technika, kter√° umo≈æ≈àuje automatizovan√© sb√≠r√°n√≠ dat z webov√Ωch str√°nek. Je to nezbytn√Ω n√°stroj pro vƒõt≈°inu datov√Ωch projekt≈Ø, kter√© se zamƒõ≈ôuj√≠ na anal√Ωzu informac√≠ z internetu. P≈ôi pr√°ci s velk√Ωmi datov√Ωmi sadami se stane≈° v≈ædycky p≈ôi pot≈ôebƒõ extrahovat data z webov√Ωch str√°nek ‚Äì tedy z nich ‚Äûvyt√°hnout‚Äú konkr√©tn√≠ informace, kter√© jsou ve form√°tu HTML nebo XML.\n",
    "\n",
    "V praxi se web scraping pou≈æ√≠v√° nap≈ô√≠klad v obchodn√≠m anal√Ωze (nap≈ô. sledov√°n√≠ cen produkt≈Ø), novin√°≈ôsk√©m report√°≈æn√≠m (sbƒõr informac√≠ z r≈Øzn√Ωch zdroj≈Ø), marketingu (anal√Ωza konkurence), vƒõdeck√Ωch v√Ωzkumech (z√≠sk√°n√≠ dat pro modely) a mnoha dal≈°√≠ch oblastech. Znalost web scrapingu ti umo≈æn√≠ z√≠sk√°vat data automaticky, bez ruƒçn√≠ho kop√≠rov√°n√≠, co≈æ ≈°et≈ô√≠ ƒças a sni≈æuje chyby.\n",
    "\n",
    "V t√©to kapitole se nauƒç√≠≈° pracovat s knihovnami jako `BeautifulSoup4` a `Selenium`, kter√© jsou z√°kladn√≠mi n√°stroji pro parsov√°n√≠ HTML a pr√°ci s dynamick√Ωmi webov√Ωmi str√°nkami. Nauƒç√≠≈° se jak naj√≠t konkr√©tn√≠ prvky na str√°nce, jak je extrahovat a jak je ulo≈æit do strukturovan√©ho form√°tu.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Hlavn√≠ koncepty\n",
    "\n",
    "### **BeautifulSoup4 installation**\n",
    "\n",
    "Nejprve je t≈ôeba nainstalovat knihovnu BeautifulSoup4, kter√° umo≈æ≈àuje jednoduch√© parsov√°n√≠ HTML a XML dokument≈Ø v Pythonu.\n",
    "\n",
    "```bash\n",
    "pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "M≈Ø≈æe≈° tak√© pot≈ôebovat `requests`, kter√° slou≈æ√≠ k naƒç√≠t√°n√≠ webov√Ωch str√°nek:\n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "Po instalaci m≈Ø≈æe≈° zaƒç√≠t pracovat s HTML obsahem, kter√Ω se stane strukturou, kterou m≈Ø≈æe≈° snadno proch√°zet a vyb√≠rat jednotliv√© prvky.\n",
    "\n",
    "---\n",
    "\n",
    "### **HTML/XML parsing**\n",
    "\n",
    "Parsov√°n√≠ HTML nebo XML znamen√° p≈ôev√©st textov√Ω obsah webov√© str√°nky do strukturovan√©ho stromu, kter√Ω m≈Ø≈æe≈° proch√°zet a vyb√≠rat podle pot≈ôeby. BeautifulSoup vytv√°≈ô√≠ objekt `Tag`, kter√Ω reprezentuje jednotliv√© HTML prvky (nap≈ô. `<div>`, `<p>`, `<a>`), a umo≈æ≈àuje tyto prvky snadno vyhled√°vat, upravovat a extrahovat.\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"<div><p>Ahoj svƒõte</p></div>\"\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup.p.text)  # Vytiskne: Ahoj svƒõte\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **CSS selectors a find methods**\n",
    "\n",
    "BeautifulSoup nab√≠z√≠ nƒõkolik zp≈Øsob≈Ø, jak vyhled√°vat prvky ve stromu. Dva hlavn√≠ zp≈Øsoby jsou:\n",
    "\n",
    "- `find()` ‚Äì hled√° prvn√≠ v√Ωskyt elementu podle zadan√©ho krit√©ria.\n",
    "- `find_all()` ‚Äì hled√° v≈°echny v√Ωskyty elementu.\n",
    "\n",
    "M≈Ø≈æe≈° pou≈æ√≠t CSS selektory (nap≈ô. `soup.select('div p')`) nebo hledat podle atribut≈Ø:\n",
    "\n",
    "```python\n",
    "# Hled√°n√≠ pomoc√≠ tagu a atributu\n",
    "tag = soup.find('a', {'href': '/link'})\n",
    "\n",
    "# Hled√°n√≠ pomoc√≠ CSS selektoru\n",
    "tags = soup.select('div.article p')\n",
    "```\n",
    "\n",
    "CSS selektory jsou siln√Ω n√°stroj, proto≈æe umo≈æ≈àuj√≠ velmi p≈ôesn√© vyhled√°v√°n√≠ vƒçetnƒõ kombinac√≠ t≈ô√≠d, ID a dal≈°√≠ch prvk≈Ø.\n",
    "\n",
    "---\n",
    "\n",
    "### **Navigating the parse tree**\n",
    "\n",
    "V BeautifulSoup je strom HTML dokumentu strukturovan√Ω jako stromov√° struktura. Ka≈æd√Ω prvek m√° ‚Äûrodiny‚Äú ‚Äì rodiƒçe (`parent`), potomky (`children`), sourozence (`siblings`) a podobnƒõ.\n",
    "\n",
    "P≈ô√≠klad:\n",
    "\n",
    "```python\n",
    "for sibling in tag.next_siblings:\n",
    "    print(sibling)\n",
    "```\n",
    "\n",
    "Tato navigace je d≈Øle≈æit√°, kdy≈æ pot≈ôebuje≈° p≈ôistupovat k element≈Øm, kter√© nejsou p≈ô√≠mo vnit≈ôn√≠m obsahem jin√©ho elementu.\n",
    "\n",
    "---\n",
    "\n",
    "### **Data extraction patterns**\n",
    "\n",
    "Extrahov√°n√≠ dat z webov√Ωch str√°nek obvykle n√°sleduje urƒçit√Ω vzor:\n",
    "\n",
    "1. Naƒçti HTML str√°nku (nap≈ô. pomoc√≠ `requests`).\n",
    "2. P≈ôeveƒè ji do BeautifulSoup objektu.\n",
    "3. Najdi prvek nebo prvky, kter√© obsahuj√≠ data.\n",
    "4. Vyextrahuj z nich pot≈ôebn√© informace (text, atributy).\n",
    "5. Ulo≈æ do form√°tu, kter√Ω lze d√°le pou≈æ√≠t ‚Äì nap≈ô. CSV, JSON, datab√°ze.\n",
    "\n",
    "Tento proces se opakuje pro v√≠ce str√°nek nebo prvk≈Ø.\n",
    "\n",
    "---\n",
    "\n",
    "### **Robots.txt a etika scrapingu**\n",
    "\n",
    "P≈ôi web scrapingu je d≈Øle≈æit√© dodr≈æovat pravidla webov√Ωch str√°nek. Vƒõt≈°ina webov√Ωch server≈Ø m√° soubor `robots.txt`, kter√Ω ≈ô√≠k√°, co sm√≠ b√Ωt scrapov√°no a co ne.\n",
    "\n",
    "P≈ô√≠klad:\n",
    "\n",
    "```http\n",
    "User-agent: *\n",
    "Disallow: /private/\n",
    "```\n",
    "\n",
    "Tento soubor ≈ô√≠k√°, ≈æe roboti nesm√≠ p≈ôistupovat k cestƒõ `/private/`.\n",
    "\n",
    "Nen√≠ dobr√© scrapovat p≈ô√≠li≈° ƒçasto ‚Äì m≈Ø≈æe≈° zp≈Øsobit zat√≠≈æen√≠ serveru a b√Ωt blokov√°n. Vhodn√© je p≈ôid√°vat pauzy mezi po≈æadavky (nap≈ô. `time.sleep(1)`), respektovat `robots.txt` a pokud mo≈æno pou≈æ√≠vat ofici√°ln√≠ API, pokud jsou dostupn√°.\n",
    "\n",
    "---\n",
    "\n",
    "### **Selenium pro dynamic content**\n",
    "\n",
    "Mnoho modern√≠ch webov√Ωch str√°nek (nap≈ô. React, Vue, Angular) generuje obsah dynamicky pomoc√≠ JavaScriptu. BeautifulSoup s√°m o sobƒõ nem≈Ø≈æe spustit JavaScript, proto je t≈ôeba pou≈æ√≠t n√°stroj jako `Selenium`, kter√Ω otev√≠r√° webovou str√°nku v re√°ln√©m prohl√≠≈æeƒçi a ƒçek√° na vygenerov√°n√≠ obsahu.\n",
    "\n",
    "Instalace:\n",
    "\n",
    "```bash\n",
    "pip install selenium\n",
    "```\n",
    "\n",
    "Pou≈æit√≠:\n",
    "\n",
    "```python\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://example.com\")\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "driver.quit()\n",
    "```\n",
    "\n",
    "Tato technika je silnƒõj≈°√≠, ale z√°rove≈à n√°roƒçnƒõj≈°√≠ a pomalej≈°√≠. Pou≈æ√≠v√° se tam, kde je t≈ôeba ƒçekat na dynamick√Ω obsah.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. D≈Øle≈æit√© detaily\n",
    "\n",
    "### **ƒåast√© chyby**\n",
    "\n",
    "- Zapom√≠n√°n√≠ na `time.sleep()` ‚Äì m≈Ø≈æe≈° b√Ωt blokov√°n.\n",
    "- Pou≈æ√≠v√°n√≠ ≈°patn√Ωch CSS selektor≈Ø ‚Äì m≈Ø≈æe≈° nez√≠skat data, kter√° jsi chtƒõl.\n",
    "- Ignorov√°n√≠ `robots.txt` ‚Äì m≈Ø≈æe≈° zp≈Øsobit probl√©my na serveru.\n",
    "\n",
    "### **Best practices**\n",
    "\n",
    "- V≈ædy se ujisti, ≈æe nesm√≠≈° scrapovat nep≈ô√≠mo (nap≈ô. p≈ô√≠mo z datab√°ze).\n",
    "- Pou≈æ√≠vej `try-except` pro zpracov√°n√≠ chyb.\n",
    "- Ukl√°dej data do strukturovan√Ωch form√°t≈Ø ‚Äì nap≈ô. CSV, JSON.\n",
    "- Dƒõlej testy na mal√© mno≈æstv√≠ dat p≈ôed spu≈°tƒõn√≠m velk√©ho scrapingu.\n",
    "\n",
    "### **Performance tipy**\n",
    "\n",
    "- Pou≈æ√≠vej `requests` m√≠sto `Selenium`, pokud mo≈æno.\n",
    "- Cacheuj st√°hnut√© str√°nky, pokud se opakuj√≠.\n",
    "- Nepou≈æ√≠vej zbyteƒçn√© `find_all()` ‚Äì v≈ædy se zamƒõ≈ô na konkr√©tn√≠ elementy.\n",
    "- Vyu≈æ√≠vej paralelizaci (nap≈ô. `concurrent.futures`) pro v√≠ce str√°nek.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Propojen√≠ s p≈ôedchoz√≠mi kapitolami\n",
    "\n",
    "Tato kapitola vych√°z√≠ z p≈ôedchoz√≠ch znalost√≠ jako pr√°ce s Pythonem, pr√°ce se soubory a pochopen√≠ HTML struktury. Zat√≠mco d≈ô√≠ve jsme se uƒçili, jak ƒç√≠st soubory a pracovat s promƒõnn√Ωmi, nyn√≠ to roz≈°i≈ôujeme na parsov√°n√≠ webov√Ωch obsah≈Ø.\n",
    "\n",
    "Tato kapitola p≈ôipravuje z√°klady pro pokroƒçilej≈°√≠ techniky jako anal√Ωza dat, tvorba API a automatizovan√© reportov√°n√≠ ‚Äì v≈°echno to je mo≈æn√© jen s vƒõdom√≠m, jak extrahovat data z webu.\n",
    "\n",
    "--- \n",
    "\n",
    "> **Tip:** P≈ôi pr√°ci s web scrapingem se neboj, pokud se nƒõkdy nƒõco nepoda≈ô√≠. Je to bƒõ≈æn√° ƒç√°st procesu ‚Äì v≈ædy m≈Ø≈æe≈° ladit k√≥d, upravovat selektory a testovat na jednodu≈°≈°√≠ch str√°nk√°ch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191e1a5",
   "metadata": {},
   "source": [
    "## üíª Praktick√© p≈ô√≠klady\n",
    "\n",
    "<div style=\"background: #e8f5e9; padding: 20px; border-left: 5px solid #4caf50; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">üë®‚Äçüíª Hands-on p≈ô√≠klady ke spu≈°tƒõn√≠</h3>\n",
    "    <p>N√°sleduj√≠c√≠ p≈ô√≠klady si m≈Ø≈æete hned vyzkou≈°et. Ka≈æd√Ω p≈ô√≠klad je samostatnƒõ spustiteln√Ω.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99792a8",
   "metadata": {},
   "source": [
    "### P≈ô√≠klad 1\n",
    "\n",
    "### Co p≈ô√≠klad demonstruje:\n",
    "Tento p≈ô√≠klad ukazuje, jak nainstalovat a importovat knihovnu BeautifulSoup4 pro pr√°ci s HTML obsahem.\n",
    "\n",
    "```python\n",
    "# Instalaƒçn√≠ p≈ô√≠kaz (nen√≠ souƒç√°st√≠ k√≥du):\n",
    "# pip install beautifulsoup4 requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "print(\"BeautifulSoup4 byl √∫spƒõ≈°nƒõ nainstalov√°n a importov√°n!\")\n",
    "print(\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed65e5",
   "metadata": {},
   "source": [
    "### P≈ô√≠klad 2\n",
    "\n",
    "P≈ô√≠klad demonstruje z√°kladn√≠ import knihovny pro web scraping.\")\n",
    "\n",
    "# Vytvo≈ôen√≠ jednoduch√©ho HTML obsahu pro testov√°n√≠\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><title>Testovac√≠ str√°nka</title></head>\n",
    "<body>\n",
    "    <h1>V√≠tejte na testovac√≠ str√°nce</h1>\n",
    "    <p class=\"intro\">Toto je √∫vodn√≠ odstavec.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parsujeme HTML obsah\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Zobraz√≠me hlavn√≠ nadpis\n",
    "print(\"Hlavn√≠ nadpis:\", soup.h1.text)\n",
    "\n",
    "# V√Ωstup:\n",
    "# BeautifulSoup4 byl √∫spƒõ≈°nƒõ nainstalov√°n a importov√°n!\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3743ba",
   "metadata": {},
   "source": [
    "### P≈ô√≠klad 3\n",
    "\n",
    "P≈ô√≠klad demonstruje z√°kladn√≠ import knihovny pro web scraping.\n",
    "# Hlavn√≠ nadpis: V√≠tejte na testovac√≠ str√°nce\n",
    "```\n",
    "\n",
    "## 2. Z√°kladn√≠ HTML parsing a CSS selektory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098292b8",
   "metadata": {},
   "source": [
    "### P≈ô√≠klad 4\n",
    "\n",
    "### Co p≈ô√≠klad demonstruje:\n",
    "Z√°kladn√≠ pr√°ce s BeautifulSoup k parsingu HTML dokumentu, pou≈æit√≠ CSS selektor≈Ø pro vyb√≠r√°n√≠ prvk≈Ø.\n",
    "\n",
    "\n",
    "\n",
    "## 3. Navigace stromem a find metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09aeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Jednoduch√Ω HTML obsah\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<head><title>Seznam produkt≈Ø</title></head>\n",
    "<body>\n",
    "    <h1 class=\"main-title\">N√°≈° e-shop</h1>\n",
    "    <div id=\"products\">\n",
    "        <div class=\"product\">\n",
    "            <h2 class=\"name\">Laptop</h2>\n",
    "            <span class=\"price\">8999 Kƒç</span>\n",
    "            <p class=\"description\">V√Ωkonn√Ω pracovn√≠ laptop</p>\n",
    "        </div>\n",
    "        <div class=\"product\">\n",
    "            <h2 class=\"name\">Tablet</h2>\n",
    "            <span class=\"price\">4999 Kƒç</span>\n",
    "            <p class=\"description\">P≈ôenosn√Ω tablet</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Vytvo≈ôen√≠ soup objektu\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Pou≈æit√≠ CSS selektoru\n",
    "title = soup.select_one('h1.main-title')\n",
    "print(\"Nadpis str√°nky:\", title.text)\n",
    "\n",
    "# Vybr√°n√≠ v≈°ech produkt≈Ø\n",
    "products = soup.select('.product')\n",
    "for i, product in enumerate(products, 1):\n",
    "    name = product.select_one('.name').text\n",
    "    price = product.select_one('.price').text\n",
    "    desc = product.select_one('.description').text\n",
    "    print(f\"Produkt {i}: {name}, cena: {price}, popis: {desc}\")\n",
    "\n",
    "# V√Ωstup:\n",
    "# Nadpis str√°nky: N√°≈° e-shop\n",
    "# Produkt 1: Laptop, cena: 8999 Kƒç, popis: V√Ωkonn√Ω pracovn√≠ laptop\n",
    "# Produkt 2: Tablet, cena: 4999 Kƒç, popis: P≈ôenosn√Ω tablet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644249a",
   "metadata": {},
   "source": [
    "### P≈ô√≠klad 5\n",
    "\n",
    "### Co p≈ô√≠klad demonstruje:\n",
    "Navigace v HTML stromu pomoc√≠ BeautifulSoup metod find(), find_all() a jin√Ωch navigaƒçn√≠ch metod.\n",
    "\n",
    "\n",
    "\n",
    "## 4. Praktick√Ω p≈ô√≠klad extrakce dat z re√°ln√©ho webu (simulace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Slo≈æitƒõj≈°√≠ HTML obsah s v√≠ce √∫rovnƒõmi\n",
    "html = \"\"\"\n",
    "<ul class=\"menu\">\n",
    "    <li><a href=\"/home\">Dom≈Ø</a></li>\n",
    "    <li><a href=\"/products\">Produkty</a>\n",
    "        <ul class=\"submenu\">\n",
    "            <li><a href=\"/laptops\">Laptopy</a></li>\n",
    "            <li><a href=\"/phones\">Mobiln√≠ telefony</a></li>\n",
    "            <li><a href=\"/tablets\">Tablety</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><a href=\"/about\">O n√°s</a></li>\n",
    "    <li><a href=\"/contact\">Kontakt</a></li>\n",
    "</ul>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Najdeme prvn√≠ odkaz v menu\n",
    "first_link = soup.find('a')\n",
    "print(\"Prvn√≠ odkaz:\", first_link.get_text())\n",
    "\n",
    "# Najdeme v≈°echny odkazy v hlavn√≠m menu\n",
    "main_links = soup.find_all('a', href=True)\n",
    "print(\"Poƒçet odkaz≈Ø:\", len(main_links))\n",
    "\n",
    "# Navigujeme k submenu\n",
    "submenu = soup.find('ul', class_='submenu')\n",
    "if submenu:\n",
    "    submenu_items = submenu.find_all('li')\n",
    "    print(\"Polo≈æky v podmenu:\", [item.get_text() for item in submenu_items])\n",
    "\n",
    "# Najdeme konkr√©tn√≠ odkaz podle textu\n",
    "laptop_link = soup.find('a', string='Laptopy')\n",
    "if laptop_link:\n",
    "    print(\"Odkaz na laptopy:\", laptop_link['href'])\n",
    "\n",
    "# V√Ωstup:\n",
    "# Prvn√≠ odkaz: Dom≈Ø\n",
    "# Poƒçet odkaz≈Ø: 6\n",
    "# Polo≈æky v podmenu: ['Laptopy', 'Mobiln√≠ telefony', 'Tablety']\n",
    "# Odkaz na laptopy: /laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c8e15",
   "metadata": {},
   "source": [
    "### P≈ô√≠klad 6\n",
    "\n",
    "### Co p≈ô√≠klad demonstruje:\n",
    "Praktick√° aplikace extrakce dat z HTML str√°nky, kter√° simuluje re√°ln√Ω web scraping sc√©n√°≈ô.\n",
    "\n",
    "\n",
    "\n",
    "## 5. Pr√°ce s robots.txt a etika scrapingu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Simulovan√Ω HTML obsah (nap≈ô. v√Ωsledky vyhled√°v√°n√≠)\n",
    "html_content = \"\"\"\n",
    "<div class=\"search-results\">\n",
    "    <div class=\"result-item\" data-id=\"1\">\n",
    "        <h3 class=\"title\">Python programovac√≠ jazyk</h3>\n",
    "        <p class=\"description\">Python je v√Ωkonn√Ω a jednoduch√Ω programovac√≠ jazyk.</p>\n",
    "        <span class=\"rating\">4.5</span>\n",
    "        <a href=\"/python-programming\" class=\"link\">ƒå√≠st v√≠ce</a>\n",
    "    </div>\n",
    "    <div class=\"result-item\" data-id=\"2\">\n",
    "        <h3 class=\"title\">Web scraping tutori√°l</h3>\n",
    "        <p class=\"description\">Nauƒçte se jak extrahovat data z webu pomoc√≠ Pythonu.</p>\n",
    "        <span class=\"rating\">4.8</span>\n",
    "        <a href=\"/web-scraping-tutorial\" class=\"link\">ƒå√≠st v√≠ce</a>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extrahujeme v≈°echny polo≈æky v√Ωsledk≈Ø\n",
    "results = soup.find_all('div', class_='result-item')\n",
    "extracted_data = []\n",
    "\n",
    "for result in results:\n",
    "    title = result.find('h3', class_='title').get_text()\n",
    "    description = result.find('p', class_='description').get_text()\n",
    "    rating = result.find('span', class_='rating').get_text()\n",
    "    \n",
    "    # Vytvo≈ô√≠me slovn√≠k s daty\n",
    "    data_item = {\n",
    "        'n√°zev': title,\n",
    "        'popis': description,\n",
    "        'hodnocen√≠': float(rating),\n",
    "        'odkaz': result.find('a', class_='link')['href']\n",
    "    }\n",
    "    \n",
    "    extracted_data.append(data_item)\n",
    "\n",
    "# V√Ωpis extrahovan√Ωch dat\n",
    "for item in extracted_data:\n",
    "    print(f\"N√°zev: {item['n√°zev']}\")\n",
    "    print(f\"Popis: {item['popis']}\")\n",
    "    print(f\"Hodnocen√≠: {item['hodnocen√≠']}\")\n",
    "    print(f\"Odkaz: {item['odkaz']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# V√Ωstup:\n",
    "# N√°zev: Python programovac√≠ jazyk\n",
    "# Popis: Python je v√Ωkonn√Ω a jednoduch√Ω programovac√≠ jazyk.\n",
    "# Hodnocen√≠: 4.5\n",
    "# Odkaz: /python-programming\n",
    "# --------------------------------------------------\n",
    "# N√°zev: Web scraping tutori√°l\n",
    "# Popis: Nauƒçte se jak extrahovat data z webu pomoc√≠ Pythonu.\n",
    "# Hodnocen√≠: 4.8\n",
    "# Odkaz: /web-scraping-tutorial\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5018bd2",
   "metadata": {},
   "source": [
    "### P≈ô√≠klad 7\n",
    "\n",
    "### Co p≈ô√≠klad demonstruje:\n",
    "Demonstrace z√°sad etiky p≈ôi web scrapingu, kontrola robots.txt a dodr≈æov√°n√≠ z√°sad.\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import time\n",
    "\n",
    "# P≈ôedstavme si, ≈æe m√°me tuto URL\n",
    "url = \"https://example.com\"\n",
    "\n",
    "def check_robots_txt(url):\n",
    "    \"\"\"Zkontroluje robots.txt pro danou URL\"\"\"\n",
    "    try:\n",
    "        # Z√≠sk√°me URL pro robots.txt\n",
    "        robots_url = url + \"/robots.txt\"\n",
    "        \n",
    "        # Vytvo≈ô√≠me parser\n",
    "        rp = RobotFileParser()\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        \n",
    "        print(f\"Zkontrolov√°n√≠ robots.txt pro {url}\")\n",
    "        print(\"Povolen√© cesty:\", rp.allowances(url))\n",
    "        print(\"Zak√°zan√© cesty:\", rp.disallows(url))\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Nelze naƒç√≠st robots.txt: {e}\")\n",
    "        return False\n",
    "\n",
    "def safe_scraping_example():\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af4941",
   "metadata": {},
   "source": [
    "### P≈ô√≠klad 8\n",
    "\n",
    "P≈ô√≠klad bezpeƒçn√©ho scrapingu s pauzami\"\"\"\n",
    "    \n",
    "    # Simulace z√≠sk√°n√≠ str√°nky\n",
    "    html_content = \"\"\"\n",
    "    <div class=\"content\">\n",
    "        <h1>Bezpeƒçn√Ω scrapovac√≠ p≈ô√≠klad</h1>\n",
    "        <p>Uk√°zka obsahu pro scraping</p>\n",
    "        <ul>\n",
    "            <li>Polo≈æka 1</li>\n",
    "            <li>Polo≈æka 2</li>\n",
    "            <li>Polo≈æka 3</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e0148",
   "metadata": {},
   "source": [
    "### P≈ô√≠klad 9\n",
    "\n",
    "P≈ô√≠klad z√≠sk√°n√≠ dat\n",
    "    title = soup.find('h1').text\n",
    "    items = [item.text for item in soup.find_all('li')]\n",
    "    \n",
    "    print(\"Bezpeƒçn√Ω scraping:\")\n",
    "    print(f\"Nadpis: {title}\")\n",
    "    print(f\"Polo≈æky: {',"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a65e3",
   "metadata": {},
   "source": [
    "## üéØ Cviƒçen√≠ a √∫koly\n",
    "\n",
    "<div style=\"background: #fff3e0; padding: 20px; border-left: 5px solid #ff9800; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">‚úçÔ∏è Praktick√° cviƒçen√≠ k procviƒçen√≠</h3>\n",
    "    <p>Vy≈ôe≈°te n√°sleduj√≠c√≠ √∫koly. Zaƒçnƒõte od jednodu≈°≈°≈°√≠ch a postupujte k slo≈æitƒõj≈°√≠m.</p>\n",
    "</div>\n",
    "\n",
    "# 5 praktick√Ωch cviƒçen√≠ pro Web scraping z√°klady\n",
    "\n",
    "---\n",
    "\n",
    "## Cviƒçen√≠ 1: Z√≠sk√°n√≠ titulk≈Ø ƒçl√°nk≈Ø z novinov√©ho port√°lu\n",
    "\n",
    "### **N√°zev √∫kolu**\n",
    "Extrahuj titulky ƒçl√°nk≈Ø z novinov√©ho webu\n",
    "\n",
    "### **Detailn√≠ zad√°n√≠**\n",
    "Vytvo≈ô skript, kter√Ω st√°hne HTML obsah str√°nky s novinkami (nap≈ô. https://www.blesk.cz) a extrahuje titulky v≈°ech ƒçl√°nk≈Ø na prvn√≠ stranƒõ. V√Ωsledek ulo≈æ do seznamu.\n",
    "\n",
    "### **Vstupn√≠ data/po≈æadavky**\n",
    "- URL: https://www.blesk.cz\n",
    "- Pou≈æij `requests` a `BeautifulSoup`\n",
    "- Vyhledej titulky v elementech typu `<h2>` nebo `<h3>` s t≈ô√≠dou `title`\n",
    "\n",
    "### **Oƒçek√°van√Ω v√Ωstup**\n",
    "Seznam titulk≈Ø ƒçl√°nk≈Ø ve form√°tu:\n",
    "```python\n",
    "[\n",
    "    \"Titulek ƒçl√°nku 1\",\n",
    "    \"Titulek ƒçl√°nku 2\",\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "### **Hints/N√°povƒõda**\n",
    "1. Pou≈æij `requests.get()` na z√≠sk√°n√≠ HTML obsahu.\n",
    "2. Pro parsov√°n√≠ pou≈æij `BeautifulSoup(html, 'html.parser')`.\n",
    "3. Najdi spr√°vn√Ω element (nap≈ô. `<h2 class=\"title\">`) a vyt√°hni text pomoc√≠ `.text`.\n",
    "4. Nezapome≈à pou≈æ√≠t `try-except` pro zachycen√≠ chyb p≈ôi stahov√°n√≠.\n",
    "\n",
    "### **Kostra ≈ôe≈°en√≠**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.blesk.cz\"\n",
    "# TODO: St√°hni HTML a parsovej ho\n",
    "titles = []\n",
    "# TODO: Najdi a ulo≈æ titulky do seznamu\n",
    "print(titles)\n",
    "```\n",
    "\n",
    "### **Bonusov√© roz≈°√≠≈ôen√≠**\n",
    "P≈ôidej funkci, kter√° ukl√°d√° titulky do CSV souboru.\n",
    "\n",
    "---\n",
    "\n",
    "## Cviƒçen√≠ 2: Sta≈æen√≠ a zpracov√°n√≠ dat z tabulky\n",
    "\n",
    "### **N√°zev √∫kolu**\n",
    "Z√≠skej data z HTML tabulky na webov√© str√°nce\n",
    "\n",
    "### **Detailn√≠ zad√°n√≠**\n",
    "Na webov√© str√°nce (nap≈ô. https://www.w3schools.com/html/html_tables.asp) najdi tabulku a extrahuj v≈°echny ≈ô√°dky, jejich bu≈àky a ulo≈æ je jako seznam slovn√≠k≈Ø.\n",
    "\n",
    "### **Vstupn√≠ data/po≈æadavky**\n",
    "- URL: https://www.w3schools.com/html/html_tables.asp\n",
    "- Vyhledej tabulku (`<table>`) a jej√≠ ≈ô√°dky (`<tr>`), bu≈àky (`<td>`)\n",
    "\n",
    "### **Oƒçek√°van√Ω v√Ωstup**\n",
    "Seznam slovn√≠k≈Ø, kde kl√≠ƒçe jsou n√°zvy sloupc≈Ø:\n",
    "```python\n",
    "[\n",
    "    {\"Jm√©no\": \"Petr\", \"Vƒõk\": \"25\", \"Mƒõsto\": \"Praha\"},\n",
    "    {\"Jm√©no\": \"Jana\", \"Vƒõk\": \"30\", \"Mƒõsto\": \"Brno\"},\n",
    "]\n",
    "```\n",
    "\n",
    "### **Hints/N√°povƒõda**\n",
    "1. Najdi tabulku a v≈°echny ≈ô√°dky (`<tr>`).\n",
    "2. Prvn√≠ ≈ô√°dek obvykle obsahuje hlaviƒçky sloupc≈Ø.\n",
    "3. Ka≈æd√° bu≈àka `<td>` nebo `<th>` se mus√≠ p≈ôev√©st na text.\n",
    "4. Vytvo≈ô slovn√≠k pro ka≈æd√Ω ≈ô√°dek.\n",
    "\n",
    "### **Kostra ≈ôe≈°en√≠**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.w3schools.com/html/html_tables.asp\"\n",
    "# TODO: Naƒçti HTML a najdi tabulku\n",
    "rows = []\n",
    "# TODO: Projdi ≈ô√°dky a vytvo≈ô seznam slovn√≠k≈Ø\n",
    "print(rows)\n",
    "```\n",
    "\n",
    "### **Bonusov√© roz≈°√≠≈ôen√≠**\n",
    "Ulo≈æ data do SQLite datab√°ze.\n",
    "\n",
    "---\n",
    "\n",
    "## Cviƒçen√≠ 3: Web scraping s delayem a error handlingem\n",
    "\n",
    "### **N√°zev √∫kolu**\n",
    "Scrape web s chybovou kontrolou a omezen√≠m rychlosti\n",
    "\n",
    "### **Detailn√≠ zad√°n√≠**\n",
    "Vytvo≈ô skript, kter√Ω st√°hne nƒõkolik str√°nek (nap≈ô. z https://quotes.toscrape.com), p≈ôiƒçem≈æ se bude udr≈æovat pauza mezi po≈æadavky a bude ≈ôe≈°it chyby.\n",
    "\n",
    "### **Vstupn√≠ data/po≈æadavky**\n",
    "- URL: https://quotes.toscrape.com\n",
    "- Vyhledej cit√°ty (`<span class=\"text\">`) a autory (`<small class=\"author\">`)\n",
    "\n",
    "### **Oƒçek√°van√Ω v√Ωstup**\n",
    "Seznam slovn√≠k≈Ø:\n",
    "```python\n",
    "[\n",
    "    {\"quote\": \"Cit√°t 1\", \"author\": \"Autor 1\"},\n",
    "    {\"quote\": \"Cit√°t 2\", \"author\": \"Autor 2\"}\n",
    "]\n",
    "```\n",
    "\n",
    "### **Hints/N√°povƒõda**\n",
    "1. Pou≈æij `time.sleep()` mezi po≈æadavky.\n",
    "2. Vyu≈æij `try-except` pro zachycen√≠ chyb (nap≈ô. timeout).\n",
    "3. Pou≈æij `random.uniform(1, 3)` pro n√°hodn√Ω delay.\n",
    "4. Vyhledej v≈°echny str√°nky (pokud existuj√≠).\n",
    "\n",
    "### **Kostra ≈ôe≈°en√≠**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "base_url = \"https://quotes.toscrape.com\"\n",
    "# TODO: St√°hni str√°nku a extrahuj cit√°ty\n",
    "quotes = []\n",
    "# TODO: P≈ôidej delay a error handling\n",
    "print(quotes)\n",
    "```\n",
    "\n",
    "### **Bonusov√© roz≈°√≠≈ôen√≠**\n",
    "P≈ôidej logov√°n√≠ do souboru (`logging`).\n",
    "\n",
    "---\n",
    "\n",
    "## Cviƒçen√≠ 4: Scraping s vyu≈æit√≠m Selenium (pokud je pot≈ôeba)\n",
    "\n",
    "### **N√°zev √∫kolu**\n",
    "Z√≠sk√°n√≠ dynamicky naƒç√≠tan√Ωch dat pomoc√≠ Selenium\n",
    "\n",
    "### **Detailn√≠ zad√°n√≠**\n",
    "Nav≈°tiv webovou str√°nku, kter√° naƒç√≠t√° data dynamicky (nap≈ô. https://quotes.toscrape.com/js/), a pou≈æij Selenium pro ƒçek√°n√≠ na obsah.\n",
    "\n",
    "### **Vstupn√≠ data/po≈æadavky**\n",
    "- URL: https://quotes.toscrape.com/js/\n",
    "- Pou≈æij Selenium WebDriver\n",
    "- Vyƒçkej, ne≈æ se naƒçtou cit√°ty (nap≈ô. pomoc√≠ `WebDriverWait`)\n",
    "\n",
    "### **Oƒçek√°van√Ω v√Ωstup**\n",
    "Seznam cit√°t≈Ø a autor≈Ø:\n",
    "```python\n",
    "[\n",
    "    {\"quote\": \"Cit√°t 1\", \"author\": \"Autor 1\"},\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "### **Hints/N√°povƒõda**\n",
    "1. Nainstaluj `selenium` a staƒç√≠ chromedriver.\n",
    "2. Pou≈æij `WebDriverWait(driver, 10).until(...)`.\n",
    "3. Najdi elementy pomoc√≠ CSS selektoru nebo XPath.\n",
    "4. Z√≠skej text z elementu `.text`.\n",
    "\n",
    "### **Kostra ≈ôe≈°en√≠**\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "url = \"https://quotes.toscrape.com/js/\"\n",
    "# TODO: Otev≈ôi str√°nku a ƒçekej na naƒçten√≠ obsahu\n",
    "driver = webdriver.Chrome()\n",
    "# TODO: Najdi cit√°ty a autory\n",
    "driver.quit()\n",
    "```\n",
    "\n",
    "### **Bonusov√© roz≈°√≠≈ôen√≠**\n",
    "Ulo≈æ data do JSON souboru.\n",
    "\n",
    "---\n",
    "\n",
    "## Cviƒçen√≠ 5: Zpracov√°n√≠ dat do strukturovan√©ho form√°tu\n",
    "\n",
    "### **N√°zev √∫kolu**\n",
    "Z√≠sk√°n√≠ a ulo≈æen√≠ dat do CSV nebo JSON\n",
    "\n",
    "### **Detailn√≠ zad√°n√≠**\n",
    "Naƒçti data z nƒõkolika str√°nek, zpracuj je a ulo≈æ do CSV nebo JSON souboru.\n",
    "\n",
    "### **Vstupn√≠ data/po≈æadavky**\n",
    "- Nap≈ô. https://quotes.toscrape.com\n",
    "- Z√≠skej cit√°ty, autory a tagy (pokud jsou k dispozici)\n",
    "- Ulo≈æ do CSV nebo JSON\n",
    "\n",
    "### **Oƒçek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6173f0b",
   "metadata": {},
   "source": [
    "## üìö Dal≈°√≠ zdroje a materi√°ly\n",
    "\n",
    "<div style=\"background: #e3f2fd; padding: 20px; border-left: 5px solid #2196f3; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">üîó Doporuƒçen√© materi√°ly k dal≈°√≠mu studiu</h3>\n",
    "</div>\n",
    "\n",
    "# Web scraping z√°klady ‚Äì Seznam zdroj≈Ø\n",
    "\n",
    "## ƒål√°nky a tutori√°ly\n",
    "\n",
    "1. **[Real Python ‚Äì Web Scraping with Python](https://realpython.com/beautiful-soup-web-scraper-python/)**  \n",
    "   Popis: Podrobn√Ω √∫vod do web scrapingu pomoc√≠ knihovny BeautifulSoup. Vysvƒõtluje z√°kladn√≠ koncepty, pr√°ci s HTML a CSS selektory.\n",
    "\n",
    "2. **[Automate the Boring Stuff with Python ‚Äì Web Scraping](https://automatetheboringstuff.com/2e/chapter12/)**  \n",
    "   Popis: Kapitola zab√Ωvaj√≠c√≠ se scrapingem pomoc√≠ `requests` a `BeautifulSoup`. Vhodn√© pro zaƒç√°teƒçn√≠ky, kter√Ωm chyb√≠ z√°kladn√≠ programovac√≠ znalosti.\n",
    "\n",
    "3. **[Scrapy Documentation ‚Äì Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)**  \n",
    "   Popis: Ofici√°ln√≠ tutori√°l k knihovnƒõ Scrapy. Nab√≠z√≠ hlubok√© vysvƒõtlen√≠ pokroƒçil√©ho scrapingu a pr√°ce s frameworkem.\n",
    "\n",
    "4. **[DataCamp ‚Äì Web Scraping in Python](https://www.datacamp.com/tutorial/web-scraping-python)**  \n",
    "   Popis: Interaktivn√≠ kurz zamƒõ≈ôen√Ω na praktick√© pou≈æit√≠ scrapingu v Pythonu, vƒçetnƒõ pr√°ce s API a regulac√≠ (nap≈ô. `requests`, `lxml`).\n",
    "\n",
    "5. **[Kaggle ‚Äì Web Scraping with Python](https://www.kaggle.com/code/colinmorris/web-scraping)**  \n",
    "   Popis: Praktick√Ω p≈ô√≠klad scrapingu na re√°ln√Ωch datech z internetu, vƒçetnƒõ ≈ôe≈°en√≠ bƒõ≈æn√Ωch probl√©m≈Ø jako nap≈ô. ochrany proti bot≈Øm.\n",
    "\n",
    "## YouTube videa\n",
    "\n",
    "1. **[Web Scraping in Python (BeautifulSoup) ‚Äì freeCodeCamp.org](https://www.youtube.com/watch?v=Jk589H563Qs)**  \n",
    "   D√©lka: 24 minuty  \n",
    "   Popis: √övod do scrapingu s vyu≈æit√≠m `requests` a `BeautifulSoup`, praktick√© p≈ô√≠klady, k√≥d ve form√°tu notebooku.\n",
    "\n",
    "2. **[Python Web Scraping Tutorial ‚Äì Corey Schafer](https://www.youtube.com/watch?v=ng2o98k983A)**  \n",
    "   D√©lka: 18 minut  \n",
    "   Popis: Klasick√Ω tutori√°l o web scrapingu, kter√Ω ukazuje z√°kladn√≠ pr√°ci s HTML a v√Ωbƒõr dat.\n",
    "\n",
    "3. **[Scrapy Web Scraping Tutorial ‚Äì Telusko](https://www.youtube.com/watch?v=1QV780jxZ4k)**  \n",
    "   D√©lka: 29 minut  \n",
    "   Popis: Vhodn√© pro pokroƒçil√© u≈æivatele ‚Äì √∫vod do Scrapy, z√°kladn√≠ konfigurace a vytv√°≈ôen√≠ item≈Ø.\n",
    "\n",
    "## Knihy a ofici√°ln√≠ dokumentace\n",
    "\n",
    "1. **[Automate the Boring Stuff with Python ‚Äì Al Sweigart](https://automatetheboringstuff.com/)**  \n",
    "   Popis: U≈æiteƒçn√° kniha pro zaƒç√°teƒçn√≠ky, kter√° vysvƒõtluje praktick√© vyu≈æit√≠ scrapingu v r√°mci automatizace √∫kol≈Ø.\n",
    "\n",
    "2. **[Web Scraping with Python ‚Äì Ryan Mitchell](https://www.amazon.com/Web-Scraping-Python-Collecting-Data/dp/1491910291)**  \n",
    "   Popis: Detailn√≠ pr≈Øvodce web scrapingem, vƒçetnƒõ technik pro pr√°ci s r≈Øzn√Ωmi typy web≈Ø a ≈ôe≈°en√≠ probl√©m≈Ø.\n",
    "\n",
    "3. **[BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)**  \n",
    "   Popis: Ofici√°ln√≠ dokumentace knihovny BeautifulSoup, obsahuje v≈°echny metody, p≈ô√≠klady a pokyny pro pr√°ci s HTML/XML.\n",
    "\n",
    "## Praktick√© projekty\n",
    "\n",
    "1. **Naplnƒõn√≠ tabulky dat z webov√Ωch str√°nek (nap≈ô. aktu√°ln√≠ kurzy mƒõn, seznam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5575e86",
   "metadata": {},
   "source": [
    "## üìù Shrnut√≠ kapitoly\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin: 30px 0;\">\n",
    "    <h3 style=\"color: white; margin-top: 0;\">‚úÖ Co jste se nauƒçili</h3>\n",
    "    <ul style=\"list-style: none; padding-left: 0;\">\n",
    "        <li>‚úì BeautifulSoup4 installation</li>\n",
    "<li>‚úì HTML/XML parsing</li>\n",
    "<li>‚úì CSS selectors a find methods</li>\n",
    "<li>‚úì Navigating the parse tree</li>\n",
    "<li>‚úì Data extraction patterns</li>\n",
    "    </ul>\n",
    "    \n",
    "    <h3 style=\"color: white;\">üéØ Kl√≠ƒçov√© dovednosti</h3>\n",
    "    <p>Po dokonƒçen√≠ t√©to kapitoly byste mƒõli b√Ωt schopni prakticky pou≈æ√≠t v≈°echny probran√© koncepty.</p>\n",
    "    \n",
    "    <h3 style='color: white;'>‚û°Ô∏è Dal≈°√≠ kapitola</h3><p>Kapitola 15 - pokraƒçujte ve studiu!</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "*üìÖ Notebook vygenerov√°n: 2025-09-29 12:19:07*  \n",
    "*ü§ñ Gener√°tor: Comprehensive Colab Generator v2.0*  \n",
    "*üìö Uƒçebnice programov√°n√≠ - Od z√°klad≈Ø k AI*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993caa0b",
   "metadata": {},
   "source": [
    "## üß™ Sandbox - Prostor pro experimenty\n",
    "\n",
    "Pou≈æijte n√°sleduj√≠c√≠ bu≈àky pro vlastn√≠ experimenty a testov√°n√≠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Zde m≈Ø≈æete experimentovat s k√≥dem z kapitoly\n",
    "# Napi≈°te sv≈Øj k√≥d zde:\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
