{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Fine-tuning Model≈Ø s Hugging Face\n",
    "\n",
    "**Autor:** Praut s.r.o. - AI Integration & Business Automation\n",
    "\n",
    "V tomto notebooku se nauƒç√≠me:\n",
    "- Jak funguje fine-tuning a kdy ho pou≈æ√≠t\n",
    "- Fine-tuning klasifikaƒçn√≠ho modelu na vlastn√≠ch datech\n",
    "- Fine-tuning modelu pro NER (Named Entity Recognition)\n",
    "- Pokroƒçil√© techniky: LoRA a PEFT pro efektivn√≠ tr√©nov√°n√≠\n",
    "- Evaluace a deployment fine-tunovan√©ho modelu\n",
    "\n",
    "## Kdy pou≈æ√≠t fine-tuning?\n",
    "\n",
    "| Situace | ≈òe≈°en√≠ |\n",
    "|---------|--------|\n",
    "| Obecn√© √∫lohy (sentiment, NER) | Pou≈æ√≠t p≈ôedtr√©novan√Ω model |\n",
    "| Dom√©novƒõ specifick√© √∫lohy | Fine-tuning na vlastn√≠ch datech |\n",
    "| Velmi specifick√° terminologie | Fine-tuning + vlastn√≠ tokenizer |\n",
    "| Omezen√© GPU zdroje | LoRA/PEFT fine-tuning |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalace pot≈ôebn√Ωch knihoven\n",
    "!pip install -q transformers datasets accelerate evaluate peft bitsandbytes scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, Optional\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Hugging Face knihovny\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import evaluate\n",
    "\n",
    "# Kontrola GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Pou≈æ√≠v√°m za≈ô√≠zen√≠: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Dostupn√° pamƒõ≈•: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fine-tuning pro Klasifikaci Textu\n",
    "\n",
    "Zaƒçneme s nejƒçastƒõj≈°√≠m p≈ô√≠padem - klasifikace textu na vlastn√≠ kategorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P≈ô√≠klad: Klasifikace z√°kaznick√Ωch po≈æadavk≈Ø\n",
    "# Vytvo≈ô√≠me uk√°zkov√° data pro e-shop\n",
    "\n",
    "train_data = {\n",
    "    \"text\": [\n",
    "        # Reklamace\n",
    "        \"Objedn√°vka p≈ôi≈°la po≈°kozen√°, chci vr√°tit pen√≠ze\",\n",
    "        \"Produkt nefunguje, po≈æaduji v√Ωmƒõnu\",\n",
    "        \"Zbo≈æ√≠ neodpov√≠d√° popisu, chci reklamovat\",\n",
    "        \"Bal√≠k dorazil rozbit√Ω\",\n",
    "        \"V√Ωrobek je vadn√Ω hned po vybalen√≠\",\n",
    "        \"Chci uplatnit reklamaci na tento produkt\",\n",
    "        \"Zbo≈æ√≠ m√° vadu, pros√≠m o ≈ôe≈°en√≠\",\n",
    "        \"Produkt p≈ôestal fungovat po t√Ωdnu pou≈æ√≠v√°n√≠\",\n",
    "        \n",
    "        # Dotaz na produkt\n",
    "        \"Jak√© jsou rozmƒõry tohoto produktu?\",\n",
    "        \"Je tento v√Ωrobek kompatibiln√≠ s iPhone?\",\n",
    "        \"M√°te tento produkt i v modr√© barvƒõ?\",\n",
    "        \"Kolik v√°≈æ√≠ tato polo≈æka?\",\n",
    "        \"Z jak√©ho materi√°lu je vyrobeno?\",\n",
    "        \"Jak√° je kapacita baterie?\",\n",
    "        \"Podporuje tento p≈ô√≠stroj USB-C?\",\n",
    "        \"Je mo≈æn√© koupit n√°hradn√≠ d√≠ly?\",\n",
    "        \n",
    "        # Dotaz na dopravu\n",
    "        \"Kdy doraz√≠ moje objedn√°vka?\",\n",
    "        \"Lze doruƒçit na Slovensko?\",\n",
    "        \"Jak√© jsou mo≈ænosti dopravy?\",\n",
    "        \"Kolik stoj√≠ express doruƒçen√≠?\",\n",
    "        \"M≈Ø≈æu si vyzvednout na poboƒçce?\",\n",
    "        \"Jak dlouho trv√° dod√°n√≠?\",\n",
    "        \"Pos√≠l√°te i na po≈°tu?\",\n",
    "        \"Je mo≈æn√© doruƒçen√≠ o v√≠kendu?\",\n",
    "        \n",
    "        # Platba\n",
    "        \"Lze platit kartou?\",\n",
    "        \"P≈ôij√≠m√°te platbu na fakturu?\",\n",
    "        \"Mohu platit p≈ôi p≈ôevzet√≠?\",\n",
    "        \"Jak funguje spl√°tkov√Ω prodej?\",\n",
    "        \"Je mo≈æn√° platba p≈ôes PayPal?\",\n",
    "        \"Nab√≠z√≠te firemn√≠ fakturu?\",\n",
    "        \"Jak√© platebn√≠ metody akceptujete?\",\n",
    "        \"Lze uplatnit d√°rkov√Ω poukaz?\"\n",
    "    ],\n",
    "    \"label\": [\n",
    "        0, 0, 0, 0, 0, 0, 0, 0,  # Reklamace\n",
    "        1, 1, 1, 1, 1, 1, 1, 1,  # Dotaz na produkt\n",
    "        2, 2, 2, 2, 2, 2, 2, 2,  # Dotaz na dopravu\n",
    "        3, 3, 3, 3, 3, 3, 3, 3   # Platba\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Validaƒçn√≠ data\n",
    "val_data = {\n",
    "    \"text\": [\n",
    "        \"Produkt je po≈°kozen√Ω, co m√°m dƒõlat?\",\n",
    "        \"Jak√° je z√°ruka na tento v√Ωrobek?\",\n",
    "        \"Kde je teƒè m≈Øj bal√≠k?\",\n",
    "        \"Akceptujete kryptomƒõny?\",\n",
    "        \"Chci vr√°tit vadn√© zbo≈æ√≠\",\n",
    "        \"M√°te to skladem?\",\n",
    "        \"Doruƒçujete do zahraniƒç√≠?\",\n",
    "        \"Mohu platit na spl√°tky?\"\n",
    "    ],\n",
    "    \"label\": [0, 1, 2, 3, 0, 1, 2, 3]\n",
    "}\n",
    "\n",
    "# Mapov√°n√≠ kategori√≠\n",
    "label_names = [\"reklamace\", \"produkt\", \"doprava\", \"platba\"]\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {label: i for i, label in enumerate(label_names)}\n",
    "\n",
    "print(f\"Tr√©novac√≠ vzorky: {len(train_data['text'])}\")\n",
    "print(f\"Validaƒçn√≠ vzorky: {len(val_data['text'])}\")\n",
    "print(f\"Kategorie: {label_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvo≈ôen√≠ Hugging Face datasetu\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naƒçten√≠ modelu a tokenizeru\n",
    "# Pou≈æijeme men≈°√≠ BERT model optimalizovan√Ω pro ƒçe≈°tinu\n",
    "model_name = \"Seznam/small-e-czech\"  # ƒåesk√Ω model od Seznamu\n",
    "\n",
    "# Alternativa pro obecn√© pou≈æit√≠:\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Model pro klasifikaci\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Poƒçet parametr≈Ø: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizace dat\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Odebr√°n√≠ nepot≈ôebn√Ωch sloupc≈Ø\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definice metrik\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nastaven√≠ tr√©ninku\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/customer_classifier\",\n",
    "    \n",
    "    # Parametry tr√©ninku\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    \n",
    "    # Learning rate a scheduler\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "    \n",
    "    # Evaluace\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    \n",
    "    # Logging\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    \n",
    "    # Optimalizace\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision na GPU\n",
    "    \n",
    "    # Reproducibilita\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spu≈°tƒõn√≠ tr√©ninku\n",
    "print(\"Zaƒç√≠n√°m tr√©nink...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# V√Ωsledky\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"V√Ωsledky tr√©ninku:\")\n",
    "print(f\"  Tr√©ninkov√° loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Poƒçet krok≈Ø: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluace na validaƒçn√≠ch datech\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"V√Ωsledky evaluace:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ulo≈æen√≠ modelu\n",
    "model_save_path = \"./models/customer_classifier\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model ulo≈æen do: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fine-tunovan√©ho modelu\n",
    "from transformers import pipeline\n",
    "\n",
    "# Naƒçten√≠ ulo≈æen√©ho modelu\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_save_path,\n",
    "    tokenizer=model_save_path,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Testovac√≠ zpr√°vy\n",
    "test_messages = [\n",
    "    \"Produkt dorazil rozbit√Ω, chci vr√°tit\",\n",
    "    \"Jak√© jsou parametry baterie?\",\n",
    "    \"Kdy mi p≈ôijde bal√≠k?\",\n",
    "    \"M≈Ø≈æu platit na dob√≠rku?\",\n",
    "    \"Zbo≈æ√≠ nefunguje spr√°vnƒõ\",\n",
    "    \"Je skladem varianta XL?\"\n",
    "]\n",
    "\n",
    "print(\"Test fine-tunovan√©ho modelu:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for msg in test_messages:\n",
    "    result = classifier(msg)[0]\n",
    "    print(f\"\\nZpr√°va: {msg}\")\n",
    "    print(f\"Kategorie: {result['label']} ({result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-tuning pro NER (Named Entity Recognition)\n",
    "\n",
    "Nauƒç√≠me model rozpozn√°vat vlastn√≠ entity specifick√© pro n√°≈° dom√©nov√Ω kontext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pro NER - rozpozn√°v√°n√≠ entit v z√°kaznick√Ωch zpr√°v√°ch\n",
    "# Form√°t: BIO tagging (Beginning, Inside, Outside)\n",
    "\n",
    "# Definice entit\n",
    "ner_labels = [\n",
    "    \"O\",           # Outside - nen√≠ entita\n",
    "    \"B-PRODUCT\",   # Beginning of product name\n",
    "    \"I-PRODUCT\",   # Inside product name\n",
    "    \"B-ORDER\",     # Order number\n",
    "    \"I-ORDER\",\n",
    "    \"B-DATE\",      # Date\n",
    "    \"I-DATE\",\n",
    "    \"B-PRICE\",     # Price\n",
    "    \"I-PRICE\",\n",
    "    \"B-PERSON\",    # Person name\n",
    "    \"I-PERSON\"\n",
    "]\n",
    "\n",
    "ner_id2label = {i: label for i, label in enumerate(ner_labels)}\n",
    "ner_label2id = {label: i for i, label in enumerate(ner_labels)}\n",
    "\n",
    "print(f\"NER labels: {ner_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uk√°zkov√° tr√©novac√≠ data pro NER\n",
    "# Ka≈æd√Ω p≈ô√≠klad obsahuje tokeny a odpov√≠daj√≠c√≠ NER tagy\n",
    "\n",
    "ner_train_data = [\n",
    "    {\n",
    "        \"tokens\": [\"Objednal\", \"jsem\", \"iPhone\", \"15\", \"Pro\", \"dne\", \"15\", \".\", \"ledna\", \".\"],\n",
    "        \"ner_tags\": [0, 0, 1, 2, 2, 0, 5, 6, 6, 0]  # O, O, B-PRODUCT, I-PRODUCT, I-PRODUCT, O, B-DATE, I-DATE, I-DATE, O\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"Objedn√°vka\", \"ƒç√≠slo\", \"ORD\", \"-\", \"2024\", \"-\", \"001\", \"obsahuje\", \"vadn√Ω\", \"Samsung\", \"Galaxy\", \".\"],\n",
    "        \"ner_tags\": [0, 0, 3, 4, 4, 4, 4, 0, 0, 1, 2, 0]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"Cena\", \"1999\", \"Kƒç\", \"je\", \"p≈ô√≠li≈°\", \"vysok√°\", \".\"],\n",
    "        \"ner_tags\": [0, 7, 8, 0, 0, 0, 0]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"Kontaktujte\", \"pros√≠m\", \"pana\", \"Nov√°ka\", \".\"],\n",
    "        \"ner_tags\": [0, 0, 0, 9, 0]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"MacBook\", \"Air\", \"M2\", \"za\", \"35000\", \"Kƒç\", \"objedn√°n\", \"20\", \".\", \"prosince\", \".\"],\n",
    "        \"ner_tags\": [1, 2, 2, 0, 7, 8, 0, 5, 6, 6, 0]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"Pan√≠\", \"Svobodov√°\", \"reklamuje\", \"PlayStation\", \"5\", \".\"],\n",
    "        \"ner_tags\": [0, 9, 0, 1, 2, 0]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Konverze na Dataset\n",
    "ner_dataset = Dataset.from_dict({\n",
    "    \"tokens\": [item[\"tokens\"] for item in ner_train_data],\n",
    "    \"ner_tags\": [item[\"ner_tags\"] for item in ner_train_data]\n",
    "})\n",
    "\n",
    "print(f\"NER tr√©novac√≠ vzorky: {len(ner_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model pro NER\n",
    "ner_model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    ner_model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=ner_id2label,\n",
    "    label2id=ner_label2id\n",
    ")\n",
    "\n",
    "print(f\"NER model: {ner_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizace pro NER - mus√≠me spr√°vnƒõ zarovnat labely s tokeny\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = ner_tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Speci√°ln√≠ tokeny ignorujeme\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # Pro subword tokeny pou≈æijeme stejn√Ω label\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_ner_dataset = ner_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=ner_dataset.column_names\n",
    ")\n",
    "\n",
    "print(tokenized_ner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER metriky\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_ner_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Konverze na label stringy\n",
    "    true_predictions = [\n",
    "        [ner_id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ner_id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tr√©nink NER modelu (zkr√°cen√Ω pro demo)\n",
    "ner_training_args = TrainingArguments(\n",
    "    output_dir=\"./results/ner_model\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "ner_data_collator = DataCollatorForTokenClassification(tokenizer=ner_tokenizer)\n",
    "\n",
    "ner_trainer = Trainer(\n",
    "    model=ner_model,\n",
    "    args=ner_training_args,\n",
    "    train_dataset=tokenized_ner_dataset,\n",
    "    tokenizer=ner_tokenizer,\n",
    "    data_collator=ner_data_collator,\n",
    ")\n",
    "\n",
    "# Tr√©nink\n",
    "print(\"Tr√©nuji NER model...\")\n",
    "ner_trainer.train()\n",
    "\n",
    "# Ulo≈æen√≠\n",
    "ner_save_path = \"./models/custom_ner\"\n",
    "ner_trainer.save_model(ner_save_path)\n",
    "ner_tokenizer.save_pretrained(ner_save_path)\n",
    "print(f\"NER model ulo≈æen do: {ner_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Efektivn√≠ Fine-tuning s LoRA (PEFT)\n",
    "\n",
    "LoRA (Low-Rank Adaptation) umo≈æ≈àuje fine-tuning velk√Ωch model≈Ø s minim√°ln√≠mi n√°roky na pamƒõ≈•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "# Konfigurace LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification\n",
    "    r=8,                          # Rank - ni≈æ≈°√≠ = men≈°√≠ model, vy≈°≈°√≠ = lep≈°√≠ v√Ωsledky\n",
    "    lora_alpha=32,                # Scaling faktor\n",
    "    lora_dropout=0.1,             # Dropout pro regularizaci\n",
    "    target_modules=[\"query\", \"value\"],  # Kter√© vrstvy adaptovat\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "print(\"LoRA konfigurace:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvo≈ôen√≠ PEFT modelu\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=len(label_names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Aplikace LoRA\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Porovn√°n√≠ poƒçtu parametr≈Ø\n",
    "total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nPorovn√°n√≠ parametr≈Ø:\")\n",
    "print(f\"  Celkem parametr≈Ø: {total_params:,}\")\n",
    "print(f\"  Tr√©novateln√Ωch parametr≈Ø: {trainable_params:,}\")\n",
    "print(f\"  Procento tr√©novateln√Ωch: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tr√©nink s LoRA - mnohem rychlej≈°√≠ a m√©nƒõ n√°roƒçn√Ω na pamƒõ≈•\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=\"./results/lora_classifier\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-4,  # Vy≈°≈°√≠ learning rate pro LoRA\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Tr√©nuji LoRA model...\")\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ulo≈æen√≠ LoRA adapt√©r≈Ø (velmi mal√© soubory!)\n",
    "lora_save_path = \"./models/lora_classifier\"\n",
    "peft_model.save_pretrained(lora_save_path)\n",
    "\n",
    "# Zjist√≠me velikost\n",
    "import os\n",
    "lora_size = sum(\n",
    "    os.path.getsize(os.path.join(lora_save_path, f))\n",
    "    for f in os.listdir(lora_save_path)\n",
    "    if os.path.isfile(os.path.join(lora_save_path, f))\n",
    ")\n",
    "\n",
    "print(f\"LoRA adapt√©ry ulo≈æeny: {lora_save_path}\")\n",
    "print(f\"Velikost LoRA adapt√©r≈Ø: {lora_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naƒçten√≠ a pou≈æit√≠ LoRA modelu\n",
    "def load_lora_model(base_model_name: str, lora_path: str, num_labels: int):\n",
    "    \"\"\"Naƒçte z√°kladn√≠ model s LoRA adapt√©ry.\"\"\"\n",
    "    # Naƒçten√≠ z√°kladn√≠ho modelu\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    \n",
    "    # Naƒçten√≠ LoRA adapt√©r≈Ø\n",
    "    model = PeftModel.from_pretrained(base, lora_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test\n",
    "loaded_model = load_lora_model(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    lora_save_path,\n",
    "    len(label_names)\n",
    ")\n",
    "\n",
    "print(\"LoRA model √∫spƒõ≈°nƒõ naƒçten!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kompletn√≠ Pipeline pro Fine-tuning\n",
    "\n",
    "Vytvo≈ô√≠me znovupou≈æitelnou t≈ô√≠du pro fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningPipeline:\n",
    "    \"\"\"\n",
    "    Kompletn√≠ pipeline pro fine-tuning klasifikaƒçn√≠ch model≈Ø.\n",
    "    Podporuje standardn√≠ fine-tuning i LoRA.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model: str = \"bert-base-multilingual-cased\",\n",
    "        use_lora: bool = True,\n",
    "        lora_r: int = 8,\n",
    "        lora_alpha: int = 32\n",
    "    ):\n",
    "        self.base_model = base_model\n",
    "        self.use_lora = use_lora\n",
    "        self.lora_r = lora_r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        \n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        self.label_names = None\n",
    "        \n",
    "    def prepare_data(\n",
    "        self,\n",
    "        train_texts: List[str],\n",
    "        train_labels: List[int],\n",
    "        val_texts: List[str],\n",
    "        val_labels: List[int],\n",
    "        label_names: List[str],\n",
    "        max_length: int = 128\n",
    "    ) -> DatasetDict:\n",
    "        \"\"\"P≈ôiprav√≠ data pro tr√©nink.\"\"\"\n",
    "        self.label_names = label_names\n",
    "        self.id2label = {i: l for i, l in enumerate(label_names)}\n",
    "        self.label2id = {l: i for i, l in enumerate(label_names)}\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model)\n",
    "        \n",
    "        # Vytvo≈ôen√≠ dataset≈Ø\n",
    "        train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "        val_dataset = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n",
    "        \n",
    "        # Tokenizace\n",
    "        def tokenize(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            )\n",
    "        \n",
    "        train_tokenized = train_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "        val_tokenized = val_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "        \n",
    "        train_tokenized.set_format(\"torch\")\n",
    "        val_tokenized.set_format(\"torch\")\n",
    "        \n",
    "        return DatasetDict({\"train\": train_tokenized, \"validation\": val_tokenized})\n",
    "    \n",
    "    def setup_model(self):\n",
    "        \"\"\"Nastav√≠ model pro tr√©nink.\"\"\"\n",
    "        # Z√°kladn√≠ model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.base_model,\n",
    "            num_labels=len(self.label_names),\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id\n",
    "        )\n",
    "        \n",
    "        # Aplikace LoRA pokud je povolena\n",
    "        if self.use_lora:\n",
    "            from peft import LoraConfig, get_peft_model, TaskType\n",
    "            \n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.SEQ_CLS,\n",
    "                r=self.lora_r,\n",
    "                lora_alpha=self.lora_alpha,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"query\", \"value\"]\n",
    "            )\n",
    "            \n",
    "            self.model = get_peft_model(self.model, lora_config)\n",
    "            print(f\"LoRA aplikov√°na (r={self.lora_r}, alpha={self.lora_alpha})\")\n",
    "        \n",
    "        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"Tr√©novateln√© parametry: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        dataset: DatasetDict,\n",
    "        output_dir: str,\n",
    "        epochs: int = 10,\n",
    "        batch_size: int = 8,\n",
    "        learning_rate: float = 2e-5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Spust√≠ tr√©nink modelu.\"\"\"\n",
    "        \n",
    "        # Metriky\n",
    "        accuracy_metric = evaluate.load(\"accuracy\")\n",
    "        f1_metric = evaluate.load(\"f1\")\n",
    "        \n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "            return {\n",
    "                \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "                \"f1\": f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "            }\n",
    "        \n",
    "        # Training arguments\n",
    "        lr = learning_rate if not self.use_lora else learning_rate * 5  # Vy≈°≈°√≠ LR pro LoRA\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            learning_rate=lr,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=50,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            logging_steps=10,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "        \n",
    "        # Tr√©nink\n",
    "        print(\"\\nZaƒç√≠n√°m tr√©nink...\")\n",
    "        train_result = self.trainer.train()\n",
    "        \n",
    "        # Evaluace\n",
    "        eval_result = self.trainer.evaluate()\n",
    "        \n",
    "        return {\n",
    "            \"training_loss\": train_result.training_loss,\n",
    "            \"eval_accuracy\": eval_result[\"eval_accuracy\"],\n",
    "            \"eval_f1\": eval_result[\"eval_f1\"]\n",
    "        }\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Ulo≈æ√≠ model a tokenizer.\"\"\"\n",
    "        self.trainer.save_model(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        \n",
    "        # Ulo≈æen√≠ konfigurace\n",
    "        config = {\n",
    "            \"base_model\": self.base_model,\n",
    "            \"use_lora\": self.use_lora,\n",
    "            \"label_names\": self.label_names\n",
    "        }\n",
    "        with open(f\"{path}/pipeline_config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"Model ulo≈æen do: {path}\")\n",
    "    \n",
    "    def predict(self, texts: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Predikce na nov√Ωch datech.\"\"\"\n",
    "        self.model.eval()\n",
    "        results = []\n",
    "        \n",
    "        for text in texts:\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "                pred_idx = probs.argmax().item()\n",
    "            \n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"label\": self.label_names[pred_idx],\n",
    "                \"confidence\": probs[pred_idx].item(),\n",
    "                \"all_scores\": {l: probs[i].item() for i, l in enumerate(self.label_names)}\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pou≈æit√≠ pipeline\n",
    "pipeline = FineTuningPipeline(\n",
    "    base_model=\"bert-base-multilingual-cased\",\n",
    "    use_lora=True,\n",
    "    lora_r=8\n",
    ")\n",
    "\n",
    "# P≈ô√≠prava dat\n",
    "dataset = pipeline.prepare_data(\n",
    "    train_texts=train_data[\"text\"],\n",
    "    train_labels=train_data[\"label\"],\n",
    "    val_texts=val_data[\"text\"],\n",
    "    val_labels=val_data[\"label\"],\n",
    "    label_names=label_names\n",
    ")\n",
    "\n",
    "# Setup modelu\n",
    "pipeline.setup_model()\n",
    "\n",
    "# Tr√©nink\n",
    "results = pipeline.train(\n",
    "    dataset=dataset,\n",
    "    output_dir=\"./results/pipeline_model\",\n",
    "    epochs=10,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"V√Ωsledky:\")\n",
    "print(f\"  Accuracy: {results['eval_accuracy']:.2%}\")\n",
    "print(f\"  F1 Score: {results['eval_f1']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pipeline\n",
    "test_texts = [\n",
    "    \"Zbo≈æ√≠ je vadn√©, co s t√≠m?\",\n",
    "    \"Jak√© jsou rozmƒõry produktu?\",\n",
    "    \"Kdy doraz√≠ z√°silka?\",\n",
    "    \"P≈ôij√≠m√°te Apple Pay?\"\n",
    "]\n",
    "\n",
    "predictions = pipeline.predict(test_texts)\n",
    "\n",
    "print(\"Predikce:\")\n",
    "print(\"=\"*50)\n",
    "for pred in predictions:\n",
    "    print(f\"\\nText: {pred['text']}\")\n",
    "    print(f\"Kategorie: {pred['label']} ({pred['confidence']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ulo≈æen√≠ kompletn√≠ pipeline\n",
    "pipeline.save(\"./models/complete_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tipy pro Produkƒçn√≠ Fine-tuning\n",
    "\n",
    "### Doporuƒçen√© postupy:\n",
    "\n",
    "1. **Data**\n",
    "   - Minim√°lnƒõ 100-500 vzork≈Ø na kategorii\n",
    "   - Vyv√°≈æen√© t≈ô√≠dy nebo pou≈æit√≠ weighted loss\n",
    "   - Kvalitn√≠ anotace jsou d≈Øle≈æitƒõj≈°√≠ ne≈æ kvantita\n",
    "\n",
    "2. **Hyperparametry**\n",
    "   - Learning rate: 1e-5 a≈æ 5e-5 pro full fine-tuning\n",
    "   - Learning rate: 1e-4 a≈æ 1e-3 pro LoRA\n",
    "   - Batch size: 8-32 (vƒõt≈°√≠ = stabilnƒõj≈°√≠ gradient)\n",
    "   - Epochs: 3-10 s early stopping\n",
    "\n",
    "3. **LoRA vs Full Fine-tuning**\n",
    "   - LoRA: rychlej≈°√≠, m√©nƒõ pamƒõti, snadn√© p≈ôep√≠n√°n√≠ mezi √∫lohami\n",
    "   - Full: lep≈°√≠ v√Ωsledky na velmi specifick√Ωch dom√©n√°ch\n",
    "\n",
    "4. **Evaluace**\n",
    "   - V≈ædy pou≈æ√≠vat holdout test set\n",
    "   - Sledovat F1 score pro nevyv√°≈æen√° data\n",
    "   - Cross-validace pro mal√© datasety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Export modelu pro ONNX (rychl√° inference)\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch.onnx\n",
    "\n",
    "def export_to_onnx(model_path: str, output_path: str):\n",
    "    \"\"\"Exportuje model do ONNX form√°tu pro rychlou inference.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Dummy input\n",
    "    dummy_input = tokenizer(\n",
    "        \"Test text\",\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # Export\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
    "        output_path,\n",
    "        input_names=[\"input_ids\", \"attention_mask\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "            \"attention_mask\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "            \"logits\": {0: \"batch_size\"}\n",
    "        },\n",
    "        opset_version=14\n",
    "    )\n",
    "    \n",
    "    print(f\"Model exportov√°n do: {output_path}\")\n",
    "\n",
    "# P≈ô√≠klad pou≈æit√≠ (zakomentov√°no - vy≈æaduje ulo≈æen√Ω model)\n",
    "# export_to_onnx(\"./models/customer_classifier\", \"./models/classifier.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrnut√≠\n",
    "\n",
    "V tomto notebooku jsme se nauƒçili:\n",
    "\n",
    "1. **Z√°kladn√≠ fine-tuning** - klasifikace textu na vlastn√≠ch datech\n",
    "2. **NER fine-tuning** - rozpozn√°v√°n√≠ vlastn√≠ch entit\n",
    "3. **LoRA/PEFT** - efektivn√≠ fine-tuning s minim√°ln√≠ pamƒõt√≠\n",
    "4. **Pipeline** - znovupou≈æiteln√° t≈ô√≠da pro fine-tuning\n",
    "5. **Produkƒçn√≠ tipy** - best practices pro deployment\n",
    "\n",
    "### Dal≈°√≠ kroky:\n",
    "- Experiment s vƒõt≈°√≠mi modely (BERT-large, RoBERTa)\n",
    "- Quantizace pro rychlej≈°√≠ inference\n",
    "- A/B testov√°n√≠ v produkci\n",
    "- Continuous learning s nov√Ωmi daty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
