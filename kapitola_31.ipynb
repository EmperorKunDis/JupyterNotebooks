{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitola 31: Zpetna propagace - Jak se neuronova sit uci ze svych chyb\n",
    "\n",
    "## Cile kapitoly\n",
    "- Pochopit 4 kroky uceni neuronove site\n",
    "- Naucit se pouzivat PyTorch pro trenovani siti\n",
    "- Vizualizovat ucici krivku (learning curve)\n",
    "- Experimentovat s hyperparametry\n",
    "\n",
    "## Predpoklady\n",
    "- Aktivacni funkce (kapitola 30)\n",
    "- Vicevrstve site a XOR problem (kapitola 29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Co je zpetna propagace (Backpropagation)?\n",
    "\n",
    "Zpetna propagace je algoritmus pro **uceni neuronovych siti**. Umoznuje siti se zlepsovat na zaklade svych chyb.\n",
    "\n",
    "### Analogie: Tym delníku stavejici zed\n",
    "\n",
    "Predstavte si, ze sit je tym delniku, kteri stavi zed:\n",
    "\n",
    "1. **Delnci postavi zed** (Forward Pass)\n",
    "2. **Mistr zmeri chybu** - \"Zed je o 10 cm kriva!\" (Loss)\n",
    "3. **Mistr jde od posledniho delnka k prvnimu** a kazdemu rekne, jak moc se podilel na chybe (Backward Pass)\n",
    "4. **Kazdy delnik upravi svou praci** (Weight Update)\n",
    "\n",
    "Tento cyklus opakujeme tisickrat a zed je stale presnejsi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalace PyTorch\n",
    "!pip install torch matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch verze: {torch.__version__}\")\n",
    "print(\"Knihovny uspesne nacteny!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ctyri kroky uceni neuronove site\n",
    "\n",
    "Kazda epocha (trenovaci cyklus) se sklada ze 4 kroku:\n",
    "\n",
    "| Krok | Nazev | Popis |\n",
    "|------|-------|-------|\n",
    "| 1 | **Forward Pass** | Data projdou siti, ziskame predikci |\n",
    "| 2 | **Loss Calculation** | Porovname predikci se skutecnosti |\n",
    "| 3 | **Backward Pass** | Spocitame gradienty (mira viny) |\n",
    "| 4 | **Weight Update** | Upravime vahy podle gradientu |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizace 4 kroku uceni\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "steps = [\n",
    "    (\"1. Forward Pass\", \"Vstup -> Sit -> Predikce\", \"#3498db\"),\n",
    "    (\"2. Vypocet ztráty\", \"Predikce vs Skutecnost\\n= Chyba (Loss)\", \"#e74c3c\"),\n",
    "    (\"3. Backward Pass\", \"Chyba -> Gradienty\\n(zpetne sireni)\", \"#f39c12\"),\n",
    "    (\"4. Update vah\", \"Vahy -= lr * gradient\", \"#2ecc71\")\n",
    "]\n",
    "\n",
    "for ax, (title, text, color) in zip(axes, steps):\n",
    "    ax.add_patch(plt.Rectangle((0.1, 0.2), 0.8, 0.6, \n",
    "                                facecolor=color, alpha=0.7, edgecolor='black', linewidth=2))\n",
    "    ax.text(0.5, 0.7, title, ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(0.5, 0.4, text, ha='center', va='center', fontsize=10)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('4 kroky uceni neuronove site', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Problem XOR v PyTorch\n",
    "\n",
    "Pojdme si ukazat cele uceni na problemu XOR. PyTorch nam neuveriitelne zjednodusi kod!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pro XOR jako PyTorch tenzory\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "print(\"XOR pravdivostni tabulka:\")\n",
    "print(\"Vstup 1 | Vstup 2 | Vystup\")\n",
    "print(\"-\" * 25)\n",
    "for i in range(4):\n",
    "    print(f\"   {int(X[i][0])}    |    {int(X[i][1])}    |    {int(y[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definice architektury site\n",
    "class XORNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORNet, self).__init__()\n",
    "        # Vstupni vrstva (2 neurony) -> Skryta vrstva (4 neurony)\n",
    "        self.layer1 = nn.Linear(2, 4)\n",
    "        # Aktivacni funkce pro skrytou vrstvu\n",
    "        self.activation = nn.ReLU()\n",
    "        # Skryta vrstva -> Vystupni vrstva (1 neuron)\n",
    "        self.layer2 = nn.Linear(4, 1)\n",
    "        # Sigmoid na vystupu (pro pravdepodobnost 0-1)\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.output_activation(x)\n",
    "        return x\n",
    "\n",
    "# Vytvoreni modelu\n",
    "model = XORNet()\n",
    "print(\"Architektura site:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definice ztratove funkce a optimalizatoru\n",
    "criterion = nn.MSELoss()  # Mean Squared Error\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)  # Stochastic Gradient Descent\n",
    "\n",
    "print(\"Ztratova funkce: Mean Squared Error (MSE)\")\n",
    "print(\"Optimizer: SGD (Stochastic Gradient Descent)\")\n",
    "print(\"Learning rate: 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trenovaci smycka - 4 kroky v akci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenovaci smycka\n",
    "epochs = 5000\n",
    "losses = []  # Seznam pro ukladani ztraty v kazde epose\n",
    "\n",
    "print(\"Zaciname trenovani...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ===== KROK 1: Dopredny pruchod (Forward Pass) =====\n",
    "    outputs = model(X)\n",
    "    \n",
    "    # ===== KROK 2: Vypocet ztraty (Loss Calculation) =====\n",
    "    loss = criterion(outputs, y)\n",
    "    losses.append(loss.item())  # Ulozime hodnotu ztraty\n",
    "    \n",
    "    # ===== KROK 3: Zpetny pruchod (Backward Pass) =====\n",
    "    optimizer.zero_grad()  # Vynulovani gradientu pred zpetnym pruchodem\n",
    "    loss.backward()        # Zpetna propagace - vypocet gradientu\n",
    "    \n",
    "    # ===== KROK 4: Aktualizace vah (Weight Update) =====\n",
    "    optimizer.step()       # Aktualizace vah podle gradientu\n",
    "    \n",
    "    # Vypiseme progress kazdych 1000 epoch\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epocha [{epoch+1:5d}/{epochs}], Ztrata: {loss.item():.6f}')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Trenovani dokonceno!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vizualizace ucici krivky (Learning Curve)\n",
    "\n",
    "Ucici krivka je jako **EKG nasi site** - ukazuje, jak dobre se uci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizace ucici krivky\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Levy graf - cela krivka\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, 'b-', linewidth=1)\n",
    "plt.title('Prubeh uceni neuronove site', fontsize=14)\n",
    "plt.xlabel('Epocha', fontsize=12)\n",
    "plt.ylabel('Ztrata (Loss)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Pravy graf - logaritmicka skala\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses, 'b-', linewidth=1)\n",
    "plt.title('Ucici krivka (logaritmicka skala)', fontsize=14)\n",
    "plt.xlabel('Epocha', fontsize=12)\n",
    "plt.ylabel('Ztrata (Loss)', fontsize=12)\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPocatecni ztrata: {losses[0]:.6f}\")\n",
    "print(f\"Konecna ztrata:   {losses[-1]:.6f}\")\n",
    "print(f\"Zlepseni:         {losses[0]/losses[-1]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalni test modelu\n",
    "print(\"\\nFinalni predikce modelu:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "with torch.no_grad():  # Vypneme vypocet gradientu pro testovani\n",
    "    predictions = model(X)\n",
    "    rounded = torch.round(predictions)\n",
    "    \n",
    "    print(\"Vstup    | Predikce | Zaokrouhleno | Spravne\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(4):\n",
    "        pred = predictions[i].item()\n",
    "        rnd = int(rounded[i].item())\n",
    "        expected = int(y[i].item())\n",
    "        is_correct = \"ANO\" if rnd == expected else \"NE\"\n",
    "        if rnd == expected:\n",
    "            correct += 1\n",
    "        print(f\"[{int(X[i][0])}, {int(X[i][1])}]   |  {pred:.4f}  |      {rnd}       |   {is_correct}\")\n",
    "    \n",
    "    print(\"-\" * 45)\n",
    "    print(f\"Presnost: {correct}/4 = {correct/4*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment: Vliv rychlosti uceni (Learning Rate)\n",
    "\n",
    "Rychlost uceni je jeden z nejdulezitejsich hyperparametru. Pojdme experimentovat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xor_with_lr(learning_rate, epochs=3000):\n",
    "    \"\"\"Trenuje XOR model s danou rychlosti uceni\"\"\"\n",
    "    # Novy model\n",
    "    torch.manual_seed(42)  # Pro reprodukovatelnost\n",
    "    model = XORNet()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Testovani ruznych rychlosti uceni\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Levy graf - normalni skala\n",
    "plt.subplot(1, 2, 1)\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    losses = train_xor_with_lr(lr)\n",
    "    plt.plot(losses, label=f'LR = {lr}', color=color, linewidth=1.5)\n",
    "\n",
    "plt.title('Vliv rychlosti uceni na trenovani', fontsize=14)\n",
    "plt.xlabel('Epocha', fontsize=12)\n",
    "plt.ylabel('Ztrata (Loss)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 0.5)\n",
    "\n",
    "# Pravy graf - detail prvnich 500 epoch\n",
    "plt.subplot(1, 2, 2)\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    losses = train_xor_with_lr(lr)\n",
    "    plt.plot(losses[:500], label=f'LR = {lr}', color=color, linewidth=1.5)\n",
    "\n",
    "plt.title('Detail prvnich 500 epoch', fontsize=14)\n",
    "plt.xlabel('Epocha', fontsize=12)\n",
    "plt.ylabel('Ztrata (Loss)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPozorovani:\")\n",
    "print(\"- Prilis mala LR (0.01): Uceni je pomale\")\n",
    "print(\"- Optimalni LR (0.1-0.5): Rychle a stabilni uceni\")\n",
    "print(\"- Prilis velka LR (2.0): Nestabilni, muze divergovat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment: Ruzne optimalizatory\n",
    "\n",
    "PyTorch nabizi ruzne optimalizatory. Kazdy ma sve vyhody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_optimizer(optimizer_class, optimizer_name, epochs=2000, lr=0.1):\n",
    "    \"\"\"Trenuje model s danym optimalizatorem\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    model = XORNet()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Testovani ruznych optimalizatoru\n",
    "optimizers = [\n",
    "    (torch.optim.SGD, 'SGD'),\n",
    "    (torch.optim.Adam, 'Adam'),\n",
    "    (torch.optim.RMSprop, 'RMSprop'),\n",
    "    (torch.optim.Adagrad, 'Adagrad')\n",
    "]\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for (opt_class, opt_name), color in zip(optimizers, colors):\n",
    "    losses = train_with_optimizer(opt_class, opt_name)\n",
    "    plt.plot(losses, label=opt_name, color=color, linewidth=2)\n",
    "\n",
    "plt.title('Porovnani optimalizatoru', fontsize=14)\n",
    "plt.xlabel('Epocha', fontsize=12)\n",
    "plt.ylabel('Ztrata (Loss)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPrehled optimalizatoru:\")\n",
    "print(\"- SGD: Zakladni, jednoduchy, ale spolehlivy\")\n",
    "print(\"- Adam: Adaptivni, casto nejlepsi volba pro zacatek\")\n",
    "print(\"- RMSprop: Dobry pro rekurentni site\")\n",
    "print(\"- Adagrad: Adaptivni learning rate pro kazdy parametr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vizualizace gradientu v siti\n",
    "\n",
    "Pojdme se podivat, jak gradienty \"tecou\" zpet siti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvorime novy model a provedeme jeden krok\n",
    "torch.manual_seed(42)\n",
    "model = XORNet()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(X)\n",
    "loss = criterion(outputs, y)\n",
    "\n",
    "# Backward pass (vypocet gradientu)\n",
    "loss.backward()\n",
    "\n",
    "# Zobrazeni gradientu\n",
    "print(\"Gradienty vah po jednom backward pass:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_mean = param.grad.abs().mean().item()\n",
    "        grad_max = param.grad.abs().max().item()\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Tvar: {list(param.shape)}\")\n",
    "        print(f\"  Prumerny gradient: {grad_mean:.6f}\")\n",
    "        print(f\"  Max gradient:      {grad_max:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizace gradientu jako heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Layer 1 gradienty\n",
    "ax1 = axes[0]\n",
    "grad1 = model.layer1.weight.grad.detach().numpy()\n",
    "im1 = ax1.imshow(grad1, cmap='coolwarm', aspect='auto')\n",
    "ax1.set_title('Gradienty vah - Vrstva 1 (2->4)', fontsize=12)\n",
    "ax1.set_xlabel('Vstupni neurony')\n",
    "ax1.set_ylabel('Skryte neurony')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Layer 2 gradienty\n",
    "ax2 = axes[1]\n",
    "grad2 = model.layer2.weight.grad.detach().numpy()\n",
    "im2 = ax2.imshow(grad2, cmap='coolwarm', aspect='auto')\n",
    "ax2.set_title('Gradienty vah - Vrstva 2 (4->1)', fontsize=12)\n",
    "ax2.set_xlabel('Skryte neurony')\n",
    "ax2.set_ylabel('Vystupni neuron')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCervena = pozitivni gradient (zvysit vahu)\")\n",
    "print(\"Modra = negativni gradient (snizit vahu)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Mini-projekt: Sledovani vah behem trenovani\n",
    "\n",
    "Pojdme sledovat, jak se vahy meni behem trenovani."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenovani s ukladanim vah\n",
    "torch.manual_seed(42)\n",
    "model = XORNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "# Ulozime historii vah\n",
    "weight_history = []\n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Ulozime vahy kazdych 50 epoch\n",
    "    if epoch % 50 == 0:\n",
    "        weights = model.layer1.weight.detach().clone().numpy().flatten()\n",
    "        weight_history.append(weights)\n",
    "\n",
    "weight_history = np.array(weight_history)\n",
    "\n",
    "# Vizualizace evoluce vah\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for i in range(min(8, weight_history.shape[1])):\n",
    "    plt.plot(range(0, epochs, 50), weight_history[:, i], \n",
    "             label=f'Vaha {i+1}', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "plt.title('Evoluce vah v prvni vrstve behem trenovani', fontsize=14)\n",
    "plt.xlabel('Epocha', fontsize=12)\n",
    "plt.ylabel('Hodnota vahy', fontsize=12)\n",
    "plt.legend(loc='upper right', ncol=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Vsimněte si, jak se vahy postupne stabilizuji!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Shrnuti kapitoly\n",
    "\n",
    "### Co jsme se naucili:\n",
    "\n",
    "1. **Zpetna propagace** je algoritmus pro uceni neuronovych siti\n",
    "\n",
    "2. **4 kroky uceni**:\n",
    "   - Forward Pass: data projdou siti\n",
    "   - Loss Calculation: merime chybu\n",
    "   - Backward Pass: pocitame gradienty\n",
    "   - Weight Update: upravujeme vahy\n",
    "\n",
    "3. **PyTorch** automatizuje backpropagation jednim prikazem: `loss.backward()`\n",
    "\n",
    "4. **Ucici krivka** ukazuje prubeh uceni - chceme videt klesajici trend\n",
    "\n",
    "5. **Learning rate** je klicovy hyperparametr - ani moc maly, ani moc velky\n",
    "\n",
    "### Prakticka doporuceni:\n",
    "- Zacinajte s **Adam optimizerem** a **lr=0.001**\n",
    "- Sledujte ucici krivku pro diagnostiku problemu\n",
    "- Experimentujte s hyperparametry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Kviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kviz\n",
    "print(\"=\"*60)\n",
    "print(\"KVIZ - Zpetna propagace\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "questions = [\n",
    "    {\n",
    "        \"question\": \"1. Co dela backward pass?\",\n",
    "        \"options\": [\"a) Posila data siti\", \"b) Pocita gradienty\", \n",
    "                   \"c) Aktualizuje vahy\", \"d) Pocita loss\"],\n",
    "        \"answer\": \"b\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"2. Jaky PyTorch prikaz provede zpetnou propagaci?\",\n",
    "        \"options\": [\"a) model.forward()\", \"b) optimizer.step()\", \n",
    "                   \"c) loss.backward()\", \"d) model.train()\"],\n",
    "        \"answer\": \"c\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"3. Co ukazuje ucici krivka?\",\n",
    "        \"options\": [\"a) Presnost modelu\", \"b) Vyvoj ztraty v case\", \n",
    "                   \"c) Pocet parametru\", \"d) Strukturu site\"],\n",
    "        \"answer\": \"b\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"4. Co se stane pri prilis velke learning rate?\",\n",
    "        \"options\": [\"a) Pomale uceni\", \"b) Stabilni konvergence\", \n",
    "                   \"c) Nestabilita/divergence\", \"d) Nic se nezmeni\"],\n",
    "        \"answer\": \"c\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"5. Ktery optimizer je doporucen pro zacatecniky?\",\n",
    "        \"options\": [\"a) SGD\", \"b) Adam\", \"c) Adagrad\", \"d) RMSprop\"],\n",
    "        \"answer\": \"b\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{q['question']}\")\n",
    "    for opt in q['options']:\n",
    "        print(f\"   {opt}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ODPOVEDI: 1-b, 2-c, 3-b, 4-c, 5-b\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Vase vyzva\n",
    "\n",
    "1. Zmente learning rate na velmi vysokou hodnotu (napr. 5.0) a sledujte, co se stane\n",
    "2. Zkuste pridat treti vrstvu do site - zmeni se rychlost uceni?\n",
    "3. Experimentujte s poctem neuronu ve skryte vrstve (2, 8, 16)\n",
    "\n",
    "**Tip:** Vzdy sledujte ucici krivku - je to nejlepsi zpusob, jak diagnostikovat problemy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
