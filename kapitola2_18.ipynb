{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Detekce Anom√°li√≠ s Transformery\n",
    "\n",
    "**Autor:** Praut s.r.o. - AI Integration & Business Automation\n",
    "\n",
    "V tomto notebooku se nauƒç√≠me pou≈æ√≠vat Transformer modely pro detekci anom√°li√≠ v r≈Øzn√Ωch typech dat - ƒçasov√Ωch ≈ôad√°ch, textu a tabulkov√Ωch datech.\n",
    "\n",
    "## Obsah\n",
    "1. √övod do detekce anom√°li√≠\n",
    "2. Anom√°lie v ƒçasov√Ωch ≈ôad√°ch (Autoencoder Transformer)\n",
    "3. Anom√°lie v textu (outlier detection)\n",
    "4. Anom√°lie v logech a ud√°lostech\n",
    "5. Produkƒçn√≠ monitoring syst√©m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalace knihoven\n",
    "!pip install transformers sentence-transformers torch pandas numpy scikit-learn matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Kontrola GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Pou≈æ√≠v√°m za≈ô√≠zen√≠: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. √övod do detekce anom√°li√≠\n",
    "\n",
    "Anom√°lie jsou vz√°cn√© vzory v datech, kter√© se v√Ωraznƒõ li≈°√≠ od vƒõt≈°iny. Existuje nƒõkolik p≈ô√≠stup≈Ø:\n",
    "\n",
    "| Metoda | Princip | Pou≈æit√≠ |\n",
    "|--------|---------|--------|\n",
    "| Autoencoder | Rekonstrukƒçn√≠ chyba | ƒåasov√© ≈ôady, obrazy |\n",
    "| Isolation Forest | Izolace vz√°cn√Ωch bod≈Ø | Tabulkov√° data |\n",
    "| Embedding + LOF | Vzd√°lenost v latentn√≠m prostoru | Text, logy |\n",
    "| Attention-based | Neobvykl√© attention vzory | Sekvence |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generov√°n√≠ syntetick√Ωch dat s anom√°liemi\n",
    "\n",
    "def generate_sensor_data_with_anomalies(\n",
    "    n_samples: int = 5000,\n",
    "    anomaly_ratio: float = 0.05,\n",
    "    seed: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generuje syntetick√° data ze senzor≈Ø s anom√°liemi.\n",
    "    Simuluje data z pr≈Ømyslov√©ho stroje.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Norm√°ln√≠ data - periodick√Ω sign√°l s ≈°umem\n",
    "    t = np.linspace(0, 100, n_samples)\n",
    "    \n",
    "    # 4 senzory\n",
    "    sensor1 = np.sin(t * 0.1) + np.random.normal(0, 0.1, n_samples)  # Teplota\n",
    "    sensor2 = np.cos(t * 0.15) + np.random.normal(0, 0.1, n_samples)  # Vibrace\n",
    "    sensor3 = 0.5 * np.sin(t * 0.2) + 0.3 * np.cos(t * 0.1) + np.random.normal(0, 0.05, n_samples)  # Tlak\n",
    "    sensor4 = np.random.normal(0, 0.2, n_samples)  # ≈†um (pro stabilitu)\n",
    "    \n",
    "    data = np.column_stack([sensor1, sensor2, sensor3, sensor4])\n",
    "    \n",
    "    # Oznaƒçen√≠ anom√°li√≠\n",
    "    labels = np.zeros(n_samples)\n",
    "    n_anomalies = int(n_samples * anomaly_ratio)\n",
    "    anomaly_indices = np.random.choice(n_samples, n_anomalies, replace=False)\n",
    "    \n",
    "    # Injekce r≈Øzn√Ωch typ≈Ø anom√°li√≠\n",
    "    for idx in anomaly_indices:\n",
    "        anomaly_type = np.random.choice(['spike', 'drift', 'noise', 'drop'])\n",
    "        sensor_idx = np.random.randint(0, 4)\n",
    "        \n",
    "        if anomaly_type == 'spike':  # N√°hl√Ω skok\n",
    "            data[idx, sensor_idx] += np.random.uniform(2, 4) * np.sign(np.random.randn())\n",
    "        elif anomaly_type == 'drift':  # Postupn√Ω drift\n",
    "            drift_len = min(20, n_samples - idx)\n",
    "            drift = np.linspace(0, np.random.uniform(1, 2), drift_len)\n",
    "            data[idx:idx+drift_len, sensor_idx] += drift\n",
    "        elif anomaly_type == 'noise':  # Zv√Ω≈°en√Ω ≈°um\n",
    "            noise_len = min(10, n_samples - idx)\n",
    "            data[idx:idx+noise_len, sensor_idx] += np.random.normal(0, 1, noise_len)\n",
    "        else:  # Drop - v√Ωpadek\n",
    "            data[idx, sensor_idx] = 0\n",
    "        \n",
    "        labels[idx] = 1\n",
    "    \n",
    "    return data.astype(np.float32), labels.astype(np.int64)\n",
    "\n",
    "\n",
    "# Generov√°n√≠ dat\n",
    "sensor_data, labels = generate_sensor_data_with_anomalies(n_samples=5000, anomaly_ratio=0.05)\n",
    "print(f\"Data shape: {sensor_data.shape}\")\n",
    "print(f\"Anom√°lie: {labels.sum()} ({labels.mean()*100:.1f}%)\")\n",
    "\n",
    "# Vizualizace\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 8), sharex=True)\n",
    "sensor_names = ['Teplota', 'Vibrace', 'Tlak', 'Stabilita']\n",
    "\n",
    "for i, (ax, name) in enumerate(zip(axes, sensor_names)):\n",
    "    ax.plot(sensor_data[:500, i], 'b-', alpha=0.7, linewidth=0.5)\n",
    "    # Oznaƒçen√≠ anom√°li√≠\n",
    "    anomaly_idx = np.where(labels[:500] == 1)[0]\n",
    "    ax.scatter(anomaly_idx, sensor_data[anomaly_idx, i], c='red', s=30, label='Anom√°lie')\n",
    "    ax.set_ylabel(name)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.xlabel('ƒåas')\n",
    "plt.suptitle('Senzorov√° data s anom√°liemi')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Anom√°lie v ƒçasov√Ωch ≈ôad√°ch (Autoencoder Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Pozicov√© k√≥dov√°n√≠ pro Transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Autoencoder pro detekci anom√°li√≠.\n",
    "    Anom√°lie jsou identifikov√°ny jako vzorky s vysokou rekonstrukƒçn√≠ chybou.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        d_model: int = 64,\n",
    "        n_heads: int = 4,\n",
    "        n_encoder_layers: int = 2,\n",
    "        n_decoder_layers: int = 2,\n",
    "        d_ff: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        latent_dim: int = 32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(n_features, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_encoder_layers)\n",
    "        \n",
    "        # Bottleneck (latent space)\n",
    "        self.to_latent = nn.Linear(d_model, latent_dim)\n",
    "        self.from_latent = nn.Linear(latent_dim, d_model)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_decoder_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, n_features)\n",
    "        \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Zak√≥duje vstup do latentn√≠ho prostoru.\"\"\"\n",
    "        x = self.input_projection(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        encoded = self.encoder(x)\n",
    "        latent = self.to_latent(encoded)\n",
    "        return latent\n",
    "    \n",
    "    def decode(self, latent: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Dek√≥duje z latentn√≠ho prostoru.\"\"\"\n",
    "        memory = self.from_latent(latent)\n",
    "        target_emb = self.input_projection(target)\n",
    "        target_emb = self.positional_encoding(target_emb)\n",
    "        decoded = self.decoder(target_emb, memory)\n",
    "        output = self.output_projection(decoded)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            reconstruction: Rekonstruovan√Ω vstup\n",
    "            latent: Latentn√≠ reprezentace\n",
    "        \"\"\"\n",
    "        latent = self.encode(x)\n",
    "        reconstruction = self.decode(latent, x)\n",
    "        return reconstruction, latent\n",
    "\n",
    "\n",
    "print(\"Model vytvo≈ôen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Dataset pro ƒçasov√© ≈ôady.\"\"\"\n",
    "    \n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 50, stride: int = 1):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        # ≈†k√°lov√°n√≠\n",
    "        self.scaler = StandardScaler()\n",
    "        self.data_scaled = self.scaler.fit_transform(data)\n",
    "        \n",
    "        # Vytvo≈ôen√≠ index≈Ø oken\n",
    "        self.indices = list(range(0, len(data) - window_size + 1, stride))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.indices[idx]\n",
    "        window = self.data_scaled[start_idx:start_idx + self.window_size]\n",
    "        \n",
    "        # Label je 1 pokud je v oknƒõ alespo≈à jedna anom√°lie\n",
    "        window_labels = self.labels[start_idx:start_idx + self.window_size]\n",
    "        is_anomaly = 1 if window_labels.sum() > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'data': torch.FloatTensor(window),\n",
    "            'label': torch.tensor(is_anomaly, dtype=torch.long),\n",
    "            'start_idx': start_idx\n",
    "        }\n",
    "\n",
    "\n",
    "# Vytvo≈ôen√≠ datasetu (pouze norm√°ln√≠ data pro tr√©nov√°n√≠)\n",
    "normal_mask = labels == 0\n",
    "train_data = sensor_data[normal_mask][:3000]  # Prvn√≠ 3000 norm√°ln√≠ch vzork≈Ø\n",
    "train_labels = np.zeros(len(train_data))\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, train_labels, window_size=50, stride=5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Test dataset obsahuje i anom√°lie\n",
    "test_dataset = TimeSeriesDataset(sensor_data[3000:], labels[3000:], window_size=50, stride=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train windows: {len(train_dataset)}\")\n",
    "print(f\"Test windows: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tr√©nov√°n√≠ autoencoderu\n",
    "\n",
    "n_features = sensor_data.shape[1]\n",
    "model = TransformerAutoencoder(\n",
    "    n_features=n_features,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=2,\n",
    "    latent_dim=32\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 30\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        data = batch['data'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, _ = model(data)\n",
    "        loss = criterion(reconstruction, data)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} - Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Vizualizace loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detekce anom√°li√≠ na testovac√≠ch datech\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_reconstruction_errors(model: nn.Module, dataloader: DataLoader, device: torch.device) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Vypoƒç√≠t√° rekonstrukƒçn√≠ chyby pro v≈°echna okna.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    errors = []\n",
    "    all_labels = []\n",
    "    indices = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        data = batch['data'].to(device)\n",
    "        labels = batch['label'].numpy()\n",
    "        start_idx = batch['start_idx'].numpy()\n",
    "        \n",
    "        reconstruction, _ = model(data)\n",
    "        \n",
    "        # MSE pro ka≈æd√© okno\n",
    "        mse = ((reconstruction - data) ** 2).mean(dim=(1, 2)).cpu().numpy()\n",
    "        \n",
    "        errors.extend(mse)\n",
    "        all_labels.extend(labels)\n",
    "        indices.extend(start_idx)\n",
    "    \n",
    "    return np.array(errors), np.array(all_labels), np.array(indices)\n",
    "\n",
    "\n",
    "# V√Ωpoƒçet rekonstrukƒçn√≠ch chyb\n",
    "errors, test_labels, test_indices = compute_reconstruction_errors(model, test_loader, device)\n",
    "\n",
    "print(f\"Pr≈Ømƒõrn√° chyba: {errors.mean():.6f}\")\n",
    "print(f\"Std chyby: {errors.std():.6f}\")\n",
    "\n",
    "# Urƒçen√≠ prahu pro anom√°lie (nap≈ô. 95. percentil)\n",
    "threshold = np.percentile(errors, 95)\n",
    "print(f\"Pr√°h (95. percentil): {threshold:.6f}\")\n",
    "\n",
    "# Predikce anom√°li√≠\n",
    "predictions = (errors > threshold).astype(int)\n",
    "\n",
    "# Evaluace\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, predictions, target_names=['Norm√°ln√≠', 'Anom√°lie']))\n",
    "\n",
    "try:\n",
    "    auc = roc_auc_score(test_labels, errors)\n",
    "    print(f\"\\nAUC-ROC: {auc:.4f}\")\n",
    "except:\n",
    "    print(\"AUC nelze vypoƒç√≠tat (pouze jedna t≈ô√≠da)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizace rekonstrukƒçn√≠ch chyb\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Horn√≠ graf - rekonstrukƒçn√≠ chyby\n",
    "ax1 = axes[0]\n",
    "ax1.plot(test_indices, errors, 'b-', alpha=0.7, linewidth=0.5, label='Rekonstrukƒçn√≠ chyba')\n",
    "ax1.axhline(y=threshold, color='r', linestyle='--', label=f'Pr√°h ({threshold:.4f})')\n",
    "\n",
    "# Oznaƒçen√≠ skuteƒçn√Ωch anom√°li√≠\n",
    "anomaly_mask = test_labels == 1\n",
    "ax1.scatter(test_indices[anomaly_mask], errors[anomaly_mask], c='red', s=20, alpha=0.5, label='Skuteƒçn√© anom√°lie')\n",
    "\n",
    "ax1.set_ylabel('Rekonstrukƒçn√≠ chyba')\n",
    "ax1.set_title('Detekce anom√°li√≠ pomoc√≠ Transformer Autoencoderu')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Doln√≠ graf - distribuce chyb\n",
    "ax2 = axes[1]\n",
    "ax2.hist(errors[test_labels == 0], bins=50, alpha=0.7, label='Norm√°ln√≠', density=True)\n",
    "ax2.hist(errors[test_labels == 1], bins=50, alpha=0.7, label='Anom√°lie', density=True)\n",
    "ax2.axvline(x=threshold, color='r', linestyle='--', label='Pr√°h')\n",
    "ax2.set_xlabel('Rekonstrukƒçn√≠ chyba')\n",
    "ax2.set_ylabel('Hustota')\n",
    "ax2.set_title('Distribuce rekonstrukƒçn√≠ch chyb')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anom√°lie v textu (outlier detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Detektor anom√°li√≠ v textu pomoc√≠ Sentence Transformers.\n",
    "    Identifikuje texty, kter√© jsou s√©manticky odli≈°n√© od vƒõt≈°iny.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        print(f\"Naƒç√≠t√°n√≠ modelu {model_name}...\")\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.embeddings = None\n",
    "        self.texts = None\n",
    "        self.detector = None\n",
    "        self.centroid = None\n",
    "        self.threshold = None\n",
    "        \n",
    "    def fit(self, texts: List[str], contamination: float = 0.05):\n",
    "        \"\"\"\n",
    "        Natr√©nuje detektor na kolekci text≈Ø.\n",
    "        \n",
    "        Args:\n",
    "            texts: Seznam text≈Ø\n",
    "            contamination: Oƒçek√°van√Ω pod√≠l anom√°li√≠\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        print(f\"Vytv√°≈ôen√≠ embedding≈Ø pro {len(texts)} text≈Ø...\")\n",
    "        \n",
    "        # Vytvo≈ôen√≠ embedding≈Ø\n",
    "        self.embeddings = self.encoder.encode(\n",
    "            texts,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # V√Ωpoƒçet centroidu (pr≈Ømƒõrn√Ω embedding)\n",
    "        self.centroid = self.embeddings.mean(axis=0)\n",
    "        \n",
    "        # Tr√©nov√°n√≠ Isolation Forest\n",
    "        self.detector = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            random_state=42,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        self.detector.fit(self.embeddings)\n",
    "        \n",
    "        # V√Ωpoƒçet prahu na z√°kladƒõ vzd√°lenosti od centroidu\n",
    "        distances = np.linalg.norm(self.embeddings - self.centroid, axis=1)\n",
    "        self.threshold = np.percentile(distances, (1 - contamination) * 100)\n",
    "        \n",
    "        print(f\"Model natr√©nov√°n. Pr√°h vzd√°lenosti: {self.threshold:.4f}\")\n",
    "        \n",
    "    def detect_anomalies(self, texts: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Detekuje anom√°lie v nov√Ωch textech.\n",
    "        \n",
    "        Returns:\n",
    "            Seznam slovn√≠k≈Ø s informacemi o ka≈æd√©m textu\n",
    "        \"\"\"\n",
    "        # Vytvo≈ôen√≠ embedding≈Ø\n",
    "        embeddings = self.encoder.encode(texts, convert_to_numpy=True)\n",
    "        \n",
    "        # Isolation Forest predikce\n",
    "        if_predictions = self.detector.predict(embeddings)\n",
    "        if_scores = self.detector.decision_function(embeddings)\n",
    "        \n",
    "        # Vzd√°lenost od centroidu\n",
    "        distances = np.linalg.norm(embeddings - self.centroid, axis=1)\n",
    "        \n",
    "        results = []\n",
    "        for i, text in enumerate(texts):\n",
    "            is_anomaly_if = if_predictions[i] == -1\n",
    "            is_anomaly_dist = distances[i] > self.threshold\n",
    "            \n",
    "            results.append({\n",
    "                'text': text[:100] + '...' if len(text) > 100 else text,\n",
    "                'is_anomaly': is_anomaly_if or is_anomaly_dist,\n",
    "                'isolation_forest_anomaly': is_anomaly_if,\n",
    "                'distance_anomaly': is_anomaly_dist,\n",
    "                'anomaly_score': float(-if_scores[i]),  # Vy≈°≈°√≠ = v√≠ce anom√°ln√≠\n",
    "                'distance_from_centroid': float(distances[i])\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def find_most_anomalous(self, n: int = 5) -> List[Tuple[int, str, float]]:\n",
    "        \"\"\"Najde N nejv√≠ce anom√°ln√≠ch text≈Ø v tr√©novac√≠ch datech.\"\"\"\n",
    "        distances = np.linalg.norm(self.embeddings - self.centroid, axis=1)\n",
    "        top_indices = np.argsort(distances)[-n:][::-1]\n",
    "        \n",
    "        return [\n",
    "            (int(idx), self.texts[idx], float(distances[idx]))\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "\n",
    "\n",
    "# P≈ô√≠klad textov√Ωch dat - e-shop recenze\n",
    "reviews = [\n",
    "    # Norm√°ln√≠ recenze\n",
    "    \"Skvƒõl√Ω produkt, jsem velmi spokojen√Ω s kvalitou.\",\n",
    "    \"Doruƒçen√≠ bylo rychl√© a zbo≈æ√≠ v po≈ô√°dku.\",\n",
    "    \"Doporuƒçuji, v√Ωborn√Ω pomƒõr cena/v√Ωkon.\",\n",
    "    \"Produkt odpov√≠d√° popisu, bez probl√©m≈Ø.\",\n",
    "    \"Kvalitn√≠ zpracov√°n√≠, splnilo oƒçek√°v√°n√≠.\",\n",
    "    \"Rychl√© dod√°n√≠, dob≈ôe zabalen√©.\",\n",
    "    \"Super obchod, urƒçitƒõ nakoup√≠m znovu.\",\n",
    "    \"Zbo≈æ√≠ v perfektn√≠m stavu, dƒõkuji.\",\n",
    "    \"P≈ôesnƒõ to co jsem hledal, spokojenost.\",\n",
    "    \"Bezprobl√©mov√° komunikace, v≈ôele doporuƒçuji.\",\n",
    "    \"V√Ωborn√° kvalita za rozumnou cenu.\",\n",
    "    \"Produkt p≈ôekonal m√© oƒçek√°v√°n√≠.\",\n",
    "    \"V≈°echno probƒõhlo hladce, d√≠ky.\",\n",
    "    \"Skvƒõl√° z√°kaznick√° podpora.\",\n",
    "    \"Urƒçitƒõ doporuƒçuji tento obchod.\",\n",
    "    # Anom√°ln√≠ recenze (spam, podez≈ôel√©)\n",
    "    \"SUPER V√ùHRA!!! Klikni sem a vyhraj iPhone!!!\",\n",
    "    \"asdfghjkl random text bla bla\",\n",
    "    \"Nav≈°tivte www.podvodny-web.cz pro slevy!\",\n",
    "    \"üéÅüéÅüéÅ FREE MONEY üí∞üí∞üí∞ click here!!!\",\n",
    "    \"The quick brown fox jumps over lazy dog\",  # Anglicky v ƒçesk√©m kontextu\n",
    "]\n",
    "\n",
    "# Tr√©nov√°n√≠ detektoru\n",
    "text_detector = TextAnomalyDetector()\n",
    "text_detector.fit(reviews[:15], contamination=0.1)  # Tr√©nov√°n√≠ jen na norm√°ln√≠ch\n",
    "\n",
    "# Detekce anom√°li√≠\n",
    "print(\"\\n--- Detekce anom√°li√≠ v textech ---\")\n",
    "results = text_detector.detect_anomalies(reviews)\n",
    "\n",
    "for result in results:\n",
    "    status = \"‚ùå ANOM√ÅLIE\" if result['is_anomaly'] else \"‚úì OK\"\n",
    "    print(f\"{status}: {result['text'][:50]}... (sk√≥re: {result['anomaly_score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anom√°lie v logech a ud√°lostech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LogEntry:\n",
    "    \"\"\"Reprezentace z√°znamu logu.\"\"\"\n",
    "    timestamp: datetime\n",
    "    level: str\n",
    "    source: str\n",
    "    message: str\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class LogAnomalyDetector:\n",
    "    \"\"\"\n",
    "    Detektor anom√°li√≠ v syst√©mov√Ωch logech.\n",
    "    Kombinuje anal√Ωzu frekvence, embeddingy a pattern matching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.known_patterns = {}\n",
    "        self.frequency_baseline = {}\n",
    "        self.embedding_detector = None\n",
    "        self.embeddings_cache = {}\n",
    "        \n",
    "    def learn_patterns(self, logs: List[LogEntry], window_minutes: int = 60):\n",
    "        \"\"\"\n",
    "        Nauƒç√≠ se norm√°ln√≠ vzory z historick√Ωch log≈Ø.\n",
    "        \"\"\"\n",
    "        print(f\"Uƒçen√≠ vzor≈Ø z {len(logs)} log≈Ø...\")\n",
    "        \n",
    "        # 1. Frekvence podle level a source\n",
    "        level_counts = {}\n",
    "        source_counts = {}\n",
    "        \n",
    "        for log in logs:\n",
    "            level_counts[log.level] = level_counts.get(log.level, 0) + 1\n",
    "            source_counts[log.source] = source_counts.get(log.source, 0) + 1\n",
    "        \n",
    "        total = len(logs)\n",
    "        self.frequency_baseline = {\n",
    "            'levels': {k: v / total for k, v in level_counts.items()},\n",
    "            'sources': {k: v / total for k, v in source_counts.items()}\n",
    "        }\n",
    "        \n",
    "        # 2. Embeddingy zpr√°v pro detekci neobvykl√Ωch\n",
    "        messages = [log.message for log in logs]\n",
    "        embeddings = self.encoder.encode(messages, show_progress_bar=True)\n",
    "        \n",
    "        # Isolation Forest na embedding√°ch\n",
    "        self.embedding_detector = IsolationForest(\n",
    "            contamination=0.05,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.embedding_detector.fit(embeddings)\n",
    "        \n",
    "        # 3. Ulo≈æen√≠ zn√°m√Ωch vzor≈Ø\n",
    "        unique_messages = set(messages)\n",
    "        for msg in unique_messages:\n",
    "            # Normalizace zpr√°vy (odstranƒõn√≠ ƒç√≠sel, ƒças≈Ø)\n",
    "            normalized = self._normalize_message(msg)\n",
    "            self.known_patterns[normalized] = self.known_patterns.get(normalized, 0) + 1\n",
    "        \n",
    "        print(f\"Nauƒçeno {len(self.known_patterns)} unik√°tn√≠ch vzor≈Ø\")\n",
    "        print(f\"Baseline √∫rovnƒõ: {self.frequency_baseline['levels']}\")\n",
    "    \n",
    "    def _normalize_message(self, message: str) -> str:\n",
    "        \"\"\"Normalizuje zpr√°vu odstranƒõn√≠m promƒõnn√Ωch ƒç√°st√≠.\"\"\"\n",
    "        import re\n",
    "        # Nahrazen√≠ ƒç√≠sel\n",
    "        normalized = re.sub(r'\\d+', '<NUM>', message)\n",
    "        # Nahrazen√≠ IP adres\n",
    "        normalized = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '<IP>', normalized)\n",
    "        # Nahrazen√≠ UUID\n",
    "        normalized = re.sub(r'[a-f0-9-]{36}', '<UUID>', normalized)\n",
    "        return normalized\n",
    "    \n",
    "    def analyze_log(self, log: LogEntry) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyzuje jednotliv√Ω log a vr√°t√≠ sk√≥re anom√°lie.\n",
    "        \"\"\"\n",
    "        anomaly_indicators = []\n",
    "        scores = {}\n",
    "        \n",
    "        # 1. Kontrola √∫rovnƒõ\n",
    "        level_freq = self.frequency_baseline['levels'].get(log.level, 0)\n",
    "        if level_freq < 0.01:  # Vz√°cn√° √∫rove≈à\n",
    "            anomaly_indicators.append(f\"Vz√°cn√° √∫rove≈à: {log.level}\")\n",
    "        scores['level_rarity'] = 1 - level_freq\n",
    "        \n",
    "        # 2. Kontrola zdroje\n",
    "        source_freq = self.frequency_baseline['sources'].get(log.source, 0)\n",
    "        if source_freq < 0.01:  # Nezn√°m√Ω zdroj\n",
    "            anomaly_indicators.append(f\"Nezn√°m√Ω zdroj: {log.source}\")\n",
    "        scores['source_rarity'] = 1 - source_freq\n",
    "        \n",
    "        # 3. Embedding anom√°lie\n",
    "        embedding = self.encoder.encode([log.message])\n",
    "        if_score = -self.embedding_detector.decision_function(embedding)[0]\n",
    "        if if_score > 0.1:  # Anom√°ln√≠ embedding\n",
    "            anomaly_indicators.append(\"Neobvykl√° zpr√°va (embedding)\")\n",
    "        scores['embedding_anomaly'] = max(0, if_score)\n",
    "        \n",
    "        # 4. Pattern matching\n",
    "        normalized = self._normalize_message(log.message)\n",
    "        if normalized not in self.known_patterns:\n",
    "            anomaly_indicators.append(\"Nezn√°m√Ω vzor zpr√°vy\")\n",
    "            scores['pattern_novelty'] = 1.0\n",
    "        else:\n",
    "            scores['pattern_novelty'] = 0.0\n",
    "        \n",
    "        # 5. V√Ωpoƒçet celkov√©ho sk√≥re\n",
    "        weights = {\n",
    "            'level_rarity': 0.2,\n",
    "            'source_rarity': 0.2,\n",
    "            'embedding_anomaly': 0.4,\n",
    "            'pattern_novelty': 0.2\n",
    "        }\n",
    "        total_score = sum(scores[k] * weights[k] for k in weights)\n",
    "        \n",
    "        return {\n",
    "            'log': log,\n",
    "            'is_anomaly': total_score > 0.3 or log.level == 'CRITICAL',\n",
    "            'anomaly_score': total_score,\n",
    "            'indicators': anomaly_indicators,\n",
    "            'detailed_scores': scores\n",
    "        }\n",
    "    \n",
    "    def analyze_batch(self, logs: List[LogEntry]) -> List[Dict]:\n",
    "        \"\"\"Analyzuje batch log≈Ø.\"\"\"\n",
    "        return [self.analyze_log(log) for log in logs]\n",
    "\n",
    "\n",
    "# Vytvo≈ôen√≠ syntetick√Ωch log≈Ø\n",
    "def generate_logs(n_normal: int = 200, n_anomalous: int = 20) -> List[LogEntry]:\n",
    "    \"\"\"Generuje syntetick√© logy.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    logs = []\n",
    "    \n",
    "    # Norm√°ln√≠ zpr√°vy\n",
    "    normal_messages = [\n",
    "        \"Request processed successfully in {ms}ms\",\n",
    "        \"User {user_id} logged in from {ip}\",\n",
    "        \"Database query completed in {ms}ms\",\n",
    "        \"Cache hit for key {key}\",\n",
    "        \"Background job {job_id} completed\",\n",
    "        \"Health check passed\",\n",
    "        \"Connection established to {service}\",\n",
    "        \"Metrics exported successfully\",\n",
    "    ]\n",
    "    \n",
    "    sources = ['api-server', 'database', 'cache', 'worker', 'gateway']\n",
    "    \n",
    "    for i in range(n_normal):\n",
    "        msg_template = np.random.choice(normal_messages)\n",
    "        msg = msg_template.format(\n",
    "            ms=np.random.randint(10, 500),\n",
    "            user_id=np.random.randint(1000, 9999),\n",
    "            ip=f\"{np.random.randint(1,255)}.{np.random.randint(1,255)}.{np.random.randint(1,255)}.{np.random.randint(1,255)}\",\n",
    "            key=f\"cache_key_{np.random.randint(1,100)}\",\n",
    "            job_id=f\"job_{np.random.randint(1,1000)}\",\n",
    "            service=np.random.choice(['redis', 'postgres', 'elasticsearch'])\n",
    "        )\n",
    "        \n",
    "        logs.append(LogEntry(\n",
    "            timestamp=datetime.now() - timedelta(minutes=np.random.randint(0, 1440)),\n",
    "            level=np.random.choice(['INFO', 'DEBUG', 'INFO', 'INFO', 'WARNING']),\n",
    "            source=np.random.choice(sources),\n",
    "            message=msg\n",
    "        ))\n",
    "    \n",
    "    # Anom√°ln√≠ logy\n",
    "    anomalous_messages = [\n",
    "        \"SECURITY: Multiple failed login attempts from {ip}\",\n",
    "        \"ERROR: Database connection timeout after 30s\",\n",
    "        \"CRITICAL: Disk space below 5%\",\n",
    "        \"ALERT: Unusual traffic spike detected\",\n",
    "        \"ERROR: Unhandled exception in payment processing\",\n",
    "        \"WARNING: SSL certificate expires in 2 days\",\n",
    "        \"CRITICAL: Memory usage above 95%\",\n",
    "        \"SECURITY: SQL injection attempt detected\",\n",
    "    ]\n",
    "    \n",
    "    for i in range(n_anomalous):\n",
    "        msg = np.random.choice(anomalous_messages).format(\n",
    "            ip=f\"{np.random.randint(1,255)}.{np.random.randint(1,255)}.{np.random.randint(1,255)}.{np.random.randint(1,255)}\"\n",
    "        )\n",
    "        \n",
    "        level = 'ERROR' if 'ERROR' in msg else ('CRITICAL' if 'CRITICAL' in msg else 'WARNING')\n",
    "        \n",
    "        logs.append(LogEntry(\n",
    "            timestamp=datetime.now() - timedelta(minutes=np.random.randint(0, 1440)),\n",
    "            level=level,\n",
    "            source=np.random.choice(sources + ['unknown-service']),\n",
    "            message=msg\n",
    "        ))\n",
    "    \n",
    "    return logs\n",
    "\n",
    "\n",
    "# Generov√°n√≠ a anal√Ωza log≈Ø\n",
    "all_logs = generate_logs(n_normal=200, n_anomalous=20)\n",
    "\n",
    "# Tr√©nov√°n√≠ na norm√°ln√≠ch log√°ch (prvn√≠ch 150)\n",
    "log_detector = LogAnomalyDetector()\n",
    "log_detector.learn_patterns(all_logs[:150])\n",
    "\n",
    "# Anal√Ωza v≈°ech log≈Ø\n",
    "print(\"\\n--- Anal√Ωza log≈Ø ---\")\n",
    "results = log_detector.analyze_batch(all_logs[150:])\n",
    "\n",
    "# Zobrazen√≠ anom√°li√≠\n",
    "print(\"\\nDetekovan√© anom√°lie:\")\n",
    "for result in sorted(results, key=lambda x: -x['anomaly_score'])[:10]:\n",
    "    if result['is_anomaly']:\n",
    "        log = result['log']\n",
    "        print(f\"\\n[{log.level}] {log.source}: {log.message[:60]}...\")\n",
    "        print(f\"  Sk√≥re: {result['anomaly_score']:.3f}\")\n",
    "        print(f\"  Indik√°tory: {', '.join(result['indicators'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Produkƒçn√≠ monitoring syst√©m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Alert:\n",
    "    \"\"\"Reprezentace alertu.\"\"\"\n",
    "    timestamp: datetime\n",
    "    severity: str  # 'low', 'medium', 'high', 'critical'\n",
    "    source: str\n",
    "    message: str\n",
    "    anomaly_score: float\n",
    "    details: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class ProductionAnomalyMonitor:\n",
    "    \"\"\"\n",
    "    Produkƒçn√≠ syst√©m pro real-time detekci anom√°li√≠.\n",
    "    Kombinuje v√≠ce detektor≈Ø a poskytuje alerting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        time_series_model: TransformerAutoencoder,\n",
    "        text_detector: TextAnomalyDetector,\n",
    "        log_detector: LogAnomalyDetector,\n",
    "        device: torch.device,\n",
    "        alert_threshold: float = 0.5,\n",
    "        window_size: int = 50\n",
    "    ):\n",
    "        self.ts_model = time_series_model\n",
    "        self.text_detector = text_detector\n",
    "        self.log_detector = log_detector\n",
    "        self.device = device\n",
    "        self.alert_threshold = alert_threshold\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Bufery pro streaming data\n",
    "        self.sensor_buffer = deque(maxlen=window_size * 2)\n",
    "        \n",
    "        # Alerty\n",
    "        self.alerts = []\n",
    "        self.alert_callbacks = []\n",
    "        \n",
    "        # Statistiky\n",
    "        self.stats = {\n",
    "            'sensor_checks': 0,\n",
    "            'text_checks': 0,\n",
    "            'log_checks': 0,\n",
    "            'alerts_raised': 0,\n",
    "            'start_time': datetime.now()\n",
    "        }\n",
    "        \n",
    "        # Scaler pro senzory (mƒõl by b√Ωt natr√©novan√Ω na historick√Ωch datech)\n",
    "        self.sensor_scaler = StandardScaler()\n",
    "        self.sensor_scaler_fitted = False\n",
    "        \n",
    "    def register_alert_callback(self, callback):\n",
    "        \"\"\"Registruje callback pro nov√© alerty.\"\"\"\n",
    "        self.alert_callbacks.append(callback)\n",
    "        \n",
    "    def _raise_alert(self, alert: Alert):\n",
    "        \"\"\"Vytvo≈ô√≠ nov√Ω alert.\"\"\"\n",
    "        self.alerts.append(alert)\n",
    "        self.stats['alerts_raised'] += 1\n",
    "        \n",
    "        # Zavol√°n√≠ callback≈Ø\n",
    "        for callback in self.alert_callbacks:\n",
    "            try:\n",
    "                callback(alert)\n",
    "            except Exception as e:\n",
    "                print(f\"Chyba v alert callback: {e}\")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def process_sensor_data(self, data_point: np.ndarray) -> Optional[Alert]:\n",
    "        \"\"\"\n",
    "        Zpracuje nov√Ω bod senzorov√Ωch dat.\n",
    "        Vr√°t√≠ alert pokud je detekov√°na anom√°lie.\n",
    "        \"\"\"\n",
    "        self.stats['sensor_checks'] += 1\n",
    "        \n",
    "        # P≈ôid√°n√≠ do bufferu\n",
    "        self.sensor_buffer.append(data_point)\n",
    "        \n",
    "        # Pot≈ôebujeme alespo≈à window_size bod≈Ø\n",
    "        if len(self.sensor_buffer) < self.window_size:\n",
    "            return None\n",
    "        \n",
    "        # Fit scaler p≈ôi prvn√≠m pou≈æit√≠\n",
    "        if not self.sensor_scaler_fitted:\n",
    "            self.sensor_scaler.fit(list(self.sensor_buffer))\n",
    "            self.sensor_scaler_fitted = True\n",
    "        \n",
    "        # P≈ô√≠prava dat\n",
    "        window = np.array(list(self.sensor_buffer)[-self.window_size:])\n",
    "        window_scaled = self.sensor_scaler.transform(window)\n",
    "        window_tensor = torch.FloatTensor(window_scaled).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Predikce\n",
    "        self.ts_model.eval()\n",
    "        reconstruction, _ = self.ts_model(window_tensor)\n",
    "        \n",
    "        # Rekonstrukƒçn√≠ chyba\n",
    "        mse = ((reconstruction - window_tensor) ** 2).mean().item()\n",
    "        \n",
    "        # Kontrola prahu\n",
    "        if mse > self.alert_threshold:\n",
    "            severity = 'critical' if mse > self.alert_threshold * 2 else 'high' if mse > self.alert_threshold * 1.5 else 'medium'\n",
    "            \n",
    "            alert = Alert(\n",
    "                timestamp=datetime.now(),\n",
    "                severity=severity,\n",
    "                source='sensor_monitor',\n",
    "                message=f\"Anom√°lie v senzorov√Ωch datech detekov√°na (MSE: {mse:.4f})\",\n",
    "                anomaly_score=mse,\n",
    "                details={\n",
    "                    'window_end_values': data_point.tolist(),\n",
    "                    'reconstruction_error': mse\n",
    "                }\n",
    "            )\n",
    "            self._raise_alert(alert)\n",
    "            return alert\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def process_text(self, text: str, source: str = 'text_input') -> Optional[Alert]:\n",
    "        \"\"\"\n",
    "        Analyzuje text na anom√°lie.\n",
    "        \"\"\"\n",
    "        self.stats['text_checks'] += 1\n",
    "        \n",
    "        results = self.text_detector.detect_anomalies([text])\n",
    "        result = results[0]\n",
    "        \n",
    "        if result['is_anomaly']:\n",
    "            severity = 'high' if result['anomaly_score'] > 0.8 else 'medium'\n",
    "            \n",
    "            alert = Alert(\n",
    "                timestamp=datetime.now(),\n",
    "                severity=severity,\n",
    "                source=source,\n",
    "                message=f\"Anom√°ln√≠ text detekov√°n: {text[:50]}...\",\n",
    "                anomaly_score=result['anomaly_score'],\n",
    "                details=result\n",
    "            )\n",
    "            self._raise_alert(alert)\n",
    "            return alert\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def process_log(self, log: LogEntry) -> Optional[Alert]:\n",
    "        \"\"\"\n",
    "        Analyzuje log na anom√°lie.\n",
    "        \"\"\"\n",
    "        self.stats['log_checks'] += 1\n",
    "        \n",
    "        result = self.log_detector.analyze_log(log)\n",
    "        \n",
    "        if result['is_anomaly']:\n",
    "            severity_map = {\n",
    "                'CRITICAL': 'critical',\n",
    "                'ERROR': 'high',\n",
    "                'WARNING': 'medium'\n",
    "            }\n",
    "            severity = severity_map.get(log.level, 'low')\n",
    "            \n",
    "            alert = Alert(\n",
    "                timestamp=datetime.now(),\n",
    "                severity=severity,\n",
    "                source='log_monitor',\n",
    "                message=f\"Anom√°ln√≠ log [{log.level}]: {log.message[:50]}...\",\n",
    "                anomaly_score=result['anomaly_score'],\n",
    "                details={\n",
    "                    'log_level': log.level,\n",
    "                    'log_source': log.source,\n",
    "                    'indicators': result['indicators']\n",
    "                }\n",
    "            )\n",
    "            self._raise_alert(alert)\n",
    "            return alert\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        \"\"\"Vr√°t√≠ statistiky monitoringu.\"\"\"\n",
    "        runtime = (datetime.now() - self.stats['start_time']).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            'runtime_seconds': runtime,\n",
    "            'sensor_checks': self.stats['sensor_checks'],\n",
    "            'text_checks': self.stats['text_checks'],\n",
    "            'log_checks': self.stats['log_checks'],\n",
    "            'total_alerts': self.stats['alerts_raised'],\n",
    "            'alert_rate': self.stats['alerts_raised'] / max(1, runtime) * 60,  # per minute\n",
    "            'alerts_by_severity': self._count_alerts_by_severity()\n",
    "        }\n",
    "    \n",
    "    def _count_alerts_by_severity(self) -> Dict[str, int]:\n",
    "        \"\"\"Poƒç√≠t√° alerty podle severity.\"\"\"\n",
    "        counts = {'low': 0, 'medium': 0, 'high': 0, 'critical': 0}\n",
    "        for alert in self.alerts:\n",
    "            counts[alert.severity] = counts.get(alert.severity, 0) + 1\n",
    "        return counts\n",
    "    \n",
    "    def get_recent_alerts(self, n: int = 10) -> List[Alert]:\n",
    "        \"\"\"Vr√°t√≠ N nejnovƒõj≈°√≠ch alert≈Ø.\"\"\"\n",
    "        return sorted(self.alerts, key=lambda x: x.timestamp, reverse=True)[:n]\n",
    "\n",
    "\n",
    "# Vytvo≈ôen√≠ produkƒçn√≠ho monitoru\n",
    "monitor = ProductionAnomalyMonitor(\n",
    "    time_series_model=model,\n",
    "    text_detector=text_detector,\n",
    "    log_detector=log_detector,\n",
    "    device=device,\n",
    "    alert_threshold=0.3,\n",
    "    window_size=50\n",
    ")\n",
    "\n",
    "# Registrace callback pro alerty\n",
    "def alert_handler(alert: Alert):\n",
    "    print(f\"üö® [{alert.severity.upper()}] {alert.message}\")\n",
    "\n",
    "monitor.register_alert_callback(alert_handler)\n",
    "\n",
    "print(\"Produkƒçn√≠ monitor nastaven.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulace real-time monitoringu\n",
    "\n",
    "print(\"=== Simulace real-time monitoringu ===\")\n",
    "print(\"\\n1. Streaming senzorov√Ωch dat:\")\n",
    "\n",
    "# Simulace streaming dat (prvn√≠ch 100 bod≈Ø)\n",
    "for i in range(100):\n",
    "    # Norm√°ln√≠ data\n",
    "    data_point = sensor_data[i]\n",
    "    monitor.process_sensor_data(data_point)\n",
    "\n",
    "# Injekce anom√°lie\n",
    "print(\"\\n(Injekce anom√°lie do senzorov√Ωch dat...)\")\n",
    "anomalous_point = sensor_data[100].copy()\n",
    "anomalous_point[0] += 5  # Spike v teplotƒõ\n",
    "monitor.process_sensor_data(anomalous_point)\n",
    "\n",
    "print(\"\\n2. Kontrola textov√Ωch vstup≈Ø:\")\n",
    "texts_to_check = [\n",
    "    \"Norm√°ln√≠ zpr√°va od z√°kazn√≠ka.\",\n",
    "    \"KLIKNI SEM PRO V√ùHRU!!! www.spam.cz\",\n",
    "    \"Produkt je v po≈ô√°dku, dƒõkuji.\"\n",
    "]\n",
    "\n",
    "for text in texts_to_check:\n",
    "    monitor.process_text(text, source='customer_feedback')\n",
    "\n",
    "print(\"\\n3. Kontrola log≈Ø:\")\n",
    "test_logs = [\n",
    "    LogEntry(datetime.now(), 'INFO', 'api-server', 'Request processed in 45ms'),\n",
    "    LogEntry(datetime.now(), 'CRITICAL', 'database', 'Connection pool exhausted'),\n",
    "    LogEntry(datetime.now(), 'ERROR', 'unknown-service', 'Segmentation fault in worker')\n",
    "]\n",
    "\n",
    "for log in test_logs:\n",
    "    monitor.process_log(log)\n",
    "\n",
    "# Zobrazen√≠ statistik\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Statistiky monitoringu:\")\n",
    "stats = monitor.get_statistics()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nNejnovƒõj≈°√≠ alerty:\")\n",
    "for alert in monitor.get_recent_alerts(5):\n",
    "    print(f\"  [{alert.severity}] {alert.timestamp.strftime('%H:%M:%S')}: {alert.message[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrnut√≠\n",
    "\n",
    "V tomto notebooku jsme vytvo≈ôili komplexn√≠ syst√©m pro detekci anom√°li√≠:\n",
    "\n",
    "1. **Transformer Autoencoder** - Detekce anom√°li√≠ v ƒçasov√Ωch ≈ôad√°ch pomoc√≠ rekonstrukƒçn√≠ chyby\n",
    "2. **Text Anomaly Detector** - Identifikace anom√°ln√≠ch text≈Ø pomoc√≠ embedding≈Ø a Isolation Forest\n",
    "3. **Log Analyzer** - Kombinace pattern matching, frekvence a s√©mantick√© anal√Ωzy\n",
    "4. **Production Monitor** - Real-time syst√©m kombinuj√≠c√≠ v≈°echny detektory\n",
    "\n",
    "### Kl√≠ƒçov√© poznatky\n",
    "\n",
    "- Autoencodery detekuj√≠ anom√°lie jako vzorky s vysokou rekonstrukƒçn√≠ chybou\n",
    "- Embeddingy umo≈æ≈àuj√≠ detekci s√©mantick√Ωch anom√°li√≠ v textu\n",
    "- Kombinace v√≠ce metod (ensemble) zlep≈°uje robustnost\n",
    "- Pr√°h pro anom√°lie je nutn√© kalibrovat na konkr√©tn√≠ch datech\n",
    "\n",
    "### Praktick√© tipy\n",
    "\n",
    "- V≈ædy tr√©nujte model pouze na norm√°ln√≠ch datech\n",
    "- Monitorujte false positive rate a pr≈Øbƒõ≈ænƒõ ladite prahy\n",
    "- Implementujte alerting s r≈Øzn√Ωmi √∫rovnƒõmi z√°va≈ænosti\n",
    "- Logujte v≈°echny detekce pro n√°slednou anal√Ωzu a zlep≈°ov√°n√≠"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
