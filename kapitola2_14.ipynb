{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚡ Optimalizace LLM Inference\n",
    "\n",
    "**Autor:** Praut s.r.o. - AI Integration & Business Automation\n",
    "\n",
    "V tomto notebooku se naučíme optimalizovat inference velkých jazykových modelů:\n",
    "\n",
    "- Kvantizace modelů (4-bit, 8-bit)\n",
    "- Flash Attention a další optimalizace\n",
    "- Batching a throughput optimalizace\n",
    "- Model caching a KV-cache\n",
    "- Praktické benchmarky a měření výkonu\n",
    "\n",
    "## Proč optimalizovat?\n",
    "\n",
    "| Technika | Úspora paměti | Rychlost | Kvalita |\n",
    "|----------|---------------|----------|---------||\n",
    "| FP32 (základ) | - | 1x | 100% |\n",
    "| FP16 | 50% | 1.5-2x | ~100% |\n",
    "| INT8 | 75% | 2-3x | 99%+ |\n",
    "| INT4 | 87.5% | 3-4x | 95-99% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalace knihoven\n",
    "!pip install -q transformers accelerate bitsandbytes optimum auto-gptq scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    TextStreamer\n",
    ")\n",
    "\n",
    "# Kontrola GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Zařízení: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"Paměť GPU: {gpu_memory:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Utility pro Měření Výkonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Výsledek benchmarku.\"\"\"\n",
    "    model_name: str\n",
    "    quantization: str\n",
    "    memory_gb: float\n",
    "    load_time: float\n",
    "    tokens_per_second: float\n",
    "    first_token_latency: float\n",
    "    total_time: float\n",
    "    output_text: str\n",
    "\n",
    "\n",
    "def get_gpu_memory_usage() -> float:\n",
    "    \"\"\"Vrátí využití GPU paměti v GB.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1e9\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Vyčistí GPU paměť.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "class InferenceBenchmark:\n",
    "    \"\"\"\n",
    "    Nástroj pro benchmarkování LLM inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_prompt: str = None):\n",
    "        self.test_prompt = test_prompt or \"Vysvětli stručně, co je umělá inteligence:\"\n",
    "        self.results: List[BenchmarkResult] = []\n",
    "    \n",
    "    def benchmark_model(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        model_name: str,\n",
    "        quantization: str,\n",
    "        max_new_tokens: int = 100,\n",
    "        num_runs: int = 3\n",
    "    ) -> BenchmarkResult:\n",
    "        \"\"\"Spustí benchmark pro model.\"\"\"\n",
    "        \n",
    "        # Warm-up run\n",
    "        inputs = tokenizer(self.test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        _ = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n",
    "        \n",
    "        # Benchmark runs\n",
    "        times = []\n",
    "        first_token_times = []\n",
    "        total_tokens = 0\n",
    "        output_text = \"\"\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            inputs = tokenizer(self.test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Dekódování\n",
    "            generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "            output_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            total_tokens += len(generated_tokens)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        avg_tokens = total_tokens / num_runs\n",
    "        tokens_per_sec = avg_tokens / avg_time\n",
    "        \n",
    "        result = BenchmarkResult(\n",
    "            model_name=model_name,\n",
    "            quantization=quantization,\n",
    "            memory_gb=get_gpu_memory_usage(),\n",
    "            load_time=0,  # Set externally\n",
    "            tokens_per_second=tokens_per_sec,\n",
    "            first_token_latency=times[0] / max_new_tokens,  # Approximate\n",
    "            total_time=avg_time,\n",
    "            output_text=output_text[:200]\n",
    "        )\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"Vypíše výsledky benchmarku.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BENCHMARK VÝSLEDKY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for r in self.results:\n",
    "            print(f\"\\n{r.model_name} ({r.quantization})\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"  Paměť GPU: {r.memory_gb:.2f} GB\")\n",
    "            print(f\"  Rychlost: {r.tokens_per_second:.1f} tokens/s\")\n",
    "            print(f\"  Celkový čas: {r.total_time:.2f}s\")\n",
    "    \n",
    "    def compare_results(self):\n",
    "        \"\"\"Porovná výsledky a vrátí tabulku.\"\"\"\n",
    "        if not self.results:\n",
    "            return None\n",
    "        \n",
    "        baseline = self.results[0]\n",
    "        \n",
    "        print(\"\\nPOROVNÁNÍ (relativně k prvnímu modelu):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for r in self.results:\n",
    "            mem_ratio = r.memory_gb / baseline.memory_gb if baseline.memory_gb > 0 else 0\n",
    "            speed_ratio = r.tokens_per_second / baseline.tokens_per_second if baseline.tokens_per_second > 0 else 0\n",
    "            \n",
    "            print(f\"{r.quantization:15} | Paměť: {mem_ratio:.2f}x | Rychlost: {speed_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kvantizace Modelů\n",
    "\n",
    "### 2.1 Načtení v různých precizích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    \"\"\"\n",
    "    Načítá modely s různými optimalizacemi.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_fp16(model_name: str):\n",
    "        \"\"\"Načte model v FP16.\"\"\"\n",
    "        print(f\"Načítám {model_name} (FP16)...\")\n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        start = time.time()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        load_time = time.time() - start\n",
    "        \n",
    "        print(f\"  Načteno za {load_time:.1f}s, paměť: {get_gpu_memory_usage():.2f} GB\")\n",
    "        return model, tokenizer, load_time\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_8bit(model_name: str):\n",
    "        \"\"\"Načte model v 8-bit kvantizaci.\"\"\"\n",
    "        print(f\"Načítám {model_name} (8-bit)...\")\n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=6.0\n",
    "        )\n",
    "        \n",
    "        start = time.time()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        load_time = time.time() - start\n",
    "        \n",
    "        print(f\"  Načteno za {load_time:.1f}s, paměť: {get_gpu_memory_usage():.2f} GB\")\n",
    "        return model, tokenizer, load_time\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_4bit(model_name: str, use_double_quant: bool = True):\n",
    "        \"\"\"Načte model v 4-bit kvantizaci s QLoRA konfigurací.\"\"\"\n",
    "        print(f\"Načítám {model_name} (4-bit)...\")\n",
    "        clear_gpu_memory()\n",
    "        \n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",  # NormalFloat4\n",
    "            bnb_4bit_use_double_quant=use_double_quant\n",
    "        )\n",
    "        \n",
    "        start = time.time()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        load_time = time.time() - start\n",
    "        \n",
    "        print(f\"  Načteno za {load_time:.1f}s, paměť: {get_gpu_memory_usage():.2f} GB\")\n",
    "        return model, tokenizer, load_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark různých kvantizací\n",
    "# Použijeme menší model pro demonstraci\n",
    "MODEL_NAME = \"microsoft/phi-2\"  # 2.7B parametrů\n",
    "\n",
    "# Alternativy pro různé velikosti GPU:\n",
    "# \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" - 1.1B (malé GPU)\n",
    "# \"microsoft/phi-2\" - 2.7B (střední GPU)\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.2\" - 7B (velké GPU)\n",
    "\n",
    "benchmark = InferenceBenchmark(\n",
    "    test_prompt=\"Vysvětli stručně v jedné větě, co je strojové učení:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FP16\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        model_fp16, tokenizer_fp16, load_time_fp16 = ModelLoader.load_fp16(MODEL_NAME)\n",
    "        result_fp16 = benchmark.benchmark_model(\n",
    "            model_fp16, tokenizer_fp16, MODEL_NAME, \"FP16\", max_new_tokens=50\n",
    "        )\n",
    "        result_fp16.load_time = load_time_fp16\n",
    "        \n",
    "        print(f\"\\nVýstup: {result_fp16.output_text}\")\n",
    "        \n",
    "        # Uvolnění paměti\n",
    "        del model_fp16, tokenizer_fp16\n",
    "        clear_gpu_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"FP16 test selhal: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 8-bit\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        model_8bit, tokenizer_8bit, load_time_8bit = ModelLoader.load_8bit(MODEL_NAME)\n",
    "        result_8bit = benchmark.benchmark_model(\n",
    "            model_8bit, tokenizer_8bit, MODEL_NAME, \"8-bit\", max_new_tokens=50\n",
    "        )\n",
    "        result_8bit.load_time = load_time_8bit\n",
    "        \n",
    "        print(f\"\\nVýstup: {result_8bit.output_text}\")\n",
    "        \n",
    "        del model_8bit, tokenizer_8bit\n",
    "        clear_gpu_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"8-bit test selhal: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4-bit\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        model_4bit, tokenizer_4bit, load_time_4bit = ModelLoader.load_4bit(MODEL_NAME)\n",
    "        result_4bit = benchmark.benchmark_model(\n",
    "            model_4bit, tokenizer_4bit, MODEL_NAME, \"4-bit\", max_new_tokens=50\n",
    "        )\n",
    "        result_4bit.load_time = load_time_4bit\n",
    "        \n",
    "        print(f\"\\nVýstup: {result_4bit.output_text}\")\n",
    "        \n",
    "        del model_4bit, tokenizer_4bit\n",
    "        clear_gpu_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"4-bit test selhal: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Výsledky benchmarku\n",
    "benchmark.print_results()\n",
    "benchmark.compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimalizace Inference\n",
    "\n",
    "### 3.1 Optimalizovaný Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedGenerator:\n",
    "    \"\"\"\n",
    "    Optimalizovaný text generátor s různými technikami.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        quantization: str = \"4bit\",  # fp16, 8bit, 4bit\n",
    "        use_flash_attention: bool = True\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.quantization = quantization\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Kvantizační konfigurace\n",
    "        quant_config = self._get_quant_config()\n",
    "        \n",
    "        # Načtení modelu\n",
    "        model_kwargs = {\n",
    "            \"device_map\": \"auto\",\n",
    "            \"trust_remote_code\": True\n",
    "        }\n",
    "        \n",
    "        if quant_config:\n",
    "            model_kwargs[\"quantization_config\"] = quant_config\n",
    "        elif quantization == \"fp16\":\n",
    "            model_kwargs[\"torch_dtype\"] = torch.float16\n",
    "        \n",
    "        # Flash Attention 2 (pokud je k dispozici)\n",
    "        if use_flash_attention:\n",
    "            try:\n",
    "                model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "            except:\n",
    "                print(\"Flash Attention není k dispozici, používám standardní attention.\")\n",
    "        \n",
    "        print(f\"Načítám model {model_name} ({quantization})...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "        print(f\"Model načten. Paměť: {get_gpu_memory_usage():.2f} GB\")\n",
    "    \n",
    "    def _get_quant_config(self):\n",
    "        \"\"\"Vrátí kvantizační konfiguraci.\"\"\"\n",
    "        if self.quantization == \"8bit\":\n",
    "            return BitsAndBytesConfig(load_in_8bit=True)\n",
    "        elif self.quantization == \"4bit\":\n",
    "            return BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 256,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9,\n",
    "        top_k: int = 50,\n",
    "        repetition_penalty: float = 1.1,\n",
    "        do_sample: bool = True,\n",
    "        stream: bool = False\n",
    "    ) -> str:\n",
    "        \"\"\"Generuje text s optimalizacemi.\"\"\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        generation_config = {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"do_sample\": do_sample,\n",
    "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "        }\n",
    "        \n",
    "        if do_sample:\n",
    "            generation_config.update({\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"top_k\": top_k,\n",
    "                \"repetition_penalty\": repetition_penalty\n",
    "            })\n",
    "        \n",
    "        # Streaming output\n",
    "        if stream:\n",
    "            streamer = TextStreamer(self.tokenizer, skip_special_tokens=True)\n",
    "            generation_config[\"streamer\"] = streamer\n",
    "        \n",
    "        # Generování s torch.inference_mode pro lepší výkon\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(**inputs, **generation_config)\n",
    "        \n",
    "        # Dekódování\n",
    "        generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    def batch_generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        max_new_tokens: int = 256,\n",
    "        **kwargs\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generuje text pro batch promptů.\"\"\"\n",
    "        \n",
    "        # Tokenizace batche\n",
    "        inputs = self.tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                **kwargs\n",
    "            )\n",
    "        \n",
    "        # Dekódování všech výstupů\n",
    "        results = []\n",
    "        for i, output in enumerate(outputs):\n",
    "            generated = output[inputs[\"input_ids\"].shape[1]:]\n",
    "            results.append(self.tokenizer.decode(generated, skip_special_tokens=True))\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimalizovaného generátoru\n",
    "if torch.cuda.is_available():\n",
    "    generator = OptimizedGenerator(\n",
    "        model_name=MODEL_NAME,\n",
    "        quantization=\"4bit\"\n",
    "    )\n",
    "    \n",
    "    # Jednoduchý test\n",
    "    prompt = \"Napiš tři výhody umělé inteligence:\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"Generuji...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    output = generator.generate(prompt, max_new_tokens=150)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"\\nVýstup:\\n{output}\")\n",
    "    print(f\"\\nČas generování: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch generování\n",
    "if torch.cuda.is_available() and 'generator' in dir():\n",
    "    prompts = [\n",
    "        \"Co je Python?\",\n",
    "        \"Co je JavaScript?\",\n",
    "        \"Co je machine learning?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Batch generování:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = generator.batch_generate(prompts, max_new_tokens=50, do_sample=False)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    for prompt, output in zip(prompts, outputs):\n",
    "        print(f\"\\nQ: {prompt}\")\n",
    "        print(f\"A: {output[:150]}...\")\n",
    "    \n",
    "    print(f\"\\nCelkový čas: {elapsed:.2f}s\")\n",
    "    print(f\"Průměr na prompt: {elapsed/len(prompts):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KV-Cache a Optimalizace Paměti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedGenerator:\n",
    "    \"\"\"\n",
    "    Generátor s KV-cache pro efektivní konverzace.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.past_key_values = None\n",
    "        self.context_tokens = None\n",
    "    \n",
    "    def set_context(self, context: str):\n",
    "        \"\"\"Nastaví kontext a uloží KV-cache.\"\"\"\n",
    "        inputs = self.tokenizer(context, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model(**inputs, use_cache=True)\n",
    "            self.past_key_values = outputs.past_key_values\n",
    "            self.context_tokens = inputs[\"input_ids\"]\n",
    "        \n",
    "        print(f\"Kontext uložen ({inputs['input_ids'].shape[1]} tokenů)\")\n",
    "    \n",
    "    def generate_with_cache(\n",
    "        self,\n",
    "        continuation: str,\n",
    "        max_new_tokens: int = 50\n",
    "    ) -> str:\n",
    "        \"\"\"Generuje s využitím cached kontextu.\"\"\"\n",
    "        if self.past_key_values is None:\n",
    "            raise ValueError(\"Nejprve nastavte kontext pomocí set_context()\")\n",
    "        \n",
    "        # Tokenizace pokračování\n",
    "        new_inputs = self.tokenizer(continuation, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # Kombinace s kontextem\n",
    "        combined_ids = torch.cat([self.context_tokens, new_inputs[\"input_ids\"]], dim=1)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=combined_ids,\n",
    "                past_key_values=self.past_key_values,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        generated = outputs[0][combined_ids.shape[1]:]\n",
    "        return self.tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Vymaže cache.\"\"\"\n",
    "        self.past_key_values = None\n",
    "        self.context_tokens = None\n",
    "        clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrace KV-cache (teoretická - vyžaduje model s podporou)\n",
    "print(\"KV-Cache umožňuje:\")\n",
    "print(\"  1. Uložit kontext (systémový prompt, dokument)\")\n",
    "print(\"  2. Rychle generovat odpovědi bez přepočítávání kontextu\")\n",
    "print(\"  3. Úspora 50-80% času při opakovaných dotazech\")\n",
    "print(\"\\nPříklad použití:\")\n",
    "print(\"\"\"\n",
    "cached_gen = CachedGenerator(model, tokenizer)\n",
    "cached_gen.set_context(\"Dlouhý dokument o AI...\")\n",
    "\n",
    "# Rychlé odpovědi na otázky\n",
    "answer1 = cached_gen.generate_with_cache(\"Otázka: Co je AI?\")\n",
    "answer2 = cached_gen.generate_with_cache(\"Otázka: Jaké jsou výhody?\") \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Throughput Optimalizace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThroughputOptimizer:\n",
    "    \"\"\"\n",
    "    Optimalizace propustnosti pro produkční nasazení.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def find_optimal_batch_size(self, max_batch: int = 32, max_tokens: int = 100) -> int:\n",
    "        \"\"\"Najde optimální batch size pro GPU.\"\"\"\n",
    "        test_prompt = \"Test prompt for batch sizing.\"\n",
    "        \n",
    "        optimal_batch = 1\n",
    "        best_throughput = 0\n",
    "        \n",
    "        for batch_size in [1, 2, 4, 8, 16, 32]:\n",
    "            if batch_size > max_batch:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                clear_gpu_memory()\n",
    "                prompts = [test_prompt] * batch_size\n",
    "                \n",
    "                inputs = self.tokenizer(\n",
    "                    prompts, return_tensors=\"pt\", padding=True\n",
    "                ).to(self.model.device)\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                start = time.time()\n",
    "                with torch.inference_mode():\n",
    "                    _ = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_tokens,\n",
    "                        do_sample=False,\n",
    "                        pad_token_id=self.tokenizer.pad_token_id\n",
    "                    )\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                elapsed = time.time() - start\n",
    "                throughput = batch_size / elapsed\n",
    "                \n",
    "                print(f\"  Batch {batch_size}: {throughput:.2f} req/s, paměť: {get_gpu_memory_usage():.2f} GB\")\n",
    "                \n",
    "                if throughput > best_throughput:\n",
    "                    best_throughput = throughput\n",
    "                    optimal_batch = batch_size\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(f\"  Batch {batch_size}: OOM\")\n",
    "                    clear_gpu_memory()\n",
    "                    break\n",
    "                raise\n",
    "        \n",
    "        return optimal_batch\n",
    "    \n",
    "    def benchmark_throughput(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        batch_size: int,\n",
    "        max_tokens: int = 100\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Měří throughput pro dané prompty.\"\"\"\n",
    "        total_requests = len(prompts)\n",
    "        total_tokens = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batch = prompts[i:i + batch_size]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                batch, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            for output in outputs:\n",
    "                total_tokens += len(output)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        return {\n",
    "            \"total_requests\": total_requests,\n",
    "            \"total_time\": elapsed,\n",
    "            \"requests_per_second\": total_requests / elapsed,\n",
    "            \"tokens_per_second\": total_tokens / elapsed,\n",
    "            \"avg_latency\": elapsed / total_requests\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimalizace throughputu\n",
    "if torch.cuda.is_available() and 'generator' in dir():\n",
    "    optimizer = ThroughputOptimizer(generator.model, generator.tokenizer)\n",
    "    \n",
    "    print(\"Hledám optimální batch size...\")\n",
    "    optimal = optimizer.find_optimal_batch_size(max_batch=16, max_tokens=50)\n",
    "    print(f\"\\nOptimální batch size: {optimal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Produkční Inference Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from dataclasses import field\n",
    "import uuid\n",
    "\n",
    "@dataclass\n",
    "class InferenceRequest:\n",
    "    \"\"\"Request pro inference.\"\"\"\n",
    "    request_id: str\n",
    "    prompt: str\n",
    "    max_tokens: int = 256\n",
    "    temperature: float = 0.7\n",
    "    created_at: float = field(default_factory=time.time)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InferenceResponse:\n",
    "    \"\"\"Response z inference.\"\"\"\n",
    "    request_id: str\n",
    "    text: str\n",
    "    tokens_generated: int\n",
    "    latency: float\n",
    "    queue_time: float\n",
    "\n",
    "\n",
    "class InferenceServer:\n",
    "    \"\"\"\n",
    "    Jednoduchý inference server s batched processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        batch_size: int = 4,\n",
    "        max_wait_time: float = 0.1\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_wait_time = max_wait_time\n",
    "        \n",
    "        self.request_queue = Queue()\n",
    "        self.response_dict = {}\n",
    "        self.running = False\n",
    "        self.stats = {\n",
    "            \"total_requests\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"total_time\": 0\n",
    "        }\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Spustí server.\"\"\"\n",
    "        self.running = True\n",
    "        self.worker_thread = Thread(target=self._process_loop, daemon=True)\n",
    "        self.worker_thread.start()\n",
    "        print(\"Inference server spuštěn.\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Zastaví server.\"\"\"\n",
    "        self.running = False\n",
    "        print(\"Inference server zastaven.\")\n",
    "    \n",
    "    def submit(self, prompt: str, max_tokens: int = 256) -> str:\n",
    "        \"\"\"Odešle request a vrátí request_id.\"\"\"\n",
    "        request_id = str(uuid.uuid4())\n",
    "        request = InferenceRequest(\n",
    "            request_id=request_id,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        self.request_queue.put(request)\n",
    "        return request_id\n",
    "    \n",
    "    def get_response(self, request_id: str, timeout: float = 30) -> Optional[InferenceResponse]:\n",
    "        \"\"\"Čeká na response.\"\"\"\n",
    "        start = time.time()\n",
    "        while time.time() - start < timeout:\n",
    "            if request_id in self.response_dict:\n",
    "                return self.response_dict.pop(request_id)\n",
    "            time.sleep(0.01)\n",
    "        return None\n",
    "    \n",
    "    def _process_loop(self):\n",
    "        \"\"\"Hlavní loop pro zpracování requestů.\"\"\"\n",
    "        while self.running:\n",
    "            batch = []\n",
    "            wait_start = time.time()\n",
    "            \n",
    "            # Sbíráme batch\n",
    "            while len(batch) < self.batch_size:\n",
    "                if time.time() - wait_start > self.max_wait_time and batch:\n",
    "                    break\n",
    "                try:\n",
    "                    request = self.request_queue.get(timeout=0.01)\n",
    "                    batch.append(request)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if batch:\n",
    "                self._process_batch(batch)\n",
    "    \n",
    "    def _process_batch(self, batch: List[InferenceRequest]):\n",
    "        \"\"\"Zpracuje batch requestů.\"\"\"\n",
    "        prompts = [r.prompt for r in batch]\n",
    "        max_tokens = max(r.max_tokens for r in batch)\n",
    "        \n",
    "        # Tokenizace\n",
    "        inputs = self.tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Inference\n",
    "        start = time.time()\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "        inference_time = time.time() - start\n",
    "        \n",
    "        # Zpracování výstupů\n",
    "        for i, (request, output) in enumerate(zip(batch, outputs)):\n",
    "            generated = output[inputs[\"input_ids\"].shape[1]:]\n",
    "            text = self.tokenizer.decode(generated, skip_special_tokens=True)\n",
    "            \n",
    "            response = InferenceResponse(\n",
    "                request_id=request.request_id,\n",
    "                text=text,\n",
    "                tokens_generated=len(generated),\n",
    "                latency=inference_time / len(batch),\n",
    "                queue_time=time.time() - request.created_at\n",
    "            )\n",
    "            \n",
    "            self.response_dict[request.request_id] = response\n",
    "            \n",
    "            # Statistiky\n",
    "            self.stats[\"total_requests\"] += 1\n",
    "            self.stats[\"total_tokens\"] += len(generated)\n",
    "            self.stats[\"total_time\"] += response.latency\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, float]:\n",
    "        \"\"\"Vrátí statistiky serveru.\"\"\"\n",
    "        if self.stats[\"total_requests\"] == 0:\n",
    "            return self.stats\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"avg_latency\": self.stats[\"total_time\"] / self.stats[\"total_requests\"],\n",
    "            \"avg_tokens\": self.stats[\"total_tokens\"] / self.stats[\"total_requests\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference serveru\n",
    "if torch.cuda.is_available() and 'generator' in dir():\n",
    "    server = InferenceServer(\n",
    "        model=generator.model,\n",
    "        tokenizer=generator.tokenizer,\n",
    "        batch_size=4,\n",
    "        max_wait_time=0.05\n",
    "    )\n",
    "    \n",
    "    server.start()\n",
    "    \n",
    "    # Odeslání requestů\n",
    "    prompts = [\n",
    "        \"Co je Python?\",\n",
    "        \"Vysvětli AI:\",\n",
    "        \"Co je cloud computing?\",\n",
    "        \"Popiš machine learning:\"\n",
    "    ]\n",
    "    \n",
    "    request_ids = []\n",
    "    for prompt in prompts:\n",
    "        rid = server.submit(prompt, max_tokens=50)\n",
    "        request_ids.append(rid)\n",
    "        print(f\"Odesláno: {prompt[:30]}... -> {rid[:8]}\")\n",
    "    \n",
    "    # Čekání na odpovědi\n",
    "    print(\"\\nOdpovědi:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for rid in request_ids:\n",
    "        response = server.get_response(rid, timeout=30)\n",
    "        if response:\n",
    "            print(f\"\\n[{response.tokens_generated} tokenů, {response.latency:.2f}s]\")\n",
    "            print(f\"{response.text[:150]}...\")\n",
    "    \n",
    "    # Statistiky\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STATISTIKY SERVERU:\")\n",
    "    for k, v in server.get_stats().items():\n",
    "        print(f\"  {k}: {v:.3f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "    \n",
    "    server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrnutí\n",
    "\n",
    "V tomto notebooku jsme se naučili:\n",
    "\n",
    "1. **Kvantizace** - FP16, 8-bit, 4-bit pro úsporu paměti\n",
    "2. **Benchmarking** - měření výkonu a porovnání\n",
    "3. **Optimalizovaný generátor** - s flash attention a dalšími technikami\n",
    "4. **KV-Cache** - pro efektivní konverzace\n",
    "5. **Throughput optimalizace** - nalezení optimálního batch size\n",
    "6. **Inference server** - pro produkční nasazení\n",
    "\n",
    "### Doporučení pro produkci:\n",
    "- Použijte 4-bit kvantizaci pro maximální úsporu paměti\n",
    "- Optimalizujte batch size podle vaší GPU\n",
    "- Implementujte request batching pro vysoký throughput\n",
    "- Sledujte metriky: latence, throughput, využití GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
