{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6948e4",
   "metadata": {},
   "source": [
    "# ğŸ“š Kapitola 32: Ollama API integrace\n",
    "\n",
    "<div style=\"background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; margin: 20px 0;\">\n",
    "    <h2 style=\"color: white; margin: 0;\">Blok 4 | EXPERT</h2>\n",
    "    <p style=\"color: white; margin: 10px 0;\">ğŸ“– VÃ½ukovÃ¡ kapitola</p>\n",
    "</div>\n",
    "\n",
    "## ğŸ¯ Co se nauÄÃ­te\n",
    "\n",
    "V tÃ©to kapitole se zamÄ›Å™Ã­me na nÃ¡sledujÃ­cÃ­ tÃ©mata:\n",
    "\n",
    "- **Ollama REST API endpoints**\n",
    "- **Python client implementation**\n",
    "- **Streaming responses**\n",
    "- **Model parameters tuning**\n",
    "- **Context window management**\n",
    "- **Token counting**\n",
    "- **Error handling strategies**\n",
    "\n",
    "## âš ï¸ PÅ™edpoklady\n",
    "Tato kapitola navazuje na kapitoly: 31, 13\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f24ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Inicializace prostÅ™edÃ­\n",
    "# Tento kÃ³d nastavuje prostÅ™edÃ­ pro kapitolu 32\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Informace o prostÅ™edÃ­\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ“š KAPITOLA 32: Ollama API integrace\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ Python verze: {sys.version}\")\n",
    "print(f\"ğŸ“… Datum spuÅ¡tÄ›nÃ­: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ’» OS: {os.name}\")\n",
    "print(f\"ğŸ“ PracovnÃ­ adresÃ¡Å™: {os.getcwd()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Instalace potÅ™ebnÃ½ch knihoven (odkomentujte podle potÅ™eby)\n",
    "# !pip install requests pandas numpy matplotlib\n",
    "# !pip install beautifulsoup4 sqlalchemy fastapi\n",
    "\n",
    "# Import zÃ¡kladnÃ­ch knihoven\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "print(\"âœ… ProstÅ™edÃ­ pÅ™ipraveno!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe0199",
   "metadata": {},
   "source": [
    "## ğŸ“– TeoretickÃ¡ ÄÃ¡st\n",
    "\n",
    "<div style=\"background: #f0f4ff; padding: 20px; border-left: 5px solid #4a69bd; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">ğŸ“ ZÃ¡kladnÃ­ teorie a koncepty</h3>\n",
    "</div>\n",
    "\n",
    "# Ollama API Integrace\n",
    "\n",
    "## 1. Ãšvod a motivace\n",
    "\n",
    "V oblasti umÄ›lÃ© inteligence se stÃ¡le vÃ­ce rozvÃ­jÃ­ moÅ¾nost lokÃ¡lnÃ­ho provozu modelÅ¯ AI pÅ™Ã­mo na uÅ¾ivatelskÃ©m zaÅ™Ã­zenÃ­. Ollama je platforma, kterÃ¡ umoÅ¾Åˆuje jednoduÅ¡e spouÅ¡tÄ›t a spravovat rÅ¯znÃ© LLM (Large Language Models) lokÃ¡lnÄ›, vÄetnÄ› moÅ¾nosti jejich integrace pÅ™es REST API. VÃ½znam tÃ©to kapitoly spoÄÃ­vÃ¡ v tom, Å¾e studenti se nauÄÃ­, jak efektivnÄ› komunikovat s Ollama serverem pÅ™es API, zpracovÃ¡vat odpovÄ›di a optimalizovat vÃ½kon modelÅ¯. Tato dovednost je nezbytnÃ¡ pro vÃ½voj aplikacÃ­, kterÃ© vyÅ¾adujÃ­ rychlÃ© a bezpeÄnÃ© zpracovÃ¡nÃ­ textu ve vlastnÃ­m prostÅ™edÃ­ â€“ napÅ™Ã­klad v rÃ¡mci internÃ­ch systÃ©mÅ¯, bezpeÄnostnÃ­ch Å™eÅ¡enÃ­ nebo edge computing aplikacÃ­.\n",
    "\n",
    "Integrace Ollama API je v praxi velmi uÅ¾iteÄnÃ¡ napÅ™Ã­klad pro vÃ½voj chatbotÅ¯, analÃ½zu textu, automatizaci Ãºloh a vytvÃ¡Å™enÃ­ custom AI Å™eÅ¡enÃ­. Student po tÃ©to kapitole zÃ­skÃ¡ schopnost vytvÃ¡Å™et robustnÃ­ PythonovÃ© klienty, kterÃ© mohou komunikovat s lokÃ¡lnÃ­m Ollama serverem a efektivnÄ› zpracovÃ¡vat odpovÄ›di modelÅ¯.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. HlavnÃ­ koncepty\n",
    "\n",
    "### **Ollama REST API endpoints**\n",
    "\n",
    "Ollama poskytuje Å™adu REST endpointÅ¯, kterÃ© umoÅ¾ÅˆujÃ­ komunikaci s bÄ›Å¾Ã­cÃ­m serverem. NejÄastÄ›ji pouÅ¾Ã­vanÃ© jsou:\n",
    "\n",
    "- `/api/generate` â€“ generuje text na zÃ¡kladÄ› vstupnÃ­ho promptu.\n",
    "- `/api/chat` â€“ umoÅ¾Åˆuje interaktivnÃ­ konverzaci (chatovÃ¡nÃ­) s modelem.\n",
    "- `/api/tags` â€“ zÃ­skÃ¡ seznam dostupnÃ½ch modelÅ¯.\n",
    "- `/api/ps` â€“ zÃ­skÃ¡ informace o bÄ›Å¾Ã­cÃ­ch procesech.\n",
    "- `/api/embeddings` â€“ vytvÃ¡Å™Ã­ embeddingy z textu.\n",
    "\n",
    "NapÅ™Ã­klad volÃ¡nÃ­ generovacÃ­ho endpointu mÅ¯Å¾e vypadat takto:\n",
    "\n",
    "```http\n",
    "POST /api/generate HTTP/1.1\n",
    "Host: localhost:11434\n",
    "Content-Type: application/json\n",
    "\n",
    "{\n",
    "  \"model\": \"llama3\",\n",
    "  \"prompt\": \"Jak se dÄ›lÃ¡ kÃ¡va?\",\n",
    "  \"stream\": false\n",
    "}\n",
    "```\n",
    "\n",
    "OdpovÄ›Ä se vracÃ­ ve formÃ¡tu JSON:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"response\": \"KÃ¡vu se pÅ™ipravuje vaÅ™enÃ­m vody a mlÃ­ÄnÃ½ch produktÅ¯...\",\n",
    "  \"done\": true,\n",
    "  \"context\": [123, 456, 789]\n",
    "}\n",
    "```\n",
    "\n",
    "Tato struktura umoÅ¾Åˆuje jednoduchou integraci do jakÃ©koli aplikace bez potÅ™eby sloÅ¾itÃ½ch knihoven.\n",
    "\n",
    "---\n",
    "\n",
    "### **Python client implementation**\n",
    "\n",
    "Implementace Python klienta pro Ollama API je pomÄ›rnÄ› jednoduchÃ¡ dÃ­ky pouÅ¾itÃ­ `requests` knihovny. NÃ¡sledujÃ­cÃ­ kÃ³d ukazuje jednoduchÃ© volÃ¡nÃ­:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def generate_text(prompt, model=\"llama3\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()[\"response\"]\n",
    "```\n",
    "\n",
    "Tento zÃ¡kladnÃ­ klient mÅ¯Å¾e bÃ½t rozÅ¡Ã­Å™en o logiku pro streamovÃ¡nÃ­, kontroly chyb nebo pÅ™idÃ¡nÃ­ parametrÅ¯ jako `temperature` a `max_tokens`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Streaming responses**\n",
    "\n",
    "PÅ™i pouÅ¾itÃ­ streamovÃ¡nÃ­ v Ollama API je moÅ¾nÃ© zÃ­skÃ¡vat odpovÄ›di postupnÄ›, coÅ¾ je velmi uÅ¾iteÄnÃ© pro vizuÃ¡lnÃ­ pÅ™edstavu generace textu nebo pro reÃ¡lnÃ© interaktivnÃ­ aplikace. NastavenÃ­ `stream: true` v poÅ¾adavku zpÅ¯sobÃ­, Å¾e odpovÄ›Ä se posÃ­lÃ¡ postupnÄ› v jednotlivÃ½ch ÄÃ¡stech:\n",
    "\n",
    "```python\n",
    "def stream_response(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": True\n",
    "    }\n",
    "    response = requests.post(url, json=payload, stream=True)\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            data = json.loads(line)\n",
    "            print(data[\"response\"], end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "Tato metoda umoÅ¾Åˆuje vytvÃ¡Å™et plynulÃ© uÅ¾ivatelskÃ© prostÅ™edÃ­, napÅ™Ã­klad pÅ™i simulaci chatu.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model parameters tuning**\n",
    "\n",
    "PÅ™i volÃ¡nÃ­ API mÅ¯Å¾eÅ¡ upravovat parametry modelu pro optimalizaci vÃ½stupu. KlÃ­ÄovÃ© parametry zahrnujÃ­:\n",
    "\n",
    "- `temperature` â€“ ovlivÅˆuje nÃ¡hodnost odpovÄ›dÃ­ (0 = deterministickÃ©, 1 = nÃ¡hodnÃ©)\n",
    "- `max_tokens` â€“ maximÃ¡lnÃ­ poÄet tokenÅ¯ ve vÃ½stupu\n",
    "- `top_p` â€“ filtruje tokeny podle pravdÄ›podobnosti\n",
    "- `stop` â€“ Å™etÄ›zce, kterÃ© ukonÄÃ­ generovÃ¡nÃ­\n",
    "\n",
    "PÅ™Ã­klad:\n",
    "\n",
    "```python\n",
    "payload = {\n",
    "    \"model\": \"llama3\",\n",
    "    \"prompt\": \"VysvÄ›tli Python\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 200,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "```\n",
    "\n",
    "Tato Ãºprava umoÅ¾Åˆuje vÃ½raznÄ› ovlivnit styl a dÃ©lku odpovÄ›di.\n",
    "\n",
    "---\n",
    "\n",
    "### **Context window management**\n",
    "\n",
    "KaÅ¾dÃ½ model mÃ¡ omezenou kontextovou velikost (napÅ™. 4096 tokenÅ¯). PÅ™i pouÅ¾itÃ­ chatovacÃ­ho endpointu se kontext udrÅ¾uje v rÃ¡mci relace. Pokud je kontext pÅ™Ã­liÅ¡ dlouhÃ½, model mÅ¯Å¾e zaÄÃ­t â€zapomÃ­natâ€œ dÅ™Ã­vÄ›jÅ¡Ã­ ÄÃ¡sti konverzace.\n",
    "\n",
    "Pro sprÃ¡vnÃ© Å™Ã­zenÃ­ kontextu je vhodnÃ© limitovat poÄet zprÃ¡v v historii a pÅ™idÃ¡vat jejich souhrny. NapÅ™Ã­klad:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Jak se dÄ›lÃ¡ kÃ¡va?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"KÃ¡vu se vaÅ™Ã­ z mlÃ©ÄnÃ½ch produktÅ¯...\"},\n",
    "]\n",
    "```\n",
    "\n",
    "PÅ™i vÄ›tÅ¡Ã­ konverzaci je vhodnÃ© pÅ™idat logiku pro shromaÅ¾ÄovÃ¡nÃ­ kontextu a jeho Ãºpravu.\n",
    "\n",
    "---\n",
    "\n",
    "### **Token counting**\n",
    "\n",
    "Tokenizace je klÃ­ÄovÃ¡ pro sprÃ¡vnÃ© nastavenÃ­ parametrÅ¯ a optimalizaci vÃ½konu. Ollama pouÅ¾Ã­vÃ¡ tokenizer (napÅ™. `llama3` pouÅ¾Ã­vÃ¡ `LlamaTokenizer`). V Pythonu lze tokeny poÄÃ­tat pomocÃ­ knihovny `tokenizers`.\n",
    "\n",
    "```python\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"path/to/tokenizer.json\")\n",
    "tokens = tokenizer.encode(\"VÃ­tejte v Ollama API\").ids\n",
    "print(len(tokens))\n",
    "```\n",
    "\n",
    "PÅ™i prÃ¡ci s velkÃ½mi texty je dÅ¯leÅ¾itÃ© monitorovat poÄet tokenÅ¯ a omezoval je podle kapacity modelu.\n",
    "\n",
    "---\n",
    "\n",
    "### **Error handling strategies**\n",
    "\n",
    "V praxi mohou nastat rÅ¯znÃ© chyby, napÅ™Ã­klad:\n",
    "\n",
    "- `ConnectionError` â€“ server nenÃ­ dostupnÃ½\n",
    "- `Timeout` â€“ odpovÄ›Ä trvÃ¡ pÅ™Ã­liÅ¡ dlouho\n",
    "- `HTTP 404` â€“ model neexistuje\n",
    "- `HTTP 500` â€“ internÃ­ chyba serveru\n",
    "\n",
    "DoporuÄenÃ© Å™eÅ¡enÃ­:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, json=payload, timeout=30)\n",
    "    response.raise_for_status()\n",
    "except RequestException as e:\n",
    "    print(f\"Chyba pÅ™i komunikaci s Ollama: {e}\")\n",
    "```\n",
    "\n",
    "TakÃ© je dobrÃ© implementovat opakovÃ¡nÃ­ (retry) v pÅ™Ã­padÄ› doÄasnÃ½ch chyb.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. DÅ¯leÅ¾itÃ© detaily\n",
    "\n",
    "### **ÄŒastÃ© chyby a jak se jim vyhnout**\n",
    "\n",
    "- NenÃ­ spuÅ¡tÄ›nÃ½ Ollama server â†’ zkontroluj, Å¾e `ollama serve` bÄ›Å¾Ã­.\n",
    "- NeplatnÃ½ model â†’ ovÄ›Å™ existenci modelu pÅ™es `/api/tags`.\n",
    "- PÅ™Ã­liÅ¡ dlouhÃ½ kontext â†’ implementuj logiku pro zkracovÃ¡nÃ­ nebo souhrny.\n",
    "- ChybnÃ© parametry API â†’ pouÅ¾Ã­vej validaci a dokumentaci.\n",
    "\n",
    "### **Best practices**\n",
    "\n",
    "- VÅ¾dy testuj pÅ™ipojenÃ­ k serveru pÅ™ed odeslÃ¡nÃ­m poÅ¾adavku.\n",
    "- Implementuj logiku pro streamovÃ¡nÃ­, pokud to podporuje model.\n",
    "- PouÅ¾Ã­vej `timeout` v poÅ¾adavcÃ­ch, aby se aplikace nesedÄ›la.\n",
    "- UdrÅ¾uj vÃ½stupnÃ­ formÃ¡t konzistentnÃ­ pro rÅ¯znÃ© modely.\n",
    "\n",
    "### **Performance tipy**\n",
    "\n",
    "- Optimalizuj `max_tokens` podle potÅ™eby â€“ pÅ™Ã­liÅ¡ dlouhÃ© odpovÄ›di sniÅ¾ujÃ­ rychlost.\n",
    "- PouÅ¾Ã­vej modely s menÅ¡Ã­ velikostÃ­, pokud je to moÅ¾nÃ©.\n",
    "- UdrÅ¾uj kontext v rÃ¡mci limitu modelu (napÅ™. 2048 tokenÅ¯ pro nÄ›kterÃ© modely).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. PropojenÃ­ s pÅ™edchozÃ­mi kapitolami\n",
    "\n",
    "Tato kapitola navazuje na znalosti z pÅ™edchozÃ­ch kapitol, jako jsou instalace Ollama, prÃ¡ce s modely a zÃ¡klady REST API. NauÄÃ­ studenty, jak pÅ™enÃ©st teoretickÃ© vÄ›domosti do praxe â€“ jak napojit aplikaci na model a efektivnÄ› komunikovat s nÃ­m.\n",
    "\n",
    "Tato kapitola rozÅ¡iÅ™uje jiÅ¾ nauÄenÃ© schopnosti vytvÃ¡Å™et API klienty a pracovat s datovÃ½mi strukturami, pÅ™iÄemÅ¾ pÅ™idÃ¡vÃ¡ konkrÃ©tnÃ­ pouÅ¾itÃ­ v kontextu lokÃ¡lnÃ­ho AI modelu. VÃ½sledkem je aplikace, kterÃ¡ dokÃ¡Å¾e efektivnÄ› komunikovat s AI modely pÅ™Ã­mo z vlastnÃ­ho zaÅ™Ã­zenÃ­ â€“ coÅ¾ je klÃ­ÄovÃ© pro bezpeÄnÃ© a efektivnÃ­ Å™eÅ¡enÃ­ ve velkÃ©m mÄ›Å™Ã­tku.\n",
    "\n",
    "--- \n",
    "\n",
    "**Konec kapitoly: Ollama API integrace**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef45246",
   "metadata": {},
   "source": [
    "## ğŸ’» PraktickÃ© pÅ™Ã­klady\n",
    "\n",
    "<div style=\"background: #e8f5e9; padding: 20px; border-left: 5px solid #4caf50; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">ğŸ‘¨â€ğŸ’» Hands-on pÅ™Ã­klady ke spuÅ¡tÄ›nÃ­</h3>\n",
    "    <p>NÃ¡sledujÃ­cÃ­ pÅ™Ã­klady si mÅ¯Å¾ete hned vyzkouÅ¡et. KaÅ¾dÃ½ pÅ™Ã­klad je samostatnÄ› spustitelnÃ½.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ef92d",
   "metadata": {},
   "source": [
    "# PÅ™Ã­klady integrace Ollama API\n",
    "\n",
    "## 1. ZÃ¡kladnÃ­ komunikace s modely\n",
    "\n",
    "**Co pÅ™Ã­klad demonstruje**: ZÃ¡kladnÃ­ volÃ¡nÃ­ REST API Ollamy pro zÃ­skÃ¡nÃ­ odpovÄ›di od modelu.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# NÃ¡zev pÅ™Ã­kladu: ZÃ¡kladnÃ­ komunikace s modely\n",
    "# Co pÅ™Ã­klad demonstruje: ZÃ¡kladnÃ­ volÃ¡nÃ­ REST API Ollamy pro zÃ­skÃ¡nÃ­ odpovÄ›di od modelu.\n",
    "\n",
    "def basic_ollama_call():\n",
    "    # URL endpointu Ollamy (pÅ™edpoklÃ¡dÃ¡me, Å¾e bÄ›Å¾Ã­ na localhostu)\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    # Parametry pro volÃ¡nÃ­ modelu\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",  # PouÅ¾ijeme model Llama3\n",
    "        \"prompt\": \"Jak se dÄ›lÃ¡ kÃ¡va?\",\n",
    "        \"stream\": False     # Nechceme streamovÃ¡nÃ­\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # OdeslÃ¡nÃ­ HTTP POST poÅ¾adavku\n",
    "        response = requests.post(url, json=payload)\n",
    "        \n",
    "        # Kontrola stavu odpovÄ›di\n",
    "        if response.status_code == 200:\n",
    "            # ZÃ­skÃ¡nÃ­ JSON odpovÄ›di\n",
    "            data = response.json()\n",
    "            \n",
    "            # VÃ½pis odpovÄ›di modelu\n",
    "            print(\"OdpovÄ›Ä modelu:\")\n",
    "            print(data[\"response\"])\n",
    "            \n",
    "            return data[\"response\"]\n",
    "        else:\n",
    "            print(f\"Chyba pÅ™i volÃ¡nÃ­ API: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Chyba pÅ™i komunikaci s API: {e}\")\n",
    "        return None\n",
    "\n",
    "# SpuÅ¡tÄ›nÃ­ funkce\n",
    "if __name__ == \"__main__\":\n",
    "    basic_ollama_call()\n",
    "```\n",
    "\n",
    "**OÄekÃ¡vanÃ½ vÃ½stup**:\n",
    "```\n",
    "OdpovÄ›Ä modelu:\n",
    "KÃ¡vu se dÄ›lÃ¡ rÅ¯znÃ½mi zpÅ¯soby, ale nejbÄ›Å¾nÄ›jÅ¡Ã­ metoda je pouÅ¾itÃ­ kÃ¡vovaru nebo filtru. \n",
    "NejdÅ™Ã­ve se zrnka kÃ¡vy mlÃ­, potÃ© se pÅ™ipravÃ­ voda a nakonec se vÅ¡e promÃ­chÃ¡.\n",
    "```\n",
    "\n",
    "**VysvÄ›tlenÃ­ fungovÃ¡nÃ­**: Tento kÃ³d demonstruje zÃ¡kladnÃ­ komunikaci s Ollama REST API. VolÃ¡ endpoint `/api/generate` s parametrem modelu, dotazem a nastavenÃ­m streamovÃ¡nÃ­ na `False`. Po ÃºspÄ›Å¡nÃ©m volÃ¡nÃ­ se extrahuje odpovÄ›Ä z JSON dat.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. StreamovanÃ¡ odpovÄ›Ä\n",
    "\n",
    "**Co pÅ™Ã­klad demonstruje**: StreamovÃ¡nÃ­ odpovÄ›dÃ­ od modelu po jednotlivÃ½ch tokenÅ¯.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# NÃ¡zev pÅ™Ã­kladu: StreamovanÃ¡ odpovÄ›Ä\n",
    "# Co pÅ™Ã­klad demonstruje: StreamovÃ¡nÃ­ odpovÄ›dÃ­ od modelu po jednotlivÃ½ch tokenÅ¯.\n",
    "\n",
    "def streaming_response():\n",
    "    # URL endpointu Ollamy\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    # Parametry pro streamovÃ¡nÃ­\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": \"VysvÄ›tli kryptomÄ›ny jednoduÅ¡e.\",\n",
    "        \"stream\": True     # Zapneme streamovÃ¡nÃ­\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # OdeslÃ¡nÃ­ poÅ¾adavku s streamovÃ¡nÃ­m\n",
    "        response = requests.post(url, json=payload, stream=True)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"StreamovanÃ¡ odpovÄ›Ä:\")\n",
    "            \n",
    "            # ÄŒtenÃ­ streamu po jednotlivÃ½ch Å™Ã¡dcÃ­ch (JSON objektech)\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    # KaÅ¾dÃ½ Å™Ã¡dek je samostatnÃ½ JSON objekt\n",
    "                    data = json.loads(line)\n",
    "                    \n",
    "                    # Pokud mÃ¡me token, vypÃ­Å¡eme jej\n",
    "                    if \"response\" in data and data[\"response\"]:\n",
    "                        print(data[\"response\"], end=\"\", flush=True)\n",
    "                        \n",
    "            print()  # NovÃ½ Å™Ã¡dek na konci\n",
    "        else:\n",
    "            print(f\"Chyba pÅ™i volÃ¡nÃ­ API: {response.status_code}\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Chyba pÅ™i komunikaci s API: {e}\")\n",
    "\n",
    "# SpuÅ¡tÄ›nÃ­ funkce\n",
    "if __name__ == \"__main__\":\n",
    "    streaming_response()\n",
    "```\n",
    "\n",
    "**OÄekÃ¡vanÃ½ vÃ½stup**:\n",
    "```\n",
    "StreamovanÃ¡ odpovÄ›Ä:\n",
    "KryptomÄ›ny jsou digitÃ¡lnÃ­ mÄ›ny, kterÃ© fungujÃ­ na technologii blockchain. \n",
    "KaÅ¾dÃ¡ transakce je zaznamenÃ¡na do bloku a tento systÃ©m zajiÅ¡Å¥uje bezpeÄnost a prÅ¯hlednost.\n",
    "```\n",
    "\n",
    "**VysvÄ›tlenÃ­ fungovÃ¡nÃ­**: Tento pÅ™Ã­klad ukazuje, jak pracovat s streamovanÃ½mi odpovÄ›Ämi. PouÅ¾Ã­vÃ¡me `stream=True` a `iter_lines()` pro ÄtenÃ­ odpovÄ›di po jednotlivÃ½ch Å™Ã¡dcÃ­ch. KaÅ¾dÃ½ Å™Ã¡dek je samostatnÃ½ JSON objekt, kterÃ½ obsahuje token odpovÄ›di.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. NastavenÃ­ parametrÅ¯ modelu\n",
    "\n",
    "**Co pÅ™Ã­klad demonstruje**: NastavovÃ¡nÃ­ rÅ¯znÃ½ch parametrÅ¯ modelu (temperature, max_tokens) pro optimalizaci vÃ½stupu.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# NÃ¡zev pÅ™Ã­kladu: NastavenÃ­ parametrÅ¯ modelu\n",
    "# Co pÅ™Ã­klad demonstruje: NastavovÃ¡nÃ­ rÅ¯znÃ½ch parametrÅ¯ modelu pro optimalizaci vÃ½stupu.\n",
    "\n",
    "def set_model_parameters():\n",
    "    # URL endpointu Ollamy\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    # Parametry modelu s nastavenÃ­m rÅ¯znÃ½ch parametrÅ¯\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": \"NapiÅ¡ krÃ¡tkÃ½ pÅ™Ã­bÄ›h o objeviteli.\",\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.7,      # NÃ¡hodnost odpovÄ›di (0-1)\n",
    "            \"max_tokens\": 200,       # MaximÃ¡lnÃ­ poÄet tokenÅ¯ vÃ½stupu\n",
    "            \"top_p\": 0.9,            # Top-p sampling\n",
    "            \"repeat_penalty\": 1.1    # Penalty za opakovÃ¡nÃ­ tokenÅ¯\n",
    "        },\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(\"OdpovÄ›Ä modelu s nastavenÃ½mi parametry:\")\n",
    "            print(data[\"response\"])\n",
    "            \n",
    "            # VypsÃ¡nÃ­ pouÅ¾itÃ½ch parametrÅ¯\n",
    "            print(\"\\nPouÅ¾itÃ© parametry:\")\n",
    "            for key, value in payload[\"options\"].items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"Chyba pÅ™i volÃ¡nÃ­ API: {response.status_code}\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Chyba pÅ™i komunikaci s API: {e}\")\n",
    "\n",
    "# SpuÅ¡tÄ›nÃ­ funkce\n",
    "if __name__ == \"__main__\":\n",
    "    set_model_parameters()\n",
    "```\n",
    "\n",
    "**OÄekÃ¡vanÃ½ vÃ½stup**:\n",
    "```\n",
    "OdpovÄ›Ä modelu s nastavenÃ½mi parametry:\n",
    "V dobÄ›, kdy lidÃ© jeÅ¡tÄ› neumÄ›li ÄÃ­st, objevil jeden chytrÃ½ muÅ¾ zpÅ¯sob, jak se spolu komunikovat. \n",
    "PomocÃ­ pÃ­smen a slov mohl pÅ™enÃ¡Å¡et svÃ© myÅ¡lenky a vÄ›domosti.\n",
    "\n",
    "PouÅ¾itÃ© parametry:\n",
    "  temperature: 0.7\n",
    "  max_tokens: 200\n",
    "  top_p: 0.9\n",
    "  repeat_penalty: 1.1\n",
    "```\n",
    "\n",
    "**VysvÄ›tlenÃ­ fungovÃ¡nÃ­**: Tento pÅ™Ã­klad ukazuje, jak nastavit rÅ¯znÃ© parametry modelu pomocÃ­ sekce `options` v JSON payloadu. Parametry jako `temperature` ovlivÅˆujÃ­ nÃ¡hodnost odpovÄ›di, `max_tokens` omezuje dÃ©lku vÃ½stupu a `top_p` ovlivÅˆuje distribuci pravdÄ›podobnosti tokenÅ¯.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. SprÃ¡va kontextu (Context window)\n",
    "\n",
    "**Co pÅ™Ã­klad demonstruje**: SprÃ¡va kontextu mezi vÃ­ce volÃ¡nÃ­mi modelu pro vytvÃ¡Å™enÃ­ rozsÃ¡hlejÅ¡Ã­ch odpovÄ›dÃ­.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# NÃ¡zev pÅ™Ã­kladu: SprÃ¡va kontextu (Context window)\n",
    "# Co pÅ™Ã­klad demonstruje: SprÃ¡va kontextu mezi vÃ­ce volÃ¡nÃ­mi modelu pro vytvÃ¡Å™enÃ­ rozsÃ¡hlejÅ¡Ã­ch odpovÄ›dÃ­.\n",
    "\n",
    "def context_management():\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    # PrvnÃ­ dotaz - zaloÅ¾enÃ­ kontextu\n",
    "    first_prompt = \"VysvÄ›tli, co je umÄ›lÃ¡ inteligence.\"\n",
    "    \n",
    "    payload1 = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": first_prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response1 = requests.post(url, json=payload1)\n",
    "    \n",
    "    if response1.status_code == 200:\n",
    "        data1 = response1.json()\n",
    "        print(\"PrvnÃ­ odpovÄ›Ä:\")\n",
    "        print(data1[\"response\"])\n",
    "        \n",
    "        # DruhÃ½ dotaz - vyuÅ¾itÃ­ kontextu\n",
    "        second_prompt = \"A jakÃ© jsou hlavnÃ­ typy AI?\"\n",
    "        \n",
    "        payload2 = {\n",
    "            \"model\": \"llama3\",\n",
    "            \"prompt\": first_prompt + \"\\n\" + data1[\"response\"] + \"\\n\" + second_prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response2 = requests.post(url, json=payload2)\n",
    "        \n",
    "        if response2.status_code == 200:\n",
    "            data2 = response2.json()\n",
    "            print(\"\\nDruhÃ¡ odpovÄ›Ä (vyuÅ¾itÃ­ kontextu):\")\n",
    "            print(data2[\"response\"])\n",
    "            \n",
    "    else:\n",
    "        print(\"Chyba pÅ™i volÃ¡nÃ­ API\")\n",
    "\n",
    "# SpuÅ¡tÄ›nÃ­ funkce\n",
    "if __name__ == \"__main__\":\n",
    "    context_management()\n",
    "```\n",
    "\n",
    "**OÄekÃ¡vanÃ½ vÃ½stup**:\n",
    "```\n",
    "PrvnÃ­ odpovÄ›Ä:\n",
    "UmÄ›lÃ¡ inteligence je technologie, kterÃ¡ simuluje lidskÃ© myÅ¡lenÃ­ a schopnosti. \n",
    "MÅ¯Å¾e se uÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745db6d5",
   "metadata": {},
   "source": [
    "## ğŸ¯ CviÄenÃ­ a Ãºkoly\n",
    "\n",
    "<div style=\"background: #fff3e0; padding: 20px; border-left: 5px solid #ff9800; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">âœï¸ PraktickÃ¡ cviÄenÃ­ k procviÄenÃ­</h3>\n",
    "    <p>VyÅ™eÅ¡te nÃ¡sledujÃ­cÃ­ Ãºkoly. ZaÄnÄ›te od jednoduÅ¡Å¡Å¡Ã­ch a postupujte k sloÅ¾itÄ›jÅ¡Ã­m.</p>\n",
    "</div>\n",
    "\n",
    "# CviÄenÃ­: Ollama API integrace\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **NÃ¡zev Ãºkolu**\n",
    "VytvoÅ™enÃ­ jednoduchÃ©ho chatovacÃ­ho klienta pro Ollama API\n",
    "\n",
    "### **DetailnÃ­ zadÃ¡nÃ­**\n",
    "VytvoÅ™te Python aplikaci, kterÃ¡ komunikuje s Ollama API prostÅ™ednictvÃ­m HTTP poÅ¾adavkÅ¯. Aplikace by mÄ›la umoÅ¾nit uÅ¾ivateli zadÃ¡vat zprÃ¡vy a dostÃ¡vat odpovÄ›di od LLM modelu (napÅ™. `llama3`). PouÅ¾ijte POST poÅ¾adavek k odeslÃ¡nÃ­ dotazu do Ollama API a extrahujte odpovÄ›Ä ze zÃ­skanÃ©ho JSON objektu.\n",
    "\n",
    "### **VstupnÃ­ data/poÅ¾adavky**\n",
    "- Model: `llama3`\n",
    "- TextovÃ½ vstup od uÅ¾ivatele (napÅ™. \"Jak se dÄ›lÃ¡ kÃ¡va?\")\n",
    "- OÄekÃ¡vanÃ© pouÅ¾itÃ­: ZprÃ¡va z konzole\n",
    "\n",
    "### **OÄekÃ¡vanÃ½ vÃ½stup**\n",
    "Aplikace vytiskne odpovÄ›Ä od modelu na obrazovku. NapÅ™Ã­klad:\n",
    "```\n",
    "UÅ¾ivatel: Jak se dÄ›lÃ¡ kÃ¡va?\n",
    "OdpovÄ›Ä: KÃ¡vu se dÄ›lÃ¡ vaÅ™enÃ­m vody a mlÃ­ÄnÃ½ch nebo kÃ¡vovÃ½ch zrn.\n",
    "\n",
    "```\n",
    "\n",
    "### **Hints/NÃ¡povÄ›da**\n",
    "1. PouÅ¾ijte knihovnu `requests` pro odeslÃ¡nÃ­ POST poÅ¾adavku.\n",
    "2. VytvoÅ™te JSON tÄ›lo s klÃ­Äi `model` a `prompt`.\n",
    "3. Zpracujte odpovÄ›Ä z API â€“ pozor na `response.json()['message']['content']`.\n",
    "4. Implementujte jednoduchou smyÄku pro pokraÄovÃ¡nÃ­ v konverzaci.\n",
    "\n",
    "### **Kostra Å™eÅ¡enÃ­**\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    # OdeÅ¡lete poÅ¾adavek a zpracujte odpovÄ›Ä\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"UÅ¾ivatel: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            break\n",
    "        response = chat_with_ollama(user_input)\n",
    "        print(f\"OdpovÄ›Ä: {response}\")\n",
    "```\n",
    "\n",
    "### **BonusovÃ© rozÅ¡Ã­Å™enÃ­**\n",
    "- PÅ™idejte historii konverzace (kontext) do poÅ¾adavku.\n",
    "- UmoÅ¾nÄ›te vÃ½bÄ›r modelu pÅ™es argumenty.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **NÃ¡zev Ãºkolu**\n",
    "Integrace Ollama API s vlastnÃ­m streamovanÃ½m generÃ¡torem odpovÄ›dÃ­\n",
    "\n",
    "### **DetailnÃ­ zadÃ¡nÃ­**\n",
    "VytvoÅ™te Python skript, kterÃ½ pouÅ¾Ã­vÃ¡ streamovanÃ© odpovÄ›di z Ollama API. PÅ™i zadÃ¡nÃ­ dotazu by mÄ›la aplikace postupnÄ› vypisovat znaky odpovÄ›di (streaming), neÅ¾ se celÃ¡ odpovÄ›Ä dokonÄÃ­.\n",
    "\n",
    "### **VstupnÃ­ data/poÅ¾adavky**\n",
    "- Model: `llama3`\n",
    "- ZprÃ¡va od uÅ¾ivatele (napÅ™. \"NapiÅ¡ krÃ¡tkÃ½ text o pÅ™Ã­rodÄ›\")\n",
    "\n",
    "### **OÄekÃ¡vanÃ½ vÃ½stup**\n",
    "PostupnÃ© vypsÃ¡nÃ­ odpovÄ›di znak po znaku nebo Å™Ã¡dek po Å™Ã¡dku, podobnÄ› jako chatovacÃ­ rozhranÃ­.\n",
    "\n",
    "### **Hints/NÃ¡povÄ›da**\n",
    "1. PouÅ¾ijte `stream=True` ve volÃ¡nÃ­ `requests.post()`.\n",
    "2. Iterujte pÅ™es `response.iter_lines()` pro zpracovÃ¡nÃ­ streamu.\n",
    "3. Zpracujte JSON Å™Ã¡dky (`line.decode(\"utf-8\")`) a extrahujte text.\n",
    "4. UmoÅ¾nÄ›te uÅ¾ivateli vypsat odpovÄ›Ä po jednotlivÃ½ch ÄÃ¡stech.\n",
    "\n",
    "### **Kostra Å™eÅ¡enÃ­**\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def stream_response(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": True\n",
    "    }\n",
    "    response = requests.post(url, json=data, stream=True)\n",
    "    # Zpracuj stream a tiskni vÃ½stup\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Dotaz: \")\n",
    "    stream_response(user_input)\n",
    "```\n",
    "\n",
    "### **BonusovÃ© rozÅ¡Ã­Å™enÃ­**\n",
    "- PÅ™idejte moÅ¾nost pÅ™eruÅ¡enÃ­ streamu (napÅ™. klÃ¡vesa `q`).\n",
    "- UmoÅ¾nÄ›te zÃ¡pis streamovanÃ© odpovÄ›di do souboru.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **NÃ¡zev Ãºkolu**\n",
    "Implementace API proxy serveru pro Ollama\n",
    "\n",
    "### **DetailnÃ­ zadÃ¡nÃ­**\n",
    "VytvoÅ™te jednoduchÃ½ Flask server, kterÃ½ pÅ™edÃ¡vÃ¡ poÅ¾adavky na Ollama API a zpÄ›t od nÄ›j. Tento server by mÄ›l fungovat jako proxy, tedy klient se obracÃ­ na vÃ¡Å¡ server, ten pak volÃ¡ Ollama a odpovÃ­dÃ¡ zpÄ›t.\n",
    "\n",
    "### **VstupnÃ­ data/poÅ¾adavky**\n",
    "- HTTP endpoint `/chat`\n",
    "- Metoda: POST\n",
    "- Vstup: JSON s klÃ­Äi `prompt` a `model`\n",
    "\n",
    "### **OÄekÃ¡vanÃ½ vÃ½stup**\n",
    "Server pÅ™ijÃ­mÃ¡ poÅ¾adavek, pÅ™edÃ¡vÃ¡ ho do Ollama a vrÃ¡tÃ­ odpovÄ›Ä klientovi.\n",
    "\n",
    "### **Hints/NÃ¡povÄ›da**\n",
    "1. PouÅ¾ijte Flask pro tvorbu API endpointu.\n",
    "2. VytvoÅ™te funkci, kterÃ¡ komunikuje s `http://localhost:11434/api/generate`.\n",
    "3. PÅ™edÃ¡vejte JSON request pÅ™Ã­mo do Ollama.\n",
    "4. VrÃ¡tÃ­te odpovÄ›Ä jako JSON.\n",
    "\n",
    "### **Kostra Å™eÅ¡enÃ­**\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = request.json\n",
    "    ollama_url = \"http://localhost:11434/api/generate\"\n",
    "    response = requests.post(ollama_url, json=data)\n",
    "    return jsonify(response.json())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000)\n",
    "```\n",
    "\n",
    "### **BonusovÃ© rozÅ¡Ã­Å™enÃ­**\n",
    "- PÅ™idejte middleware pro logovÃ¡nÃ­ poÅ¾adavkÅ¯.\n",
    "- OÅ¡etÅ™ete chyby z Ollama API (napÅ™. pÅ™i Å¡patnÃ©m modelu).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **NÃ¡zev Ãºkolu**\n",
    "VytvoÅ™enÃ­ nÃ¡stroje pro sprÃ¡vu modelÅ¯ v Ollama\n",
    "\n",
    "### **DetailnÃ­ zadÃ¡nÃ­**\n",
    "NapiÅ¡te skript, kterÃ½ umoÅ¾Åˆuje spravovat modely pomocÃ­ Ollama API (napÅ™. stahovat modely, seznam modelÅ¯, mazat jejich lokÃ¡lnÃ­ kopie). VyuÅ¾ijte GET/DELETE endpointy Ollama API.\n",
    "\n",
    "### **VstupnÃ­ data/poÅ¾adavky**\n",
    "- MoÅ¾nosti: `list`, `pull`, `delete`\n",
    "- Parametry jako nÃ¡zev modelu (`llama3`, `mistral`)\n",
    "\n",
    "### **OÄekÃ¡vanÃ½ vÃ½stup**\n",
    "Skript podporuje pÅ™Ã­kazovou Å™Ã¡dku s podporkami pro jednotlivÃ© operace (napÅ™. `python manager.py pull llama3`).\n",
    "\n",
    "### **Hints/NÃ¡povÄ›da**\n",
    "1. PouÅ¾ijte `argparse` pro zpracovÃ¡nÃ­ argumentÅ¯.\n",
    "2. VyuÅ¾ijte GET `/api/tags` pro seznam modelÅ¯.\n",
    "3. POST na `/api/generate` pro stahovÃ¡nÃ­ modelu (`pull`).\n",
    "4. DELETE na `/api/generate` pro smazÃ¡nÃ­.\n",
    "\n",
    "### **Kostra Å™eÅ¡enÃ­**\n",
    "```python\n",
    "import requests\n",
    "import argparse\n",
    "\n",
    "def list_models():\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    print(response.json())\n",
    "\n",
    "def pull_model(model):\n",
    "    data = {\"name\": model}\n",
    "    response = requests.post(\"http://localhost:11434/api/generate\", json=data)\n",
    "    print(f\"Model {model} downloaded\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca490dc",
   "metadata": {},
   "source": [
    "## ğŸ“š DalÅ¡Ã­ zdroje a materiÃ¡ly\n",
    "\n",
    "<div style=\"background: #e3f2fd; padding: 20px; border-left: 5px solid #2196f3; margin: 20px 0; border-radius: 5px;\">\n",
    "    <h3 style=\"color: #2c3e50; margin-top: 0;\">ğŸ”— DoporuÄenÃ© materiÃ¡ly k dalÅ¡Ã­mu studiu</h3>\n",
    "</div>\n",
    "\n",
    "# Zdroje pro kapitolu: Ollama API integrace\n",
    "\n",
    "## 1. DoporuÄenÃ© ÄlÃ¡nky a tutoriÃ¡ly\n",
    "\n",
    "1. **\"Integrating Ollama API with Python Applications\"** â€“ *Real Python*  \n",
    "   Popis: ÄŒlÃ¡nek se zamÄ›Å™uje na praktickÃ© pouÅ¾itÃ­ Ollama API v Pythonu, vÄetnÄ› pÅ™ipojenÃ­ k modelÅ¯m, odesÃ­lÃ¡nÃ­ dotazÅ¯ a zpracovÃ¡nÃ­ odpovÄ›dÃ­. IdeÃ¡lnÃ­ pro zaÄÃ¡teÄnÃ­ky v programovÃ¡nÃ­.\n",
    "\n",
    "2. **\"Getting Started with Ollama and LLMs\"** â€“ *Hugging Face Blog*  \n",
    "   Popis: VÃ½ukovÃ½ ÄlÃ¡nek, kterÃ½ vysvÄ›tluje, jak spustit Ollama na lokÃ¡lnÃ­m poÄÃ­taÄi a jak integrovat jej s rÅ¯znÃ½mi LLM (Large Language Models). Obsahuje i ukÃ¡zky kÃ³du pro bÄ›Å¾nÃ© scÃ©nÃ¡Å™e.\n",
    "\n",
    "3. **\"Building Chatbots with Ollama API and Node.js\"** â€“ *FreeCodeCamp*  \n",
    "   Popis: TutoriÃ¡l pro vÃ½vojÃ¡Å™e, kteÅ™Ã­ chtÄ›jÃ­ vytvÃ¡Å™et chatboty pomocÃ­ Ollama API a Node.js. VysvÄ›tluje zÃ¡kladnÃ­ principy API volÃ¡nÃ­, streaming odpovÄ›dÃ­ a integraci do webovÃ© aplikace.\n",
    "\n",
    "4. **\"Using Ollama in Production Environments\"** â€“ *Medium Article by AI Engineer*  \n",
    "   Popis: ÄŒlÃ¡nek se zamÄ›Å™uje na praxe pro nasazenÃ­ Ollama API v produkÄnÃ­ch prostÅ™edÃ­ch, zahrnuje optimalizace, zpracovÃ¡nÃ­ chyb a sprÃ¡vu tokenÅ¯.\n",
    "\n",
    "5. **\"Ollama vs Local LLMs: A Practical Guide\"** â€“ *AI Weekly*  \n",
    "   Popis: PraktickÃ½ prÅ¯vodce srovnÃ¡nÃ­m Ollama API s jinÃ½mi nÃ¡stroji pro lokÃ¡lnÃ­ spuÅ¡tÄ›nÃ­ LLM modelÅ¯. UÅ¾iteÄnÃ© pro vÃ½vojÃ¡Å™e, kteÅ™Ã­ hledajÃ­ nejlepÅ¡Ã­ Å™eÅ¡enÃ­ pro jejich projekt.\n",
    "\n",
    "## 2. YouTube videa\n",
    "\n",
    "1. **\"Ollama API Tutorial - Build a Chatbot with Python\"** â€“ *15 minut*  \n",
    "   Popis: Krok za krokem tutoriÃ¡l, jak vytvoÅ™it chatbot pomocÃ­ Ollama API a Pythonu. Videa obsahuje i ladÄ›nÃ­ problÃ©mÅ¯ s pÅ™ipojenÃ­m a zpracovÃ¡nÃ­m odpovÄ›dÃ­.\n",
    "\n",
    "2. **\"How to Use Ollama in Your Web App - Full Demo\"** â€“ *YouTube: Web Dev Simplified* â€“ *20 minut*  \n",
    "   Popis: UkÃ¡zkovÃ© vyuÅ¾itÃ­ Ollama API ve webovÃ© aplikaci s vyuÅ¾itÃ­m JavaScriptu. Demonstruje integraci do Reactu a backendu pomocÃ­ Express.js.\n",
    "\n",
    "3. **\"Ollama API Deep Dive - From Zero to Production\"** â€“ *Tech With Tim* â€“ *25 minut*  \n",
    "   Popis: KomplexnÃ­ vÃ½klad Ollama API s praktickÃ½mi pÅ™Ã­klady, zahrnuje integraci do aplikacÃ­ a zÃ¡klady sprÃ¡vy modelÅ¯ i deploymentu do cloudu.\n",
    "\n",
    "## 3. Knihy a oficiÃ¡lnÃ­ dokumentace\n",
    "\n",
    "1. **Ollama Documentation** â€“ [https://ollama.com/docs](https://ollama.com/docs)  \n",
    "   Popis: OficiÃ¡lnÃ­ dokumentace Ollama, kterÃ¡ poskytuje kompletnÃ­ informace o instalaci, nastavenÃ­ a rozhranÃ­ API. VhodnÃ¡ pro kaÅ¾dÃ©ho vÃ½vojÃ¡Å™e.\n",
    "\n",
    "2. **\"Building AI Applications with LLMs\"** â€“ *by Daniel Bourke*  \n",
    "   Popis: Knihu doporuÄujeme jako dalÅ¡Ã­ zdroj pro pochopenÃ­ prÃ¡ce s LLM a jejich integracÃ­ do aplikacÃ­, vÄetnÄ› praktickÃ½ch pÅ™Ã­kladÅ¯ s Ollama API.\n",
    "\n",
    "3. **\"Hands-On Machine Learning with Python\"** â€“ *AurÃ©lien GÃ©ron*  \n",
    "   Popis: PÅ™Ã­ruÄka zamÄ›Å™enÃ¡ na praxi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ff78f",
   "metadata": {},
   "source": [
    "## ğŸ“ ShrnutÃ­ kapitoly\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 10px; color: white; margin: 30px 0;\">\n",
    "    <h3 style=\"color: white; margin-top: 0;\">âœ… Co jste se nauÄili</h3>\n",
    "    <ul style=\"list-style: none; padding-left: 0;\">\n",
    "        <li>âœ“ Ollama REST API endpoints</li>\n",
    "<li>âœ“ Python client implementation</li>\n",
    "<li>âœ“ Streaming responses</li>\n",
    "<li>âœ“ Model parameters tuning</li>\n",
    "<li>âœ“ Context window management</li>\n",
    "    </ul>\n",
    "    \n",
    "    <h3 style=\"color: white;\">ğŸ¯ KlÃ­ÄovÃ© dovednosti</h3>\n",
    "    <p>Po dokonÄenÃ­ tÃ©to kapitoly byste mÄ›li bÃ½t schopni prakticky pouÅ¾Ã­t vÅ¡echny probranÃ© koncepty.</p>\n",
    "    \n",
    "    <h3 style='color: white;'>â¡ï¸ DalÅ¡Ã­ kapitola</h3><p>Kapitola 33 - pokraÄujte ve studiu!</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "*ğŸ“… Notebook vygenerovÃ¡n: 2025-09-29 13:17:34*  \n",
    "*ğŸ¤– GenerÃ¡tor: Comprehensive Colab Generator v2.0*  \n",
    "*ğŸ“š UÄebnice programovÃ¡nÃ­ - Od zÃ¡kladÅ¯ k AI*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e7ef1",
   "metadata": {},
   "source": [
    "## ğŸ§ª Sandbox - Prostor pro experimenty\n",
    "\n",
    "PouÅ¾ijte nÃ¡sledujÃ­cÃ­ buÅˆky pro vlastnÃ­ experimenty a testovÃ¡nÃ­:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª Zde mÅ¯Å¾ete experimentovat s kÃ³dem z kapitoly\n",
    "# NapiÅ¡te svÅ¯j kÃ³d zde:\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
