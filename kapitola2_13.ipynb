{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö RAG Syst√©my s Hugging Face\n",
    "\n",
    "**Autor:** Praut s.r.o. - AI Integration & Business Automation\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) kombinuje vyhled√°v√°n√≠ v dokumentech s generov√°n√≠m odpovƒõd√≠ pomoc√≠ LLM.\n",
    "\n",
    "V tomto notebooku se nauƒç√≠me:\n",
    "- Z√°klady RAG architektury\n",
    "- Indexov√°n√≠ dokument≈Ø s vektorov√Ωm vyhled√°v√°n√≠m\n",
    "- Chunking strategie pro dlouh√© dokumenty\n",
    "- Integrace s lok√°ln√≠mi LLM modely\n",
    "- Produkƒçn√≠ RAG pipeline s evaluac√≠\n",
    "\n",
    "## Architektura RAG\n",
    "\n",
    "```\n",
    "Dotaz ‚Üí Embedding ‚Üí Vektorov√© vyhled√°v√°n√≠ ‚Üí Relevantn√≠ chunky ‚Üí LLM ‚Üí Odpovƒõƒè\n",
    "                              ‚Üë\n",
    "                    Vektorov√° datab√°ze\n",
    "                    (indexovan√© dokumenty)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalace pot≈ôebn√Ωch knihoven\n",
    "!pip install -q transformers sentence-transformers faiss-cpu langchain langchain-community chromadb pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Vektorov√© vyhled√°v√°n√≠\n",
    "import faiss\n",
    "\n",
    "# Kontrola GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Pou≈æ√≠v√°m za≈ô√≠zen√≠: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Chunking - Rozdƒõlen√≠ Dokument≈Ø\n",
    "\n",
    "Spr√°vn√© rozdƒõlen√≠ dokument≈Ø je kl√≠ƒçov√© pro kvalitu RAG syst√©mu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    \"\"\"Reprezentace jednoho chunku dokumentu.\"\"\"\n",
    "    text: str\n",
    "    metadata: Dict[str, Any]\n",
    "    chunk_id: int\n",
    "    source: str\n",
    "\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    Rozdƒõl√≠ dokumenty na chunky pro indexov√°n√≠.\n",
    "    Podporuje r≈Øzn√© strategie chunkingu.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50,\n",
    "        separator: str = \"\\n\"\n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.separator = separator\n",
    "    \n",
    "    def chunk_by_size(self, text: str, source: str = \"unknown\") -> List[DocumentChunk]:\n",
    "        \"\"\"Rozdƒõl√≠ text na chunky podle velikosti s p≈ôekryvem.\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            # Najdeme konec chunku\n",
    "            end = start + self.chunk_size\n",
    "            \n",
    "            # Pokud nejsme na konci, zkus√≠me ukonƒçit na hranici vƒõty\n",
    "            if end < len(text):\n",
    "                # Hled√°me teƒçku, vyk≈ôiƒçn√≠k nebo otazn√≠k\n",
    "                last_sentence_end = max(\n",
    "                    text.rfind(\".\", start, end),\n",
    "                    text.rfind(\"!\", start, end),\n",
    "                    text.rfind(\"?\", start, end)\n",
    "                )\n",
    "                if last_sentence_end > start:\n",
    "                    end = last_sentence_end + 1\n",
    "            \n",
    "            chunk_text = text[start:end].strip()\n",
    "            \n",
    "            if chunk_text:\n",
    "                chunks.append(DocumentChunk(\n",
    "                    text=chunk_text,\n",
    "                    metadata={\n",
    "                        \"start_char\": start,\n",
    "                        \"end_char\": end,\n",
    "                        \"chunk_size\": len(chunk_text)\n",
    "                    },\n",
    "                    chunk_id=chunk_id,\n",
    "                    source=source\n",
    "                ))\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # Posun s p≈ôekryvem\n",
    "            start = end - self.chunk_overlap\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_paragraphs(self, text: str, source: str = \"unknown\") -> List[DocumentChunk]:\n",
    "        \"\"\"Rozdƒõl√≠ text na chunky podle odstavc≈Ø.\"\"\"\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            \n",
    "            # Pokud by chunk byl p≈ô√≠li≈° velk√Ω, ulo≈æ√≠me a zaƒçneme nov√Ω\n",
    "            if len(current_chunk) + len(para) > self.chunk_size and current_chunk:\n",
    "                chunks.append(DocumentChunk(\n",
    "                    text=current_chunk.strip(),\n",
    "                    metadata={\"type\": \"paragraph\"},\n",
    "                    chunk_id=chunk_id,\n",
    "                    source=source\n",
    "                ))\n",
    "                chunk_id += 1\n",
    "                current_chunk = \"\"\n",
    "            \n",
    "            current_chunk += para + \"\\n\\n\"\n",
    "        \n",
    "        # Posledn√≠ chunk\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(DocumentChunk(\n",
    "                text=current_chunk.strip(),\n",
    "                metadata={\"type\": \"paragraph\"},\n",
    "                chunk_id=chunk_id,\n",
    "                source=source\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_by_sentences(self, text: str, source: str = \"unknown\", sentences_per_chunk: int = 5) -> List[DocumentChunk]:\n",
    "        \"\"\"Rozdƒõl√≠ text na chunky podle vƒõt.\"\"\"\n",
    "        # Jednoduch√Ω sentence splitter\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        chunks = []\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for i in range(0, len(sentences), sentences_per_chunk):\n",
    "            chunk_sentences = sentences[i:i + sentences_per_chunk]\n",
    "            chunk_text = \" \".join(chunk_sentences)\n",
    "            \n",
    "            if chunk_text.strip():\n",
    "                chunks.append(DocumentChunk(\n",
    "                    text=chunk_text.strip(),\n",
    "                    metadata={\n",
    "                        \"type\": \"sentences\",\n",
    "                        \"sentence_count\": len(chunk_sentences)\n",
    "                    },\n",
    "                    chunk_id=chunk_id,\n",
    "                    source=source\n",
    "                ))\n",
    "                chunk_id += 1\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uk√°zkov√Ω dokument\n",
    "sample_document = \"\"\"\n",
    "Umƒõl√° inteligence v podnik√°n√≠\n",
    "\n",
    "Umƒõl√° inteligence (AI) transformuje zp≈Øsob, jak√Ωm firmy funguj√≠. Od automatizace rutinn√≠ch √∫kol≈Ø po pokroƒçilou anal√Ωzu dat, AI nab√≠z√≠ ≈°irokou ≈°k√°lu mo≈ænost√≠ pro zv√Ω≈°en√≠ efektivity a konkurenceschopnosti.\n",
    "\n",
    "Jednou z nejƒçastƒõj≈°√≠ch aplikac√≠ AI je z√°kaznick√Ω servis. Chatboty poh√°nƒõn√© AI dok√°≈æ√≠ odpov√≠dat na bƒõ≈æn√© dotazy z√°kazn√≠k≈Ø 24/7, co≈æ sni≈æuje n√°klady na podporu a zlep≈°uje z√°kaznickou zku≈°enost. Modern√≠ chatboty vyu≈æ√≠vaj√≠ zpracov√°n√≠ p≈ôirozen√©ho jazyka (NLP) k porozumƒõn√≠ kontextu a poskytov√°n√≠ relevantn√≠ch odpovƒõd√≠.\n",
    "\n",
    "Dal≈°√≠ v√Ωznamnou oblast√≠ je prediktivn√≠ analytika. AI modely dok√°≈æ√≠ analyzovat historick√° data a p≈ôedpov√≠dat budouc√≠ trendy, co≈æ pom√°h√° firm√°m l√©pe pl√°novat z√°soby, optimalizovat ceny a identifikovat potenci√°ln√≠ probl√©my d≈ô√≠ve, ne≈æ nastanou.\n",
    "\n",
    "Automatizace proces≈Ø pomoc√≠ AI zahrnuje robotickou automatizaci proces≈Ø (RPA), kter√° dok√°≈æe p≈ôevz√≠t opakuj√≠c√≠ se administrativn√≠ √∫koly. To umo≈æ≈àuje zamƒõstnanc≈Øm soust≈ôedit se na kreativnƒõj≈°√≠ a strategiƒçtƒõj≈°√≠ ƒçinnosti.\n",
    "\n",
    "Implementace AI vy≈æaduje peƒçliv√© pl√°nov√°n√≠. Firmy mus√≠ zv√°≈æit kvalitu dat, technickou infrastrukturu, ≈°kolen√≠ zamƒõstnanc≈Ø a etick√© aspekty vyu≈æ√≠v√°n√≠ AI. √öspƒõ≈°n√° implementace AI m≈Ø≈æe v√©st k v√Ωznamn√Ωm √∫spor√°m n√°klad≈Ø a zv√Ω≈°en√≠ produktivity.\n",
    "\"\"\"\n",
    "\n",
    "# Test chunkingu\n",
    "chunker = DocumentChunker(chunk_size=400, chunk_overlap=50)\n",
    "chunks = chunker.chunk_by_size(sample_document, source=\"ai_business.txt\")\n",
    "\n",
    "print(f\"Poƒçet chunk≈Ø: {len(chunks)}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"\\nChunk {chunk.chunk_id}:\")\n",
    "    print(f\"D√©lka: {len(chunk.text)} znak≈Ø\")\n",
    "    print(f\"Text: {chunk.text[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vector Store - Vektorov√° Datab√°ze\n",
    "\n",
    "Indexujeme chunky pomoc√≠ embeddings a FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Vektorov√° datab√°ze pro RAG syst√©m.\n",
    "    Pou≈æ√≠v√° Sentence Transformers pro embeddings a FAISS pro vyhled√°v√°n√≠.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        index_type: str = \"flat\"  # flat, ivf, hnsw\n",
    "    ):\n",
    "        print(f\"Naƒç√≠t√°m embedding model: {embedding_model}\")\n",
    "        self.encoder = SentenceTransformer(embedding_model)\n",
    "        self.embedding_dim = self.encoder.get_sentence_embedding_dimension()\n",
    "        self.index_type = index_type\n",
    "        \n",
    "        # Inicializace FAISS indexu\n",
    "        self.index = None\n",
    "        self.chunks: List[DocumentChunk] = []\n",
    "        \n",
    "        print(f\"Embedding dimenze: {self.embedding_dim}\")\n",
    "    \n",
    "    def _create_index(self, n_vectors: int):\n",
    "        \"\"\"Vytvo≈ô√≠ FAISS index podle typu.\"\"\"\n",
    "        if self.index_type == \"flat\":\n",
    "            # P≈ôesn√© vyhled√°v√°n√≠ - nejlep≈°√≠ pro mal√© datasety\n",
    "            self.index = faiss.IndexFlatIP(self.embedding_dim)\n",
    "        elif self.index_type == \"ivf\":\n",
    "            # Approximate search - rychlej≈°√≠ pro velk√© datasety\n",
    "            n_clusters = min(int(np.sqrt(n_vectors)), 100)\n",
    "            quantizer = faiss.IndexFlatIP(self.embedding_dim)\n",
    "            self.index = faiss.IndexIVFFlat(quantizer, self.embedding_dim, n_clusters)\n",
    "        elif self.index_type == \"hnsw\":\n",
    "            # HNSW - rychl√Ω approximate search\n",
    "            self.index = faiss.IndexHNSWFlat(self.embedding_dim, 32)\n",
    "        else:\n",
    "            self.index = faiss.IndexFlatIP(self.embedding_dim)\n",
    "    \n",
    "    def add_documents(self, chunks: List[DocumentChunk]):\n",
    "        \"\"\"P≈ôid√° dokumenty do indexu.\"\"\"\n",
    "        if not chunks:\n",
    "            return\n",
    "        \n",
    "        # Vytvo≈ôen√≠ embeddings\n",
    "        texts = [chunk.text for chunk in chunks]\n",
    "        embeddings = self.encoder.encode(\n",
    "            texts,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Vytvo≈ôen√≠ indexu pokud neexistuje\n",
    "        if self.index is None:\n",
    "            self._create_index(len(chunks))\n",
    "            \n",
    "            # IVF index mus√≠ b√Ωt natr√©nov√°n\n",
    "            if self.index_type == \"ivf\":\n",
    "                self.index.train(embeddings.astype(np.float32))\n",
    "        \n",
    "        # P≈ôid√°n√≠ do indexu\n",
    "        self.index.add(embeddings.astype(np.float32))\n",
    "        self.chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"P≈ôid√°no {len(chunks)} chunk≈Ø. Celkem: {len(self.chunks)}\")\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 5,\n",
    "        threshold: float = 0.0\n",
    "    ) -> List[Tuple[DocumentChunk, float]]:\n",
    "        \"\"\"Vyhled√° nejrelevantnƒõj≈°√≠ chunky pro dotaz.\"\"\"\n",
    "        if self.index is None or len(self.chunks) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Embedding dotazu\n",
    "        query_embedding = self.encoder.encode(\n",
    "            [query],\n",
    "            normalize_embeddings=True\n",
    "        ).astype(np.float32)\n",
    "        \n",
    "        # Vyhled√°v√°n√≠\n",
    "        scores, indices = self.index.search(query_embedding, min(top_k, len(self.chunks)))\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx >= 0 and score >= threshold:\n",
    "                results.append((self.chunks[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Ulo≈æ√≠ index a metadata.\"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        \n",
    "        # Ulo≈æen√≠ FAISS indexu\n",
    "        faiss.write_index(self.index, f\"{path}/index.faiss\")\n",
    "        \n",
    "        # Ulo≈æen√≠ chunk≈Ø\n",
    "        chunks_data = [\n",
    "            {\n",
    "                \"text\": c.text,\n",
    "                \"metadata\": c.metadata,\n",
    "                \"chunk_id\": c.chunk_id,\n",
    "                \"source\": c.source\n",
    "            }\n",
    "            for c in self.chunks\n",
    "        ]\n",
    "        with open(f\"{path}/chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Index ulo≈æen do: {path}\")\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Naƒçte index a metadata.\"\"\"\n",
    "        # Naƒçten√≠ FAISS indexu\n",
    "        self.index = faiss.read_index(f\"{path}/index.faiss\")\n",
    "        \n",
    "        # Naƒçten√≠ chunk≈Ø\n",
    "        with open(f\"{path}/chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            chunks_data = json.load(f)\n",
    "        \n",
    "        self.chunks = [\n",
    "            DocumentChunk(\n",
    "                text=c[\"text\"],\n",
    "                metadata=c[\"metadata\"],\n",
    "                chunk_id=c[\"chunk_id\"],\n",
    "                source=c[\"source\"]\n",
    "            )\n",
    "            for c in chunks_data\n",
    "        ]\n",
    "        \n",
    "        print(f\"Naƒçteno {len(self.chunks)} chunk≈Ø z: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvo≈ôen√≠ a naplnƒõn√≠ vektorov√© datab√°ze\n",
    "vector_store = VectorStore(embedding_model=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# P≈ôid√°n√≠ v√≠ce dokument≈Ø\n",
    "documents = [\n",
    "    {\n",
    "        \"source\": \"ai_business.txt\",\n",
    "        \"text\": sample_document\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"customer_service.txt\",\n",
    "        \"text\": \"\"\"\n",
    "        Z√°kaznick√Ω servis a podpora\n",
    "        \n",
    "        Kvalitn√≠ z√°kaznick√Ω servis je z√°kladem √∫spƒõ≈°n√©ho podnik√°n√≠. Spokojen√Ω z√°kazn√≠k se vrac√≠ a doporuƒçuje firmu dal≈°√≠m.\n",
    "        \n",
    "        Z√°kladn√≠ principy kvalitn√≠ho servisu zahrnuj√≠ rychlou reakci na dotazy, empatick√Ω p≈ô√≠stup a efektivn√≠ ≈ôe≈°en√≠ probl√©m≈Ø.\n",
    "        Modern√≠ technologie jako CRM syst√©my pom√°haj√≠ sledovat historii komunikace a personalizovat slu≈æby.\n",
    "        \n",
    "        Chatboty dok√°≈æ√≠ odbavit rutinn√≠ dotazy automaticky, ale slo≈æitƒõj≈°√≠ p≈ô√≠pady vy≈æaduj√≠ lidsk√Ω p≈ô√≠stup.\n",
    "        D≈Øle≈æit√© je nastavit spr√°vnou eskalaci - kdy p≈ôedat p≈ô√≠pad ≈æiv√©mu oper√°torovi.\n",
    "        \n",
    "        Mƒõ≈ôen√≠ spokojenosti z√°kazn√≠k≈Ø pomoc√≠ NPS (Net Promoter Score) a CSAT pom√°h√° identifikovat oblasti pro zlep≈°en√≠.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"ecommerce.txt\",\n",
    "        \"text\": \"\"\"\n",
    "        E-commerce a online prodej\n",
    "        \n",
    "        Online obchody za≈æ√≠vaj√≠ boom. Pro √∫spƒõch v e-commerce je kl√≠ƒçov√° u≈æivatelsk√° zku≈°enost, rychl√© naƒç√≠t√°n√≠ str√°nek a d≈Øvƒõryhodnost.\n",
    "        \n",
    "        Optimalizace konverzn√≠ho pomƒõru zahrnuje A/B testov√°n√≠, personalizaci nab√≠dek a zjednodu≈°en√≠ checkout procesu.\n",
    "        Ka≈æd√Ω krok nav√≠c v n√°kupn√≠m procesu sni≈æuje konverzi o 10-15%.\n",
    "        \n",
    "        Logistika a fulfillment jsou kritick√©. Z√°kazn√≠ci oƒçek√°vaj√≠ rychl√© doruƒçen√≠ a snadn√© vr√°cen√≠ zbo≈æ√≠.\n",
    "        Same-day delivery se st√°v√° standardem ve vƒõt≈°√≠ch mƒõstech.\n",
    "        \n",
    "        Platebn√≠ br√°ny mus√≠ podporovat r≈Øzn√© metody - karty, Apple Pay, Google Pay, bankovn√≠ p≈ôevody.\n",
    "        Bezpeƒçnost plateb je prioritou - PCI DSS compliance je nutnost√≠.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Chunk a indexace v≈°ech dokument≈Ø\n",
    "chunker = DocumentChunker(chunk_size=300, chunk_overlap=30)\n",
    "all_chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    doc_chunks = chunker.chunk_by_size(doc[\"text\"], source=doc[\"source\"])\n",
    "    all_chunks.extend(doc_chunks)\n",
    "\n",
    "vector_store.add_documents(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vyhled√°v√°n√≠\n",
    "test_queries = [\n",
    "    \"Jak m≈Ø≈æe AI pomoci se z√°kaznick√Ωm servisem?\",\n",
    "    \"Co je d≈Øle≈æit√© pro e-shop?\",\n",
    "    \"Jak mƒõ≈ôit spokojenost z√°kazn√≠k≈Ø?\",\n",
    "    \"Jak√© jsou v√Ωhody chatbot≈Ø?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nDotaz: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = vector_store.search(query, top_k=2)\n",
    "    for chunk, score in results:\n",
    "        print(f\"  [{score:.3f}] {chunk.source}: {chunk.text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Pipeline - Kompletn√≠ Syst√©m\n",
    "\n",
    "Spoj√≠me vyhled√°v√°n√≠ s generov√°n√≠m odpovƒõd√≠ pomoc√≠ LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Kompletn√≠ RAG pipeline pro question answering.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: VectorStore,\n",
    "        llm_model: str = \"google/flan-t5-base\",\n",
    "        top_k: int = 3,\n",
    "        max_context_length: int = 1500\n",
    "    ):\n",
    "        self.vector_store = vector_store\n",
    "        self.top_k = top_k\n",
    "        self.max_context_length = max_context_length\n",
    "        \n",
    "        # Naƒçten√≠ LLM\n",
    "        print(f\"Naƒç√≠t√°m LLM: {llm_model}\")\n",
    "        self.generator = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=llm_model,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            max_length=512\n",
    "        )\n",
    "        print(\"LLM naƒçten.\")\n",
    "    \n",
    "    def _build_context(self, chunks: List[Tuple[DocumentChunk, float]]) -> str:\n",
    "        \"\"\"Sestav√≠ kontext z nalezen√Ωch chunk≈Ø.\"\"\"\n",
    "        context_parts = []\n",
    "        total_length = 0\n",
    "        \n",
    "        for chunk, score in chunks:\n",
    "            if total_length + len(chunk.text) > self.max_context_length:\n",
    "                break\n",
    "            context_parts.append(chunk.text)\n",
    "            total_length += len(chunk.text)\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def _build_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Vytvo≈ô√≠ prompt pro LLM.\"\"\"\n",
    "        prompt = f\"\"\"Na z√°kladƒõ n√°sleduj√≠c√≠ho kontextu odpovƒõz na ot√°zku. \n",
    "Pokud odpovƒõƒè nen√≠ v kontextu, ≈ôekni ≈æe nev√≠≈°.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Ot√°zka: {query}\n",
    "\n",
    "Odpovƒõƒè:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        return_sources: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Zpracuje dotaz a vr√°t√≠ odpovƒõƒè.\"\"\"\n",
    "        # 1. Vyhled√°n√≠ relevantn√≠ch dokument≈Ø\n",
    "        retrieved = self.vector_store.search(question, top_k=self.top_k)\n",
    "        \n",
    "        if not retrieved:\n",
    "            return {\n",
    "                \"answer\": \"Nena≈°el jsem ≈æ√°dn√© relevantn√≠ informace.\",\n",
    "                \"sources\": [],\n",
    "                \"context\": \"\"\n",
    "            }\n",
    "        \n",
    "        # 2. Sestaven√≠ kontextu\n",
    "        context = self._build_context(retrieved)\n",
    "        \n",
    "        # 3. Generov√°n√≠ odpovƒõdi\n",
    "        prompt = self._build_prompt(question, context)\n",
    "        response = self.generator(prompt)[0][\"generated_text\"]\n",
    "        \n",
    "        # 4. P≈ô√≠prava v√Ωsledku\n",
    "        result = {\n",
    "            \"answer\": response.strip(),\n",
    "            \"context\": context\n",
    "        }\n",
    "        \n",
    "        if return_sources:\n",
    "            result[\"sources\"] = [\n",
    "                {\n",
    "                    \"source\": chunk.source,\n",
    "                    \"score\": score,\n",
    "                    \"text\": chunk.text[:200] + \"...\"\n",
    "                }\n",
    "                for chunk, score in retrieved\n",
    "            ]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Zpracuje v√≠ce dotaz≈Ø najednou.\"\"\"\n",
    "        return [self.query(q) for q in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvo≈ôen√≠ RAG pipeline\n",
    "rag = RAGPipeline(\n",
    "    vector_store=vector_store,\n",
    "    llm_model=\"google/flan-t5-base\",\n",
    "    top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG pipeline\n",
    "questions = [\n",
    "    \"Jak m≈Ø≈æe AI pomoci v z√°kaznick√©m servisu?\",\n",
    "    \"Co je d≈Øle≈æit√© pro √∫spƒõch e-shopu?\",\n",
    "    \"Jak mƒõ≈ôit spokojenost z√°kazn√≠k≈Ø?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"OT√ÅZKA: {question}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = rag.query(question)\n",
    "    \n",
    "    print(f\"\\nODPOVƒöƒé: {result['answer']}\")\n",
    "    print(f\"\\nZDROJE:\")\n",
    "    for source in result[\"sources\"]:\n",
    "        print(f\"  - {source['source']} (score: {source['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pokroƒçil√Ω RAG s Re-ranking\n",
    "\n",
    "P≈ôid√°me cross-encoder pro lep≈°√≠ hodnocen√≠ relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    \"\"\"\n",
    "    Pokroƒçil√Ω RAG s re-rankingem a hybrid search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: VectorStore,\n",
    "        llm_model: str = \"google/flan-t5-base\",\n",
    "        reranker_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "        initial_k: int = 10,\n",
    "        final_k: int = 3\n",
    "    ):\n",
    "        self.vector_store = vector_store\n",
    "        self.initial_k = initial_k\n",
    "        self.final_k = final_k\n",
    "        \n",
    "        # LLM\n",
    "        print(f\"Naƒç√≠t√°m LLM: {llm_model}\")\n",
    "        self.generator = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=llm_model,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Re-ranker (Cross-encoder)\n",
    "        print(f\"Naƒç√≠t√°m re-ranker: {reranker_model}\")\n",
    "        self.reranker = CrossEncoder(reranker_model)\n",
    "        print(\"Modely naƒçteny.\")\n",
    "    \n",
    "    def _rerank(self, query: str, chunks: List[Tuple[DocumentChunk, float]]) -> List[Tuple[DocumentChunk, float]]:\n",
    "        \"\"\"Re-rankuje v√Ωsledky pomoc√≠ cross-encoderu.\"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "        \n",
    "        # P≈ô√≠prava p√°r≈Ø pro cross-encoder\n",
    "        pairs = [(query, chunk.text) for chunk, _ in chunks]\n",
    "        \n",
    "        # Sk√≥re z cross-encoderu\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Kombinace s p≈Øvodn√≠m sk√≥re\n",
    "        reranked = []\n",
    "        for (chunk, original_score), new_score in zip(chunks, scores):\n",
    "            combined_score = 0.3 * original_score + 0.7 * new_score  # V√°ha pro cross-encoder\n",
    "            reranked.append((chunk, combined_score))\n",
    "        \n",
    "        # Se≈ôazen√≠ podle nov√©ho sk√≥re\n",
    "        reranked.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return reranked[:self.final_k]\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        use_reranking: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Zpracuje dotaz s voliteln√Ωm re-rankingem.\"\"\"\n",
    "        # 1. Inicialn√≠ vyhled√°v√°n√≠ (v√≠ce v√Ωsledk≈Ø)\n",
    "        initial_results = self.vector_store.search(question, top_k=self.initial_k)\n",
    "        \n",
    "        if not initial_results:\n",
    "            return {\"answer\": \"Nena≈°el jsem relevantn√≠ informace.\", \"sources\": []}\n",
    "        \n",
    "        # 2. Re-ranking (pokud je povolen)\n",
    "        if use_reranking:\n",
    "            final_results = self._rerank(question, initial_results)\n",
    "        else:\n",
    "            final_results = initial_results[:self.final_k]\n",
    "        \n",
    "        # 3. Sestaven√≠ kontextu\n",
    "        context = \"\\n\\n\".join([chunk.text for chunk, _ in final_results])\n",
    "        \n",
    "        # 4. Generov√°n√≠ odpovƒõdi\n",
    "        prompt = f\"\"\"Odpovƒõz na ot√°zku na z√°kladƒõ kontextu. Buƒè struƒçn√Ω a konkr√©tn√≠.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Ot√°zka: {question}\n",
    "\n",
    "Odpovƒõƒè:\"\"\"\n",
    "        \n",
    "        response = self.generator(prompt)[0][\"generated_text\"]\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.strip(),\n",
    "            \"sources\": [\n",
    "                {\"source\": c.source, \"score\": s, \"text\": c.text[:150]}\n",
    "                for c, s in final_results\n",
    "            ],\n",
    "            \"reranked\": use_reranking\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pokroƒçil√©ho RAG\n",
    "advanced_rag = AdvancedRAGPipeline(\n",
    "    vector_store=vector_store,\n",
    "    llm_model=\"google/flan-t5-base\",\n",
    "    initial_k=10,\n",
    "    final_k=3\n",
    ")\n",
    "\n",
    "question = \"Jak√© technologie pom√°haj√≠ v z√°kaznick√©m servisu?\"\n",
    "\n",
    "# Bez re-rankingu\n",
    "result_basic = advanced_rag.query(question, use_reranking=False)\n",
    "print(\"BEZ RE-RANKINGU:\")\n",
    "print(f\"Odpovƒõƒè: {result_basic['answer']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# S re-rankingem\n",
    "result_reranked = advanced_rag.query(question, use_reranking=True)\n",
    "print(\"S RE-RANKINGEM:\")\n",
    "print(f\"Odpovƒõƒè: {result_reranked['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Produkƒçn√≠ RAG Syst√©m\n",
    "\n",
    "Kompletn√≠ syst√©m s cachingem, logov√°n√≠m a evaluac√≠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import hashlib\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ProductionRAG:\n",
    "    \"\"\"\n",
    "    Produkƒçn√≠ RAG syst√©m s pokroƒçil√Ωmi funkcemi.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        llm_model: str = \"google/flan-t5-base\",\n",
    "        cache_size: int = 100\n",
    "    ):\n",
    "        # Komponenty\n",
    "        self.chunker = DocumentChunker(chunk_size=400, chunk_overlap=50)\n",
    "        self.vector_store = VectorStore(embedding_model=embedding_model)\n",
    "        \n",
    "        # LLM\n",
    "        self.generator = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=llm_model,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Cache\n",
    "        self.cache = OrderedDict()\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        # Statistiky\n",
    "        self.stats = {\n",
    "            \"total_queries\": 0,\n",
    "            \"cache_hits\": 0,\n",
    "            \"avg_retrieval_time\": 0,\n",
    "            \"avg_generation_time\": 0\n",
    "        }\n",
    "        \n",
    "        # Historie dotaz≈Ø\n",
    "        self.query_history = []\n",
    "    \n",
    "    def add_document(self, text: str, source: str, metadata: Dict = None):\n",
    "        \"\"\"P≈ôid√° dokument do syst√©mu.\"\"\"\n",
    "        chunks = self.chunker.chunk_by_size(text, source=source)\n",
    "        \n",
    "        # P≈ôid√°n√≠ metadat\n",
    "        if metadata:\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata.update(metadata)\n",
    "        \n",
    "        self.vector_store.add_documents(chunks)\n",
    "        return len(chunks)\n",
    "    \n",
    "    def add_documents_from_folder(self, folder_path: str, extensions: List[str] = [\".txt\", \".md\"]):\n",
    "        \"\"\"Naƒçte dokumenty ze slo≈æky.\"\"\"\n",
    "        total_chunks = 0\n",
    "        \n",
    "        for filename in os.listdir(folder_path):\n",
    "            if any(filename.endswith(ext) for ext in extensions):\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                chunks = self.add_document(text, source=filename)\n",
    "                total_chunks += chunks\n",
    "                print(f\"  Naƒçteno {filename}: {chunks} chunk≈Ø\")\n",
    "        \n",
    "        return total_chunks\n",
    "    \n",
    "    def _get_cache_key(self, query: str) -> str:\n",
    "        \"\"\"Vytvo≈ô√≠ kl√≠ƒç pro cache.\"\"\"\n",
    "        return hashlib.md5(query.lower().strip().encode()).hexdigest()\n",
    "    \n",
    "    def _update_cache(self, key: str, value: Dict):\n",
    "        \"\"\"Aktualizuje cache s LRU strategi√≠.\"\"\"\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        else:\n",
    "            if len(self.cache) >= self.cache_size:\n",
    "                self.cache.popitem(last=False)\n",
    "            self.cache[key] = value\n",
    "    \n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        top_k: int = 3,\n",
    "        use_cache: bool = True,\n",
    "        temperature: float = 0.7\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Zpracuje dotaz.\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.stats[\"total_queries\"] += 1\n",
    "        cache_key = self._get_cache_key(question)\n",
    "        \n",
    "        # Cache check\n",
    "        if use_cache and cache_key in self.cache:\n",
    "            self.stats[\"cache_hits\"] += 1\n",
    "            result = self.cache[cache_key].copy()\n",
    "            result[\"cached\"] = True\n",
    "            return result\n",
    "        \n",
    "        # Retrieval\n",
    "        retrieval_start = time.time()\n",
    "        retrieved = self.vector_store.search(question, top_k=top_k)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        \n",
    "        if not retrieved:\n",
    "            return {\"answer\": \"Nena≈°el jsem relevantn√≠ informace.\", \"sources\": []}\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join([chunk.text for chunk, _ in retrieved])\n",
    "        \n",
    "        # Generation\n",
    "        generation_start = time.time()\n",
    "        prompt = f\"\"\"Odpovƒõz na ot√°zku na z√°kladƒõ kontextu. Odpovƒõƒè by mƒõla b√Ωt struƒçn√° a p≈ôesn√°.\n",
    "\n",
    "Kontext:\n",
    "{context}\n",
    "\n",
    "Ot√°zka: {question}\n",
    "\n",
    "Odpovƒõƒè:\"\"\"\n",
    "        \n",
    "        response = self.generator(\n",
    "            prompt,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature if temperature > 0 else None\n",
    "        )[0][\"generated_text\"]\n",
    "        generation_time = time.time() - generation_start\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            \"answer\": response.strip(),\n",
    "            \"sources\": [\n",
    "                {\"source\": c.source, \"score\": float(s), \"chunk_id\": c.chunk_id}\n",
    "                for c, s in retrieved\n",
    "            ],\n",
    "            \"metadata\": {\n",
    "                \"retrieval_time\": retrieval_time,\n",
    "                \"generation_time\": generation_time,\n",
    "                \"total_time\": time.time() - start_time,\n",
    "                \"top_k\": top_k\n",
    "            },\n",
    "            \"cached\": False\n",
    "        }\n",
    "        \n",
    "        # Update cache\n",
    "        if use_cache:\n",
    "            self._update_cache(cache_key, result)\n",
    "        \n",
    "        # Log query\n",
    "        self.query_history.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"sources_count\": len(result[\"sources\"]),\n",
    "            \"time\": result[\"metadata\"][\"total_time\"]\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Vr√°t√≠ statistiky syst√©mu.\"\"\"\n",
    "        cache_hit_rate = (\n",
    "            self.stats[\"cache_hits\"] / self.stats[\"total_queries\"]\n",
    "            if self.stats[\"total_queries\"] > 0 else 0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": self.stats[\"total_queries\"],\n",
    "            \"cache_hit_rate\": cache_hit_rate,\n",
    "            \"documents_indexed\": len(self.vector_store.chunks),\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"recent_queries\": self.query_history[-5:] if self.query_history else []\n",
    "        }\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        test_questions: List[str],\n",
    "        expected_sources: List[str] = None\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Evaluuje syst√©m na testovac√≠ch datech.\"\"\"\n",
    "        results = {\n",
    "            \"total_questions\": len(test_questions),\n",
    "            \"avg_sources_returned\": 0,\n",
    "            \"avg_top_score\": 0,\n",
    "            \"source_accuracy\": 0 if expected_sources else None\n",
    "        }\n",
    "        \n",
    "        total_sources = 0\n",
    "        total_top_score = 0\n",
    "        correct_sources = 0\n",
    "        \n",
    "        for i, question in enumerate(test_questions):\n",
    "            response = self.query(question, use_cache=False)\n",
    "            \n",
    "            sources = response.get(\"sources\", [])\n",
    "            total_sources += len(sources)\n",
    "            \n",
    "            if sources:\n",
    "                total_top_score += sources[0][\"score\"]\n",
    "            \n",
    "            if expected_sources and i < len(expected_sources):\n",
    "                if any(s[\"source\"] == expected_sources[i] for s in sources):\n",
    "                    correct_sources += 1\n",
    "        \n",
    "        results[\"avg_sources_returned\"] = total_sources / len(test_questions)\n",
    "        results[\"avg_top_score\"] = total_top_score / len(test_questions)\n",
    "        \n",
    "        if expected_sources:\n",
    "            results[\"source_accuracy\"] = correct_sources / len(test_questions)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Ulo≈æ√≠ syst√©m.\"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        self.vector_store.save(f\"{path}/vector_store\")\n",
    "        \n",
    "        # Ulo≈æen√≠ statistik\n",
    "        with open(f\"{path}/stats.json\", \"w\") as f:\n",
    "            json.dump(self.stats, f)\n",
    "        \n",
    "        print(f\"RAG syst√©m ulo≈æen do: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvo≈ôen√≠ produkƒçn√≠ho RAG syst√©mu\n",
    "prod_rag = ProductionRAG(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    llm_model=\"google/flan-t5-base\",\n",
    "    cache_size=50\n",
    ")\n",
    "\n",
    "# P≈ôid√°n√≠ dokument≈Ø\n",
    "for doc in documents:\n",
    "    prod_rag.add_document(doc[\"text\"], source=doc[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test produkƒçn√≠ho RAG\n",
    "test_questions = [\n",
    "    \"Co je prediktivn√≠ analytika?\",\n",
    "    \"Jak zlep≈°it konverzi v e-shopu?\",\n",
    "    \"Jak√© jsou v√Ωhody chatbot≈Ø?\",\n",
    "    \"Co je NPS?\"\n",
    "]\n",
    "\n",
    "print(\"TESTOV√ÅN√ç PRODUKƒåN√çHO RAG SYST√âMU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for q in test_questions:\n",
    "    result = prod_rag.query(q)\n",
    "    print(f\"\\nOt√°zka: {q}\")\n",
    "    print(f\"Odpovƒõƒè: {result['answer']}\")\n",
    "    print(f\"ƒåas: {result['metadata']['total_time']:.3f}s\")\n",
    "    print(f\"Cached: {result['cached']}\")\n",
    "\n",
    "# Statistiky\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTIKY:\")\n",
    "stats = prod_rag.get_statistics()\n",
    "for key, value in stats.items():\n",
    "    if key != \"recent_queries\":\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cachingu - druh√Ω dotaz by mƒõl b√Ωt z cache\n",
    "print(\"TEST CACHINGU:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "question = \"Co je prediktivn√≠ analytika?\"\n",
    "\n",
    "# Prvn√≠ dotaz\n",
    "result1 = prod_rag.query(question)\n",
    "print(f\"Prvn√≠ dotaz - ƒåas: {result1['metadata']['total_time']:.3f}s, Cached: {result1['cached']}\")\n",
    "\n",
    "# Druh√Ω dotaz (z cache)\n",
    "result2 = prod_rag.query(question)\n",
    "print(f\"Druh√Ω dotaz - Cached: {result2['cached']}\")\n",
    "\n",
    "# Statistiky\n",
    "stats = prod_rag.get_statistics()\n",
    "print(f\"\\nCache hit rate: {stats['cache_hit_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluace syst√©mu\n",
    "eval_questions = [\n",
    "    \"Jak AI pom√°h√° firm√°m?\",\n",
    "    \"Co je d≈Øle≈æit√© pro e-commerce?\",\n",
    "    \"Jak funguje z√°kaznick√Ω servis?\",\n",
    "]\n",
    "\n",
    "expected = [\"ai_business.txt\", \"ecommerce.txt\", \"customer_service.txt\"]\n",
    "\n",
    "eval_results = prod_rag.evaluate(eval_questions, expected_sources=expected)\n",
    "\n",
    "print(\"EVALUACE SYST√âMU:\")\n",
    "print(\"=\"*40)\n",
    "for key, value in eval_results.items():\n",
    "    if value is not None:\n",
    "        print(f\"  {key}: {value:.3f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrnut√≠\n",
    "\n",
    "V tomto notebooku jsme implementovali:\n",
    "\n",
    "1. **Document Chunking** - r≈Øzn√© strategie rozdƒõlen√≠ dokument≈Ø\n",
    "2. **Vector Store** - FAISS index pro rychl√© vyhled√°v√°n√≠\n",
    "3. **Z√°kladn√≠ RAG Pipeline** - retrieval + generation\n",
    "4. **Pokroƒçil√Ω RAG** - s cross-encoder re-rankingem\n",
    "5. **Produkƒçn√≠ RAG** - s cachingem, statistikami a evaluac√≠\n",
    "\n",
    "### Dal≈°√≠ vylep≈°en√≠ pro produkci:\n",
    "- Hybrid search (BM25 + vektory)\n",
    "- Streaming odpovƒõd√≠\n",
    "- Multi-modal RAG (obr√°zky, tabulky)\n",
    "- RAG s citacemi\n",
    "- Fine-tuning embeddings na dom√©nov√Ωch datech"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
