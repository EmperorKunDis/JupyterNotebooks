{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitola 30: Aktivacni funkce - Dejte neuronu osobnost\n",
    "\n",
    "## Cile kapitoly\n",
    "- Pochopit roli aktivacnich funkci v neuronove siti\n",
    "- Naucit se implementovat Sigmoid, Tanh a ReLU\n",
    "- Vizualizovat a porovnat vlastnosti ruznych aktivacnich funkci\n",
    "- Pochopit problem mizejiciho gradientu (vanishing gradient)\n",
    "- Porovnat vliv aktivacnich funkci na uceni site\n",
    "\n",
    "## Predpoklady\n",
    "- Zaklady prace s NumPy a Matplotlib\n",
    "- Pochopeni Perceptronu a vicevrstvych siti (kapitoly 28-29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Co je aktivacni funkce?\n",
    "\n",
    "V minulych kapitolach jsme videli, ze neuron pocita vazeny soucet vstupu:\n",
    "\n",
    "$$z = w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n + b$$\n",
    "\n",
    "Ale co s timto cislem udelame? **Aktivacni funkce** rozhoduje, jak silny signal neuron posle dal.\n",
    "\n",
    "### Proc potrebujeme aktivacni funkce?\n",
    "\n",
    "1. **Nelinearita** - Bez aktivacni funkce by cela sit byla jen linearni transformace\n",
    "2. **Omezeni vystupu** - NekterÃ© funkce omezi vystup do urciteho rozsahu (0-1, -1 az 1)\n",
    "3. **Rozhodovani** - Neuron se musi \"rozhodnout\", jak moc ma byt aktivni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalace potrebnych knihoven\n",
    "!pip install numpy matplotlib scikit-learn torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Knihovny uspesne nacteny!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Skokova funkce (Step Function)\n",
    "\n",
    "Nejjednodussi aktivacni funkce - pouzival ji puvodni Perceptron.\n",
    "\n",
    "$$f(x) = \\begin{cases} 1 & \\text{pokud } x \\geq 0 \\\\ 0 & \\text{jinak} \\end{cases}$$\n",
    "\n",
    "**Vyhody:** Jednoducha, jasne rozhodnuti (0 nebo 1)\n",
    "\n",
    "**Nevyhody:** Neni diferencovatelna (nelze pouzit gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementace skokove funkce\n",
    "def step_function(x):\n",
    "    \"\"\"Skokova funkce - 0 nebo 1\"\"\"\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "# Vizualizace\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, step_function(x), 'b-', linewidth=2, label='Step Function')\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Vstup (z)', fontsize=12)\n",
    "plt.ylabel('Vystup f(z)', fontsize=12)\n",
    "plt.title('Skokova funkce (Step Function)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.2, 1.2)\n",
    "plt.show()\n",
    "\n",
    "print(\"Priklady:\")\n",
    "print(f\"step(-2) = {step_function(-2)}\")\n",
    "print(f\"step(0) = {step_function(0)}\")\n",
    "print(f\"step(3) = {step_function(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sigmoid funkce\n",
    "\n",
    "Klasicka \"S-krivka\", ktera zmackne jakykoliv vstup do rozsahu (0, 1).\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "**Vyhody:**\n",
    "- Hladka a diferencovatelna (lze pouzit gradient descent)\n",
    "- Vystup lze interpretovat jako pravdepodobnost\n",
    "\n",
    "**Nevyhody:**\n",
    "- Problem mizejiciho gradientu pri velmi velkych/malych hodnotach\n",
    "- Vystup neni centrovany kolem nuly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementace Sigmoid\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid aktivacni funkce\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivace sigmoidu - potrebna pro backpropagation\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Vizualizace\n",
    "x = np.linspace(-10, 10, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graf funkce\n",
    "axes[0].plot(x, sigmoid(x), 'b-', linewidth=2, label='Sigmoid')\n",
    "axes[0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='y = 0.5')\n",
    "axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Vstup (z)', fontsize=12)\n",
    "axes[0].set_ylabel('Vystup sigma(z)', fontsize=12)\n",
    "axes[0].set_title('Sigmoid funkce', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "# Graf derivace\n",
    "axes[1].plot(x, sigmoid_derivative(x), 'g-', linewidth=2, label='Derivace Sigmoid')\n",
    "axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Vstup (z)', fontsize=12)\n",
    "axes[1].set_ylabel('sigma\\'(z)', fontsize=12)\n",
    "axes[1].set_title('Derivace Sigmoid (max = 0.25)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Priklady hodnot Sigmoid:\")\n",
    "print(f\"sigmoid(-5) = {sigmoid(-5):.4f} (blizko 0)\")\n",
    "print(f\"sigmoid(0) = {sigmoid(0):.4f} (presne 0.5)\")\n",
    "print(f\"sigmoid(5) = {sigmoid(5):.4f} (blizko 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tanh funkce (Hyperbolicka tangenta)\n",
    "\n",
    "Podobna Sigmoidu, ale vystup je v rozsahu (-1, 1).\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "**Vyhody:**\n",
    "- Vystup je centrovany kolem nuly (casto lepsi pro skryte vrstvy)\n",
    "- Silnejsi gradienty nez Sigmoid\n",
    "\n",
    "**Nevyhody:**\n",
    "- Stale trpi problemem mizejiciho gradientu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementace Tanh\n",
    "def tanh(x):\n",
    "    \"\"\"Tanh aktivacni funkce\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"Derivace tanh\"\"\"\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "# Vizualizace\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graf funkce\n",
    "axes[0].plot(x, tanh(x), 'orange', linewidth=2, label='Tanh')\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Vstup (z)', fontsize=12)\n",
    "axes[0].set_ylabel('Vystup tanh(z)', fontsize=12)\n",
    "axes[0].set_title('Tanh funkce', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-1.2, 1.2)\n",
    "\n",
    "# Graf derivace\n",
    "axes[1].plot(x, tanh_derivative(x), 'red', linewidth=2, label='Derivace Tanh')\n",
    "axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Vstup (z)', fontsize=12)\n",
    "axes[1].set_ylabel('tanh\\'(z)', fontsize=12)\n",
    "axes[1].set_title('Derivace Tanh (max = 1.0)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Priklady hodnot Tanh:\")\n",
    "print(f\"tanh(-3) = {tanh(-3):.4f} (blizko -1)\")\n",
    "print(f\"tanh(0) = {tanh(0):.4f} (presne 0)\")\n",
    "print(f\"tanh(3) = {tanh(3):.4f} (blizko 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Hvezda modernich neuronovych siti!** Jednoducha a efektivni.\n",
    "\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "**Vyhody:**\n",
    "- Vypocetne velmi rychla\n",
    "- Netrpi problemem mizejiciho gradientu (pro kladne hodnoty)\n",
    "- V praxi funguje nejlepe pro vetsinu uloh\n",
    "\n",
    "**Nevyhody:**\n",
    "- Problem \"umirajicich neuronu\" (dying ReLU) - neuron muze \"odumrit\" a uz se neuci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementace ReLU\n",
    "def relu(x):\n",
    "    \"\"\"ReLU aktivacni funkce\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivace ReLU\"\"\"\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Vizualizace\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graf funkce\n",
    "axes[0].plot(x, relu(x), 'green', linewidth=2, label='ReLU')\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Vstup (z)', fontsize=12)\n",
    "axes[0].set_ylabel('Vystup ReLU(z)', fontsize=12)\n",
    "axes[0].set_title('ReLU funkce', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Graf derivace\n",
    "axes[1].plot(x, relu_derivative(x), 'darkgreen', linewidth=2, label='Derivace ReLU')\n",
    "axes[1].axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Vstup (z)', fontsize=12)\n",
    "axes[1].set_ylabel('ReLU\\'(z)', fontsize=12)\n",
    "axes[1].set_title('Derivace ReLU (0 nebo 1)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(-0.2, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Priklady hodnot ReLU:\")\n",
    "print(f\"relu(-3) = {relu(-3)} (negativa -> 0)\")\n",
    "print(f\"relu(0) = {relu(0)}\")\n",
    "print(f\"relu(3) = {relu(3)} (pozitiva -> beze zmeny)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Leaky ReLU - Reseni problemu umirajicich neuronu\n",
    "\n",
    "Varianta ReLU, ktera propusti male zaporne hodnoty.\n",
    "\n",
    "$$\\text{LeakyReLU}(x) = \\begin{cases} x & \\text{pokud } x > 0 \\\\ \\alpha x & \\text{jinak} \\end{cases}$$\n",
    "\n",
    "Kde $\\alpha$ je typicky 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementace Leaky ReLU\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU aktivacni funkce\"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Vizualizace porovnani ReLU a Leaky ReLU\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, relu(x), 'green', linewidth=2, label='ReLU')\n",
    "plt.plot(x, leaky_relu(x, alpha=0.1), 'purple', linewidth=2, label='Leaky ReLU (alpha=0.1)')\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Vstup (z)', fontsize=12)\n",
    "plt.ylabel('Vystup', fontsize=12)\n",
    "plt.title('ReLU vs Leaky ReLU', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Leaky ReLU propusti male zaporne hodnoty:\")\n",
    "print(f\"leaky_relu(-5, alpha=0.1) = {leaky_relu(-5, 0.1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Porovnani vsech aktivacnich funkci\n",
    "\n",
    "Pojdme si vizualizovat vsechny funkce na jednom grafu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porovnani vsech aktivacnich funkci\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.plot(x, sigmoid(x), 'b-', linewidth=2, label='Sigmoid (0 az 1)')\n",
    "plt.plot(x, tanh(x), 'orange', linewidth=2, label='Tanh (-1 az 1)')\n",
    "plt.plot(x, relu(x), 'green', linewidth=2, label='ReLU (0 nebo x)')\n",
    "plt.plot(x, leaky_relu(x, 0.1), 'purple', linewidth=2, linestyle='--', label='Leaky ReLU')\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Vstup neuronu (vazeny soucet)', fontsize=12)\n",
    "plt.ylabel('Vystup neuronu (aktivace)', fontsize=12)\n",
    "plt.title('Porovnani aktivacnich funkci', fontsize=14)\n",
    "plt.legend(loc='upper left', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-1.5, 5)\n",
    "plt.xlim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porovnani derivaci (dulezite pro uceni!)\n",
    "x = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.plot(x, sigmoid_derivative(x), 'b-', linewidth=2, label='Sigmoid\\' (max 0.25)')\n",
    "plt.plot(x, tanh_derivative(x), 'orange', linewidth=2, label='Tanh\\' (max 1.0)')\n",
    "plt.plot(x, relu_derivative(x), 'green', linewidth=2, label='ReLU\\' (0 nebo 1)')\n",
    "\n",
    "plt.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Vstup', fontsize=12)\n",
    "plt.ylabel('Derivace', fontsize=12)\n",
    "plt.title('Derivace aktivacnich funkci (klicove pro backpropagation)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.1, 1.2)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nProc jsou derivace dulezite?\")\n",
    "print(\"- Pri backpropagation se gradient nasobi derivaci\")\n",
    "print(\"- Sigmoid ma max derivaci 0.25 -> gradient rychle mizi\")\n",
    "print(\"- ReLU ma derivaci 1 pro kladne hodnoty -> gradient se neztraci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Problem mizejiciho gradientu (Vanishing Gradient)\n",
    "\n",
    "V hlubokych sitich se gradient nasobi pri kazde vrstve. Pokud je derivace aktivacni funkce mala, gradient rychle konverguje k nule.\n",
    "\n",
    "Pojdme si to demonstrovat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulace mizejiciho gradientu\n",
    "def simulate_gradient_flow(activation_derivative, n_layers=10, initial_gradient=1.0):\n",
    "    \"\"\"Simuluje tok gradientu skrz vrstvy site\"\"\"\n",
    "    gradients = [initial_gradient]\n",
    "    current_gradient = initial_gradient\n",
    "    \n",
    "    # Predpokladame, ze aktivace jsou v typickem rozsahu kolem 0\n",
    "    for layer in range(n_layers):\n",
    "        # Gradient se nasobi derivaci v kazde vrstve\n",
    "        derivative = activation_derivative(0)  # derivace v bode 0\n",
    "        current_gradient = current_gradient * derivative\n",
    "        gradients.append(current_gradient)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Porovnani gradientu pro ruzne aktivacni funkce\n",
    "layers = list(range(11))\n",
    "\n",
    "sigmoid_gradients = simulate_gradient_flow(sigmoid_derivative)\n",
    "tanh_gradients = simulate_gradient_flow(tanh_derivative)\n",
    "relu_gradients = simulate_gradient_flow(relu_derivative)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(layers, sigmoid_gradients, 'b-o', linewidth=2, label='Sigmoid', markersize=8)\n",
    "plt.plot(layers, tanh_gradients, 'orange', linewidth=2, marker='s', label='Tanh', markersize=8)\n",
    "plt.plot(layers, relu_gradients, 'g-^', linewidth=2, label='ReLU', markersize=8)\n",
    "\n",
    "plt.xlabel('Vrstva (od vystupu k vstupu)', fontsize=12)\n",
    "plt.ylabel('Velikost gradientu', fontsize=12)\n",
    "plt.title('Tok gradientu skrz 10 vrstev neuronove site', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Logaritmicka skala pro lepsi vizualizaci\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGradient po 10 vrstvach:\")\n",
    "print(f\"Sigmoid: {sigmoid_gradients[-1]:.10f} (prakticky nulovy!)\")\n",
    "print(f\"Tanh:    {tanh_gradients[-1]:.10f}\")\n",
    "print(f\"ReLU:    {relu_gradients[-1]:.10f} (zachovany!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prakticke cviceni: Vliv aktivacni funkce na uceni\n",
    "\n",
    "Pouzijeme sklearn MLPClassifier k porovnani, jak ruzne aktivacni funkce ovlivnuji uceni na datasetu \"dva mesice\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Vytvoreni datasetu\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vizualizace dat\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='black', alpha=0.7)\n",
    "plt.xlabel('X1', fontsize=12)\n",
    "plt.ylabel('X2', fontsize=12)\n",
    "plt.title('Dataset \"Dva mesice\" - nelinearni problem', fontsize=14)\n",
    "plt.colorbar(label='Trida')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porovnani aktivacnich funkci\n",
    "activations = ['identity', 'logistic', 'tanh', 'relu']\n",
    "activation_names = ['Bez aktivace (linearni)', 'Sigmoid (logistic)', 'Tanh', 'ReLU']\n",
    "\n",
    "results = {}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (activation, name) in enumerate(zip(activations, activation_names)):\n",
    "    # Trenovani modelu\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(16,),\n",
    "        activation=activation,\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        solver='adam',\n",
    "        learning_rate_init=0.01\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predikce a presnost\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    \n",
    "    # Vizualizace rozhodovaci hranice\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Vytvoreni mrizky pro vizualizaci\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 200),\n",
    "        np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 200)\n",
    "    )\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm', edgecolors='black')\n",
    "    ax.set_title(f'{name}\\nPresnost: {accuracy:.1%}', fontsize=12)\n",
    "    ax.set_xlabel('X1')\n",
    "    ax.set_ylabel('X2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shrnuti vysledku\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"POROVNANI AKTIVACNICH FUNKCI\")\n",
    "print(\"=\"*50)\n",
    "for name, acc in results.items():\n",
    "    print(f\"{name:30s}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Mini-projekt: Analyza rychlosti uceni\n",
    "\n",
    "Porovname, jak rychle se site s ruznymi aktivacemi uci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyza rychlosti uceni\n",
    "activations_to_test = ['logistic', 'tanh', 'relu']\n",
    "colors = {'logistic': 'blue', 'tanh': 'orange', 'relu': 'green'}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for activation in activations_to_test:\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(16,),\n",
    "        activation=activation,\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "        solver='sgd',\n",
    "        learning_rate_init=0.1\n",
    "    )\n",
    "    \n",
    "    # Trenovani s monitorovanim loss\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Vizualizace krivky uceni\n",
    "    plt.plot(model.loss_curve_, label=activation.upper(), \n",
    "             color=colors[activation], linewidth=2)\n",
    "\n",
    "plt.xlabel('Iterace (epocha)', fontsize=12)\n",
    "plt.ylabel('Chyba (loss)', fontsize=12)\n",
    "plt.title('Rychlost uceni pro ruzne aktivacni funkce', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPozorovani:\")\n",
    "print(\"- ReLU casto konverguje nejrychleji\")\n",
    "print(\"- Sigmoid muze byt pomalejsi kvuli mizejicimu gradientu\")\n",
    "print(\"- Tanh je obvykle rychlejsi nez Sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Bonus: Softmax pro vice trid\n",
    "\n",
    "Pro klasifikaci do vice trid se na vystupni vrstve pouziva **Softmax**, ktery prevadi vystupy na pravdepodobnosti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax pro vice trid\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax - pro vystupni vrstvu s vice tridami\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))  # numericky stabilni verze\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "# Priklad\n",
    "logits = np.array([2.0, 1.0, 0.1])  # Vystupy site pred softmax\n",
    "probabilities = softmax(logits)\n",
    "\n",
    "print(\"Softmax - prevod na pravdepodobnosti:\")\n",
    "print(f\"Vstupy (logits):          {logits}\")\n",
    "print(f\"Vystupy (pravdepodobnosti): {probabilities.round(3)}\")\n",
    "print(f\"Soucet pravdepodobnosti:   {probabilities.sum():.4f}\")\n",
    "\n",
    "# Vizualizace\n",
    "plt.figure(figsize=(10, 4))\n",
    "classes = ['Trida A', 'Trida B', 'Trida C']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(classes, logits, color=colors)\n",
    "plt.title('Pred Softmax (logits)', fontsize=12)\n",
    "plt.ylabel('Hodnota')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(classes, probabilities, color=colors)\n",
    "plt.title('Po Softmax (pravdepodobnosti)', fontsize=12)\n",
    "plt.ylabel('Pravdepodobnost')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Tabulka: Kdy pouzit kterou aktivacni funkci\n",
    "\n",
    "| Funkce | Rozsah | Pouziti | Vyhody | Nevyhody |\n",
    "|--------|--------|---------|--------|----------|\n",
    "| **Sigmoid** | (0, 1) | Vystupni vrstva (binarni klasifikace) | Interpretace jako pravdepodobnost | Mizejici gradient |\n",
    "| **Tanh** | (-1, 1) | Skryte vrstvy (RNN) | Centrovany vystup | Mizejici gradient |\n",
    "| **ReLU** | [0, inf) | Skryte vrstvy (CNN, MLP) | Rychle, efektivni | Umirajici neurony |\n",
    "| **Leaky ReLU** | (-inf, inf) | Skryte vrstvy | Resi umirajici neurony | O neco slozitejsi |\n",
    "| **Softmax** | (0, 1), soucet=1 | Vystupni vrstva (multi-class) | Pravdepodobnostni rozdeleni | Pouze pro vystup |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Shrnuti kapitoly\n",
    "\n",
    "### Co jsme se naucili:\n",
    "\n",
    "1. **Aktivacni funkce** jsou klicove pro pridani nelinearity do neuronove site\n",
    "\n",
    "2. **Sigmoid** - klasicka funkce (0-1), dobra pro vystup, ale trpi mizejicim gradientem\n",
    "\n",
    "3. **Tanh** - podobna Sigmoidu (-1 az 1), lepsi pro skryte vrstvy\n",
    "\n",
    "4. **ReLU** - moderni standard, rychla a efektivni, resi problem mizejiciho gradientu\n",
    "\n",
    "5. **Problem mizejiciho gradientu** - u Sigmoid/Tanh gradient rychle konverguje k nule\n",
    "\n",
    "### Prakticka doporuceni:\n",
    "- **Skryte vrstvy:** Zacinajte s ReLU\n",
    "- **Vystupni vrstva (binarni):** Sigmoid\n",
    "- **Vystupni vrstva (vice trid):** Softmax\n",
    "- **RNN/LSTM:** Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Kviz\n",
    "\n",
    "Otestujte sve znalosti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kviz\n",
    "print(\"=\"*60)\n",
    "print(\"KVIZ - Aktivacni funkce\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "questions = [\n",
    "    {\n",
    "        \"question\": \"1. Jaky je rozsah vystupu Sigmoid funkce?\",\n",
    "        \"options\": [\"a) -1 az 1\", \"b) 0 az 1\", \"c) 0 az nekonecno\", \"d) -nekonecno az nekonecno\"],\n",
    "        \"answer\": \"b\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"2. Ktera aktivacni funkce je nejpouzivanejsi pro skryte vrstvy?\",\n",
    "        \"options\": [\"a) Sigmoid\", \"b) Tanh\", \"c) ReLU\", \"d) Softmax\"],\n",
    "        \"answer\": \"c\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"3. Co je problem mizejiciho gradientu?\",\n",
    "        \"options\": [\"a) Gradient roste prilis rychle\", \"b) Gradient se blizi k nule\", \n",
    "                   \"c) Gradient je vzdy 1\", \"d) Gradient se nahodne meni\"],\n",
    "        \"answer\": \"b\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"4. Jaka je maximalni hodnota derivace Sigmoid?\",\n",
    "        \"options\": [\"a) 1.0\", \"b) 0.5\", \"c) 0.25\", \"d) 0.1\"],\n",
    "        \"answer\": \"c\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"5. Kdy pouzijeme Softmax?\",\n",
    "        \"options\": [\"a) Pro skryte vrstvy\", \"b) Pro binarni klasifikaci\", \n",
    "                   \"c) Pro klasifikaci do vice trid\", \"d) Pro regresi\"],\n",
    "        \"answer\": \"c\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{q['question']}\")\n",
    "    for opt in q['options']:\n",
    "        print(f\"   {opt}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ODPOVEDI: 1-b, 2-c, 3-b, 4-c, 5-c\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Vase vyzva\n",
    "\n",
    "1. Zkuste pridat do site druhou skrytou vrstvu a porovnejte vysledky\n",
    "2. Experimentujte s parametrem `alpha` v Leaky ReLU\n",
    "3. Vytvorte vlastni dataset pomoci `make_circles()` a otestujte ruzne aktivace\n",
    "\n",
    "**Tip:** V praxi vzdy zacinajte s ReLU a experimentujte dal pouze pokud je to potreba!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
